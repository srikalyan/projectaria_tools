---
sidebar_position: 10
title: Machine Perception Services (MPS)
---

# Project Aria Machine Perception Services

To accelerate research with Project Aria, we provide several Spatial AI machine perception capabilities that help form the foundation for future Contextualized AI applications and analysis of egocentric data. These capabilities are powered by a set of proprietary machine perception algorithms, designed for Project Aria glasses, that provide superior accuracy and robustness on Aria data compared to off-the-shelf open source algorithms.

All Machine Perception Services (MPS) are offered as post-processing of VRS files via a cloud service. Use the [MPS CLI](request_mps/mps_cli.mdx) or the [Desktop App](request_mps/desktop_mps.mdx), to request derived data from any VRS file that contains necessary sensor data.

## Current MPS offerings
The following MPS can be requested, as long as the data has been recorded with a compatible Recording Profile. The [Recording Profile Guide](/ARK/glasses_manual/profile_guide.mdx) provides a quick list of compatible sensor profile,s or go to [Recording Profiles](/tech_spec/recording_profiles.mdx) in Technical Specifications for more granular information about each profile.

MPS offerings are grouped into SLAM services (called Trajectory in the Desktop app) and Eye Gaze services.

## SLAM services

To get these outputs the [recording profile](/tech_spec/recording_profiles.mdx) must have SLAM cameras + IMU enabled.

### 6DoF trajectory
MPS provides two types of high frequency (1kHz) trajectories:
* [Open loop trajectory](/data_formats/mps/slam/mps_trajectory.mdx#open-loop-trajectory) - local odometry estimation from visual-inertial odometry (VIO)
* [Closed loop trajectory](/data_formats/mps/slam/mps_trajectory.mdx#closed-loop-trajectory) - created via batch optimization, using multi-sensors' input (SLAM, IMU, barometer, Wi-Fi and GPS), fully optimized and provides poses in a consistent frame of reference.

### Semi-dense point cloud

[Semi-dense point cloud](/data_formats/mps/slam/mps_pointcloud.mdx) data supports researchers who need static scene 3D reconstructions, reliable 2D images tracks or a representative visualization of the environment.


### Online sensor calibration
The [time-varying intrinsic and extrinsic calibrations](/data_formats/mps/slam/mps_calibration.mdx) of cameras and IMUs are estimated at the frequency of the SLAM (mono scene) cameras by our multi-sensor state estimation pipeline.

### Multi-SLAM

[Multi-SLAM](/data_formats/mps/slam/mps_multi_slam.mdx) can be requested on two or more recordings. It creates all of the above SLAM output, in a shared co-ordinate frame.

Multi-SLAM can only be requested using the [MPS CLI SDK](request_mps/mps_cli.mdx).


## Eye Gaze data

Eye gaze is one of the most important indicators of human attention, [eye gaze direction](/data_formats/mps/mps_eye_gaze.mdx) estimation with uncertainty is provided by MPS. Eye gaze estimation uses the data from the Eye Tracking (ET) cameras.

These outputs can be generated for any recording that had [ET cameras enabled](/tech_spec/recording_profiles.mdx).

If you have made a recording with [In-Session Eye Gaze Calibration](eye_gaze_calibration.mdx), you will receive a second .csv file with calibrated eye gaze outputs.


## About MPS Data Loader APIs
Please refer to our [MPS data loader APIs](/data_utilities/core_code_snippets/mps.mdx#load-mps-output) (C++ and Python support) to load the MPS outputs into your application. The [visualization guide](/data_utilities/visualization/visualization_cpp.mdx#mps-static-scene-visualizer) shows how to visualize all the MPS outputs.

## Questions & Feedback

If you have feedback you'd like to provide, be it overall trends and experiences or where we can improve, we'd love to hear from you. Go to our [Support page](/support.mdx) for different ways to get in touch.
