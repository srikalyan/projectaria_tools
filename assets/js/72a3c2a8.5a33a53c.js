"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[8540],{15680:(e,a,t)=>{t.r(a),t.d(a,{MDXContext:()=>d,MDXProvider:()=>u,mdx:()=>x,useMDXComponents:()=>c,withMDXComponents:()=>m});var r=t(96540);function o(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function n(){return n=Object.assign||function(e){for(var a=1;a<arguments.length;a++){var t=arguments[a];for(var r in t)Object.prototype.hasOwnProperty.call(t,r)&&(e[r]=t[r])}return e},n.apply(this,arguments)}function i(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);a&&(r=r.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,r)}return t}function s(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?i(Object(t),!0).forEach((function(a){o(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function l(e,a){if(null==e)return{};var t,r,o=function(e,a){if(null==e)return{};var t,r,o={},n=Object.keys(e);for(r=0;r<n.length;r++)t=n[r],a.indexOf(t)>=0||(o[t]=e[t]);return o}(e,a);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);for(r=0;r<n.length;r++)t=n[r],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(o[t]=e[t])}return o}var d=r.createContext({}),m=function(e){return function(a){var t=c(a.components);return r.createElement(e,n({},a,{components:t}))}},c=function(e){var a=r.useContext(d),t=a;return e&&(t="function"==typeof e?e(a):s(s({},a),e)),t},u=function(e){var a=c(e.components);return r.createElement(d.Provider,{value:a},e.children)},p="mdxType",f={inlineCode:"code",wrapper:function(e){var a=e.children;return r.createElement(r.Fragment,{},a)}},h=r.forwardRef((function(e,a){var t=e.components,o=e.mdxType,n=e.originalType,i=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),m=c(t),u=o,p=m["".concat(i,".").concat(u)]||m[u]||f[u]||n;return t?r.createElement(p,s(s({ref:a},d),{},{components:t})):r.createElement(p,s({ref:a},d))}));function x(e,a){var t=arguments,o=a&&a.mdxType;if("string"==typeof e||o){var n=t.length,i=new Array(n);i[0]=h;var s={};for(var l in a)hasOwnProperty.call(a,l)&&(s[l]=a[l]);s.originalType=e,s[p]="string"==typeof e?e:o,i[1]=s;for(var d=2;d<n;d++)i[d]=t[d];return r.createElement.apply(null,i)}return r.createElement.apply(null,t)}h.displayName="MDXCreateElement"},41494:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>i,default:()=>u,frontMatter:()=>n,metadata:()=>s,toc:()=>d});var r=t(58168),o=(t(96540),t(15680));const n={sidebar_position:20,title:"Format"},i="Project Aria VRS Format",s={unversionedId:"data_formats/aria_vrs/aria_vrs_format",id:"data_formats/aria_vrs/aria_vrs_format",title:"Format",description:"This page provides information about how Aria data streams are identified in VRS, Aria sensor data configuration, followed by useful VRS tools for common use cases.",source:"@site/docs/data_formats/aria_vrs/aria_vrs_format.mdx",sourceDirName:"data_formats/aria_vrs",slug:"/data_formats/aria_vrs/aria_vrs_format",permalink:"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_format",draft:!1,editUrl:"https://github.com/facebookresearch/projectaria_tools/tree/main/website/docs/data_formats/aria_vrs/aria_vrs_format.mdx",tags:[],version:"current",sidebarPosition:20,frontMatter:{sidebar_position:20,title:"Format"},sidebar:"tutorialSidebar",previous:{title:"Aria VRS",permalink:"/projectaria_tools/docs/data_formats/aria_vrs/"},next:{title:"Timestamp Definitions",permalink:"/projectaria_tools/docs/data_formats/aria_vrs/timestamps_in_aria_vrs"}},l={},d=[{value:"Aria data streams",id:"aria-data-streams",level:2},{value:"Aria sensor data and configuration",id:"aria-sensor-data-and-configuration",level:2},{value:"How data is stored for image recordings",id:"how-data-is-stored-for-image-recordings",level:3},{value:"How data is stored for audio recordings",id:"how-data-is-stored-for-audio-recordings",level:3},{value:"Sensor configuration blob",id:"sensor-configuration-blob",level:3},{value:"Useful VRS tools",id:"useful-vrs-tools",level:2},{value:"Check the VRS file\u2019s validity and integrity",id:"check-the-vrs-files-validity-and-integrity",level:3},{value:"Extract image or audio content to folders",id:"extract-image-or-audio-content-to-folders",level:3},{value:"Extract all content to folders and JSONs",id:"extract-all-content-to-folders-and-jsons",level:3},{value:"Inspect how many data recordings there are by type",id:"inspect-how-many-data-recordings-there-are-by-type",level:3}],m={toc:d},c="wrapper";function u(e){let{components:a,...t}=e;return(0,o.mdx)(c,(0,r.A)({},m,t,{components:a,mdxType:"MDXLayout"}),(0,o.mdx)("h1",{id:"project-aria-vrs-format"},"Project Aria VRS Format"),(0,o.mdx)("p",null,"This page provides information about how Aria data streams are identified in VRS, Aria sensor data configuration, followed by useful VRS tools for common use cases."),(0,o.mdx)("p",null,"Project Aria data is stored in ",(0,o.mdx)("a",{parentName:"p",href:"https://facebookresearch.github.io/vrs/"},"VRS"),", which is optimized to record and playback streams of multi-modal sensor data, such as images, audio, and other discrete sensors (IMU, temperature, etc.). Data is stored in per-device streams of time-stamped records."),(0,o.mdx)("p",null,"The most intuitive way to access Aria data is via our loaders and visualizers. Go to ",(0,o.mdx)("a",{parentName:"p",href:"/projectaria_tools/docs/data_utilities/"},"Data Utilities")," for Python and C++ interfaces to easily access VRS data. You may also want to use ",(0,o.mdx)("a",{parentName:"p",href:"https://facebookresearch.github.io/vrs/"},"vrstools")," to extract or inspect VRS data."),(0,o.mdx)("h2",{id:"aria-data-streams"},"Aria data streams"),(0,o.mdx)("p",null,"In VRS, data is organized by streams. Each stream stores the data measured by a specific sensor, except for Eye Tracking (both camers share a single data stream) and microphones (up to 7 microphones share a single stream)."),(0,o.mdx)("p",null,"The streams are identified by their stream ID. Each stream ID is composed of two parts, a recordable type ID to categorize the type of the sensor and a stream ID for identify the specific sensor instance. E.g. the first SLAM (aka Mono Scene) camera is identified as 1201-1 where 1201 is the numerical ID for SLAM camera data type, and 1 identifies the cameras as the first instance."),(0,o.mdx)("p",null,"Streams can also be identified by a short label. Labels are used to identify sensors in calibration."),(0,o.mdx)("p",null,"The following table lists the Stream ID and Recordable Type ID, as well as their label. GPS, Wi-Fi and Bluetooth have labels, but are not calibrated. If you use projectaria tools loaders, you do not have to memorize this mapping as there is an API that converts between Stream ID and labels."),(0,o.mdx)("p",null,(0,o.mdx)("strong",{parentName:"p"},"Table 1:")," ",(0,o.mdx)("em",{parentName:"p"},"IDs Used for Sensors")),(0,o.mdx)("table",null,(0,o.mdx)("thead",null,(0,o.mdx)("tr",null,(0,o.mdx)("th",null,"        ",(0,o.mdx)("b",null,"Sensor"),"         "),(0,o.mdx)("th",null,"Stream ID  "),(0,o.mdx)("th",null,"Recordable Type ID            "),(0,o.mdx)("th",null,"label           "))),(0,o.mdx)("tbody",null,(0,o.mdx)("tr",null,(0,o.mdx)("td",null,"ET camera        "),(0,o.mdx)("td",null,"211-1      "),(0,o.mdx)("td",null,"EyeCameraRecordableClass      "),(0,o.mdx)("td",null,"camera-et       ")),(0,o.mdx)("tr",null,(0,o.mdx)("td",null,"RGB camera       "),(0,o.mdx)("td",null,"214-1      "),(0,o.mdx)("td",null,"RgbCameraRecordableClass      "),(0,o.mdx)("td",null,"camera-rgb      ")),(0,o.mdx)("tr",null,(0,o.mdx)("td",null,"Microphone       "),(0,o.mdx)("td",null,"231-1      "),(0,o.mdx)("td",null,"StereoAudioRecordableClass    "),(0,o.mdx)("td",null,"mic             ")),(0,o.mdx)("tr",null,(0,o.mdx)("td",null,"Barometer        "),(0,o.mdx)("td",null,"247-1      "),(0,o.mdx)("td",null,"BarometerRecordableClass      "),(0,o.mdx)("td",null,"baro            ")),(0,o.mdx)("tr",null,(0,o.mdx)("td",null,"GPS              "),(0,o.mdx)("td",null,"281-1      "),(0,o.mdx)("td",null,"GpsRecordableClass            "),(0,o.mdx)("td",null,"gps             ")),(0,o.mdx)("tr",null,(0,o.mdx)("td",null,"Wi-Fi            "),(0,o.mdx)("td",null,"282-1      "),(0,o.mdx)("td",null,"WifiBeaconRecordableClass"),(0,o.mdx)("td",null,"wps       ")),(0,o.mdx)("tr",null,(0,o.mdx)("td",null,"Bluetooth        "),(0,o.mdx)("td",null,"283-1      "),(0,o.mdx)("td",null,"BluetoothBeaconRecordableClass"),(0,o.mdx)("td",null,"bluetooth       ")),(0,o.mdx)("tr",null,(0,o.mdx)("td",null,"SLAM/Mono Scene camera left "),(0,o.mdx)("td",null,"1201-1     "),(0,o.mdx)("td",null,"SlamCameraData                "),(0,o.mdx)("td",null,"camera-slam-left")),(0,o.mdx)("tr",null,(0,o.mdx)("td",null,"SLAM/Mono Scene camera right"),(0,o.mdx)("td",null,"1201-2     "),(0,o.mdx)("td",null,"SlamCameraData                "),(0,o.mdx)("td",null,"camera-slam-right")),(0,o.mdx)("tr",null,(0,o.mdx)("td",null,"IMU (1kHz)       "),(0,o.mdx)("td",null,"1202-1     "),(0,o.mdx)("td",null,"SlamImuData                   "),(0,o.mdx)("td",null,"imu-right       ")),(0,o.mdx)("tr",null,(0,o.mdx)("td",null,"IMU (800Hz)      "),(0,o.mdx)("td",null,"1202-2     "),(0,o.mdx)("td",null,"SlamImuData                   "),(0,o.mdx)("td",null,"imu-left        ")),(0,o.mdx)("tr",null,(0,o.mdx)("td",null,"Magnetometer     "),(0,o.mdx)("td",null,"1203-1     "),(0,o.mdx)("td",null,"SlamMagnetometerData          "),(0,o.mdx)("td",null,"mag             ")))),(0,o.mdx)("p",null,"Each stream also contains a configuration blob that stores sensor-specific information such as image resolution and nominal frame rate."),(0,o.mdx)("p",null,"All data in VRS is timestamped. Go to ",(0,o.mdx)("a",{parentName:"p",href:"/projectaria_tools/docs/data_formats/aria_vrs/timestamps_in_aria_vrs"},"Timestamps in Aria VRS")," for more details."),(0,o.mdx)("h2",{id:"aria-sensor-data-and-configuration"},"Aria sensor data and configuration"),(0,o.mdx)("p",null,"Sensor data includes:"),(0,o.mdx)("ul",null,(0,o.mdx)("li",{parentName:"ul"},"Sensor readout"),(0,o.mdx)("li",{parentName:"ul"},"Timestamps"),(0,o.mdx)("li",{parentName:"ul"},"Acquisition parameters (exposure and gain settings)"),(0,o.mdx)("li",{parentName:"ul"},"Conditions (e.g. temperature) during data collection")),(0,o.mdx)("p",null,"Most sensor data of a single stream and at a specific timestamp is stored as a single piece, except for image and audio."),(0,o.mdx)("h3",{id:"how-data-is-stored-for-image-recordings"},"How data is stored for image recordings"),(0,o.mdx)("ul",null,(0,o.mdx)("li",{parentName:"ul"},"Each camera stores a single image frame at a time, with the exception of the ET camera.",(0,o.mdx)("ul",{parentName:"li"},(0,o.mdx)("li",{parentName:"ul"},"ET cameras pair share a single image frame by concatenating horizontally."))),(0,o.mdx)("li",{parentName:"ul"},"The image frame contains two parts, the image itself and the image record.",(0,o.mdx)("ul",{parentName:"li"},(0,o.mdx)("li",{parentName:"ul"},"The image record stores timestamps, frame id, and acquisition parameters, such as exposure and gain. This avoids having to read image data to get the information in the record.")))),(0,o.mdx)("h3",{id:"how-data-is-stored-for-audio-recordings"},"How data is stored for audio recordings"),(0,o.mdx)("ul",null,(0,o.mdx)("li",{parentName:"ul"},"The audio data is grouped into data chunks of 4096 audio samples from all 7 microphones."),(0,o.mdx)("li",{parentName:"ul"},"Each chunk contains two parts, the data part for the audio signal, and the report part for the timestamps of each audio signal.")),(0,o.mdx)("h3",{id:"sensor-configuration-blob"},"Sensor configuration blob"),(0,o.mdx)("p",null,"The sensor configuration blob stores the static information of a stream. Common sensor configuration stores information, such as sensor model, sensor serial (if available) as well as frame rate."),(0,o.mdx)("p",null,"Stream-specific information, such as image resolution, is also stored in configurations."),(0,o.mdx)("p",null,"Go to the ",(0,o.mdx)("a",{parentName:"p",href:"https://github.com/facebookresearch/projectaria_tools/blob/main/core/data_provider/players"},"source code")," for the detailed implementation of sensor data and configurations. Go to the ",(0,o.mdx)("a",{parentName:"p",href:"/projectaria_tools/docs/data_utilities/advanced_code_snippets/plotting_sensor_data"},"Advanced Code Snippets")," for example sensor data and how to access sensor data using Python data utilities."),(0,o.mdx)("h2",{id:"useful-vrs-tools"},"Useful VRS tools"),(0,o.mdx)("p",null,"The most intuitive way to access Aria data is via our ",(0,o.mdx)("a",{parentName:"p",href:"/projectaria_tools/docs/data_utilities/getting_started"},"loaders and visualizers"),". We provide Python and C++ interface to easily access VRS data."),(0,o.mdx)("p",null,"You may also want to use VRS tools to extract or inspect VRS data. Here are a few common use cases:"),(0,o.mdx)("h3",{id:"check-the-vrs-files-validity-and-integrity"},"Check the VRS file\u2019s validity and integrity"),(0,o.mdx)("p",null,"The ",(0,o.mdx)("inlineCode",{parentName:"p"},"check")," command decodes every record in the VRS file and prints how many records were decoded successfully. It proves that the VRS file is correct at the VRS level. You can also compute a checksum to ensure you have valid VRS files. For more information go to ",(0,o.mdx)("a",{parentName:"p",href:"https://facebookresearch.github.io/vrs/docs/VrsCliTool#file-validation"},"VRS File Validation"),"."),(0,o.mdx)("pre",null,(0,o.mdx)("code",{parentName:"pre"},"$ vrs check <file.vrs>\n$ vrs checksum <file.vrs>\n")),(0,o.mdx)("p",null,"If the file is not valid, it is normally because there is missing data that could lead to invalid behavior with the tooling. All files in our ",(0,o.mdx)("a",{parentName:"p",href:"/projectaria_tools/docs/open_datasets/"},"open datasets")," are valid, so if you encounter issues with these, re-downloading the files should resolve the issue."),(0,o.mdx)("h3",{id:"extract-image-or-audio-content-to-folders"},"Extract image or audio content to folders"),(0,o.mdx)("p",null,"Use the following commands to extract JPEG or WAV files. Use the ",(0,o.mdx)("inlineCode",{parentName:"p"},"--to <folder_path>")," to specify a destination folder where the data will be extracted, or it will be added to the current working directory."),(0,o.mdx)("pre",null,(0,o.mdx)("code",{parentName:"pre"},"$ vrs extract-images <file.vrs> --to <image_folder>\n$ vrs extract-audio <file.vrs> --to <audio_folder>\n")),(0,o.mdx)("p",null,"To extract RAW image files, use:"),(0,o.mdx)("pre",null,(0,o.mdx)("code",{parentName:"pre"},"vrs extract-images <file.vrs> --raw-images --to <image_folder>\n")),(0,o.mdx)("h3",{id:"extract-all-content-to-folders-and-jsons"},"Extract all content to folders and JSONs"),(0,o.mdx)("p",null,"This command lets you extract all images, audio, and metadata into files:"),(0,o.mdx)("pre",null,(0,o.mdx)("code",{parentName:"pre"},"vrs extract-all <file.vrs> --to <folder>\n")),(0,o.mdx)("p",null,"The metadata is extracted into a single JSONS file that contains a succession of json messages, one per line. Each line corresponds to a single record, in timestamp order, so it is possible to parse it even if the number of records is huge. Saving all the data in a single file prevents saturating your disk with possibly millions of small files.\nOnce extracted, your file will look like this:"),(0,o.mdx)("pre",null,(0,o.mdx)("code",{parentName:"pre"},"    \u251c\u2500\u2500 file.vrs\n    \u251c\u2500\u2500 all_data\n      * `NNNN-MM` folders: image folders, one folder per stream containing images.\n      \u251c\u2500\u2500 1201-1 # SLAM Left images\n          \u251c\u2500\u2500 *.jpg\n      \u251c\u2500\u2500 1201-2 # SLAM Right images\n          \u251c\u2500\u2500 *.jpg\n      \u251c\u2500\u2500 211-1  # Eye Tracking images\n          \u251c\u2500\u2500 *.jpg\n      \u251c\u2500\u2500 214-1  # RGB (Color) Camera images\n          \u251c\u2500\u2500 *.jpg\n      \u251c\u2500\u2500 metadata.jsons\n      \u2514\u2500\u2500 ReadMe.md\n")),(0,o.mdx)("p",null,"For more information, go to ",(0,o.mdx)("a",{parentName:"p",href:"https://facebookresearch.github.io/vrs/docs/VrsCliTool#data-extraction"},"VRS Data Extraction"),"."),(0,o.mdx)("h3",{id:"inspect-how-many-data-recordings-there-are-by-type"},"Inspect how many data recordings there are by type"),(0,o.mdx)("pre",null,(0,o.mdx)("code",{parentName:"pre"},'vrs <file.vrs> | grep "] records."\n')),(0,o.mdx)("p",null,"Will get you a return like this:"),(0,o.mdx)("pre",null,(0,o.mdx)("code",{parentName:"pre"},"623 Eye Camera Class #1 - device/aria [211-1] records.\n1244 RGB Camera Class #1 - device/aria [214-1] records.\n729 Stereo Audio Class #1 - device/aria [231-1] records.\n3101 Barometer Data Class #1 - device/aria [247-1] records.\n65 Time Domain Mapping Class #1 - device/aria [285-1] records.\n623 Camera Data (SLAM) #1 - device/aria [1201-1] records.\n623 Camera Data (SLAM) #2 - device/aria [1201-2] records.\n61965 IMU Data (SLAM) #1 - device/aria [1202-1] records.\n50002 IMU Data (SLAM) #2 - device/aria [1202-2] records.\n619 Magnetometer Data (SLAM) #1 - device/aria [1203-1] records.\n")),(0,o.mdx)("p",null,"Each line reports how many data records are stored in each data stream as well as the stream ID. For example, in this line:"),(0,o.mdx)("pre",null,(0,o.mdx)("code",{parentName:"pre"},"623 Camera Data (SLAM) #2 - device/aria [1201-2] records.\n")),(0,o.mdx)("p",null,"We can see that:"),(0,o.mdx)("ul",null,(0,o.mdx)("li",{parentName:"ul"},"The stream name is ",(0,o.mdx)("inlineCode",{parentName:"li"},"Camera Data (SLAM) #2")," (Mono Scene camera on the right) and identified by numerical ID ","[1201-2]"),(0,o.mdx)("li",{parentName:"ul"},"SLAM camera #2 has recorded 623 frames")))}u.isMDXComponent=!0}}]);