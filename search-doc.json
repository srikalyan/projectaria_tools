[{"title":"Aria Research Kit","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/about_ARK","content":"","keywords":"","version":"Next"},{"title":"If you just received your Aria glasses​","type":1,"pageTitle":"Aria Research Kit","url":"/projectaria_tools/docs/ARK/about_ARK#if-you-just-received-your-aria-glasses","content":"We suggest the following steps: Go to the Quickstart guide to get set upJoin the Academic Partners Workplace GroupJoin the Project Aria DiscordIf you have any difficulties getting set up, please contact ariaops@meta.com or check out our Support page ","version":"Next","tagName":"h2"},{"title":"Mobile Companion App requirements​","type":1,"pageTitle":"Aria Research Kit","url":"/projectaria_tools/docs/ARK/about_ARK#mobile-companion-app-requirements","content":"To be able to use Aria glasses you will need to use the Aria Mobile Companion app. AndroidiOS If you're using Android, your mobile device will need: Android OS version 10 or above installedARM64 processor preferred(Optional) ARCore Depth API (https://developers.google.com/ar/devices) support needed for eye-tracking calibration64-bit is preferred, although 32-bit is supported Go to the ARK SW Downloads and Updates page to get this software. ","version":"Next","tagName":"h2"},{"title":"Other requirements (optional software)​","type":1,"pageTitle":"Aria Research Kit","url":"/projectaria_tools/docs/ARK/about_ARK#other-requirements-optional-software","content":"","version":"Next","tagName":"h2"},{"title":"Desktop Companion App​","type":1,"pageTitle":"Aria Research Kit","url":"/projectaria_tools/docs/ARK/about_ARK#desktop-companion-app","content":"To be able to request Machine Perception Services or stream data, you will need to use the Desktop Companion App. Go to the ARK SW Downloads and Updates page to get this software. Desktop Companion App runs on the following platforms: MacOS Big Sur 11.3+, both Intel and Apple SiliconWindows 10 x86_64 with DirectX (or OpenGL but the latest drivers should be installed)Windows 11 may work, but is not actively supported at this timeLinux, as a debian package for Ubuntu, 22.04 LTS version. The app was only tested for that specific version under Gnome 42.5 and X11 (X Server) as well as Wayland. Any other debian distribution (Ubuntu 22.04 fork such as Kubuntu, Mint etc.) or environment may or may not work. ","version":"Next","tagName":"h3"},{"title":"Project Aria Tools​","type":1,"pageTitle":"Aria Research Kit","url":"/projectaria_tools/docs/ARK/about_ARK#project-aria-tools","content":"Project Aria Tools is open source tooling for working with Aria data, go to Data Utilities for more details. Project Aria Tools and VRS tooling are not available on Windows at this time. The codebase is supported on the following platforms: x64 Linux distributions of: Fedora 36, 37, 38Ubuntu focal (20.04 LTS) and jammy (22.04 LTS) Mac Intel or Mac ARM-based (M1) with MacOS 11 (Big Sur) or newer ","version":"Next","tagName":"h3"},{"title":"Project Aria Client SDK and CLI​","type":1,"pageTitle":"Aria Research Kit","url":"/projectaria_tools/docs/ARK/about_ARK#project-aria-client-sdk-and-cli","content":"Through the Client SDK and CLI you can directly interact with Aria glasses and stream data from the glasses to a computer. The codebase is supported on the following platforms: x64 Linux distributions of: Fedora 36 or newerUbuntu jammy (22.04) or newer Mac Intel or Mac ARM-based (M1) with MacOS 11 (Big Sur) or newer ","version":"Next","tagName":"h3"},{"title":"ARK SW Downloads and Updates","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/ark_downloads","content":"","keywords":"","version":"Next"},{"title":"Mobile Companion App​","type":1,"pageTitle":"ARK SW Downloads and Updates","url":"/projectaria_tools/docs/ARK/ark_downloads#mobile-companion-app","content":"The latest version is v150, go to ARK Software Release Notes to find out about the latest features. iOSAndroid iOS​ The Aria Mobile Companion app is available on iOS as a beta app through TestFlight. On your phone, follow this invitation link to download the app. To update: Open the TestFlight appNext to the Aria app, select Update ","version":"Next","tagName":"h2"},{"title":"Desktop Companion App​","type":1,"pageTitle":"ARK SW Downloads and Updates","url":"/projectaria_tools/docs/ARK/ark_downloads#desktop-companion-app","content":"The latest version is v37, go to ARK Software Release Notes to find out about the latest features. To update the app, download and install the latest version. MacOSWindowsLinux Aria For MacOS Installer (Intel &amp; Apple Silicon)​ Download Aria's DMG installerOpen the downloaded fileDrag and drop Aria.app to your Applications folderDouble clicking on Aria.app to launch the app ","version":"Next","tagName":"h2"},{"title":"Are the Companion apps open source?​","type":1,"pageTitle":"ARK SW Downloads and Updates","url":"/projectaria_tools/docs/ARK/ark_downloads#are-the-companion-apps-open-source","content":"No, the Mobile and Desktop app are Meta Licensed Materials and are licensed by Meta to research partners via organizational or individual research agreements. Go to projectaria.com to find out how to become a research partner. This software is not part of Project Aria Tools, so it won’t be downloaded when you install Project Aria Tools Data Utilities, which is open source and licensed under Apache 2.0. ","version":"Next","tagName":"h2"},{"title":"Project Aria Glasses Quickstart","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/ARK_quickstart","content":"","keywords":"","version":"Next"},{"title":"Get set up​","type":1,"pageTitle":"Project Aria Glasses Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#get-set-up","content":"Once you've been approved and can receive your Aria glasses you will get two emails: Welcome to Aria Contains your account details, use these for signing into the Mobile and Desktop Companion apps Join [Person] in Project Aria Academic Partner Announcements, Feedback &amp; Support Follow the prompts to join Aria's research community space where researchers and engineers at Meta and all our Academic partners can connect, ask questions, share ideas and provide support.See How to join the Academic Partner Workplace group for further instructions. ","version":"Next","tagName":"h2"},{"title":"Get connected​","type":1,"pageTitle":"Project Aria Glasses Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#get-connected","content":"Before your glasses arrive we encourage you to join the Academic Partners Workplace group. It's a great place to get the latest announcements, provide feedback, ask questions. Unboxing videos are very welcome! ","version":"Next","tagName":"h2"},{"title":"Get to know your glasses​","type":1,"pageTitle":"Project Aria Glasses Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#get-to-know-your-glasses","content":"About the Aria Research Kit Includes hardware requirements for using ARK apps Technical Specifications Go to the Tech Spec part of the wiki to find out about Aria capabilities, recording profiles etc Glasses Manual Provides information about Project Aria glasses buttons, powering on and off, the privacy switch, how to do a factory reset, LED states, etc   ","version":"Next","tagName":"h2"},{"title":"Update your glasses​","type":1,"pageTitle":"Project Aria Glasses Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#update-your-glasses","content":"You'll need to update your Aria glasses using the Mobile companion app before you can use them. ","version":"Next","tagName":"h2"},{"title":"Install the Mobile companion app​","type":1,"pageTitle":"Project Aria Glasses Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#install-the-mobile-companion-app","content":"Follow the instructions in the ARK SW Downloads and Updates page to download and install the app (this is where you'll download updates as well). ","version":"Next","tagName":"h3"},{"title":"Pair your glasses​","type":1,"pageTitle":"Project Aria Glasses Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#pair-your-glasses","content":"Plug your glasses into power using the provided cableSign into the app using your provided user name and passwordFollow the prompts to pair your glasses Your glasses are automatically updated when you first pair themGo to the Mobile app page for further information Further updates will be queued automatically when you use your glasses via the Mobile Companion App. If you need to manually check for updates, select the gear icon (settings) next to your glasses and then select Check for OS Updates. ","version":"Next","tagName":"h3"},{"title":"Set your Default Recording Profile​","type":1,"pageTitle":"Project Aria Glasses Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#set-your-default-recording-profile","content":"You can set the default recording profile via Mobile or Desktop Companion App. Once set, this is the default profile used when recording or streaming data by any means. When you select New Recording in the Mobile Companion app, you'll initially see the default recording profile. Select the recording profile to view sensor settings or to select a different recording profile. In the Mobile Companion App, you can change the Default Recording Profile at any time via Device Settings (select the settings/gear icon next to your glasses on the dashboard).  ","version":"Next","tagName":"h2"},{"title":"Install the Desktop app (Optional)​","type":1,"pageTitle":"Project Aria Glasses Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#install-the-desktop-app-optional","content":"You need the Desktop app to request Machine Perception Services, but you don't need the app to make recordings or download data. The app provides a visualization of streaming data over Wi-Fi, but it doesn't have the capabilities of the Client SDK and you can't subscribe to the data. Go to the Desktop app page for more information. Follow the instructions in the ARK SW Downloads and Updates page to download and install the app (this is where you'll download updates as well). ","version":"Next","tagName":"h2"},{"title":"Install Client SDK and CLI (optional)​","type":1,"pageTitle":"Project Aria Glasses Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#install-client-sdk-and-cli-optional","content":"Once you've set up your glasses with the Mobile Companion app, researchers can use the Client SDK or CLI to directly interact with Aria glasses. This is particularly helpful for streaming and subscribing to sensor data. Go to the Client SDK and CLI section for how to install the SDK and explore code samples. ","version":"Next","tagName":"h2"},{"title":"Record Data​","type":1,"pageTitle":"Project Aria Glasses Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#record-data","content":"It's best to start and stop recording using the Capture button, Mobile Companion app, the CLI or SDK. While you can start and stop recording using the Desktop app, one of the other methods will provide a smoother experience. ","version":"Next","tagName":"h2"},{"title":"General recording principles​","type":1,"pageTitle":"Project Aria Glasses Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#general-recording-principles","content":"If you start recording with one method, you can stop recording using a different methodPlease allow a few seconds before the recording starts. You'll know recording has started when the Recording LEDs turn onThe recording does not stop until the Recoding LED turns off Because the Aria glasses need to finish indexing before the recording stops, may be a small delay between initiating the stop and the recording stopping The longer your glasses record for, the longer it takes a recording to stop This is because the larger the VRS file, the more time it takes for a recording to finish indexing. Go to the Aria Glasses User Manual to see images of all the buttons as well as LED placement ","version":"Next","tagName":"h3"},{"title":"Press the Capture button​","type":1,"pageTitle":"Project Aria Glasses Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#press-the-capture-button","content":"The capture button is located on the top right of your glasses. The capture button will use the default recording profile. Press the capture button to start recordingPress the capture button to stop and save the recording tip Engaging the privacy switch instead of the Capture button will stop the recording and discard your recording. Discarded recordings cannot be retrieved. ","version":"Next","tagName":"h3"},{"title":"Mobile Companion app​","type":1,"pageTitle":"Project Aria Glasses Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#mobile-companion-app","content":"Select New Recording Session in the Aria Dashboard to start recordingSelect Stop Recording in the app Go to the Mobile Companion App page for more information You can alter the Name and Notes of a recording by going to the Recordings menu, selecting a recording and then selecting Edit. ","version":"Next","tagName":"h3"},{"title":"Download data​","type":1,"pageTitle":"Project Aria Glasses Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#download-data","content":"Aria recordings are stored as a VRS file and an accompanying JSON file that includes the recording's metadata. Metadata includes: The name and description of the recording as shown in the Mobile Companion AppThe recording profile usedWhat version of the Mobile Companion App was used to create the recording Aria recordings are directly accessible from the glasses' storage. There are three ways you can download data to your local machine using: MTPAria Desktop Companion AppADB commands We recommend using MTP for a faster download experience. ","version":"Next","tagName":"h2"},{"title":"Use MTP via File Explorer​","type":1,"pageTitle":"Project Aria Glasses Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#use-mtp-via-file-explorer","content":"Windows &amp; Linux​ When you plug in your Aria glasses to your computer, you can navigate to it as if it were any other USB external storage device. Plug your Aria glasses into your computer, using the supplied cable Please allow a few minutes for your glasses to be detected Your glasses will appear in your directory as an external drive called “Aria”Select Aria and then go to Internal Storage &gt; recordingIn this folder you will see the .vrs file and .json file that stores the .vrs files' metadata You'll also see a thumbnails folder, which contains the thumbnails that are used to provide previews of your content in the Mobile app Copy the data to local storage MacOS​ MTP is not provided natively on MacOS, but there are lightweight tools that you can use, such as OpenMTP. Download and install OpenMTPConnect your Aria glasses to your computerOpen OpenMTPDrag &amp; drop Aria recordings from Aria's internal storage ","version":"Next","tagName":"h3"},{"title":"Use the Desktop App​","type":1,"pageTitle":"Project Aria Glasses Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#use-the-desktop-app","content":"Go to the Recordings section in the Desktop app to download your data. See the Desktop App page for further information. ","version":"Next","tagName":"h3"},{"title":"Use ADB​","type":1,"pageTitle":"Project Aria Glasses Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#use-adb","content":"Android Debug Bridge (adb) is a command line tool that can be used with Aria glasses. To download all your data: adb pull /sdcard/recording /home/unixname/MyVRSFolder  To download a single VRS file adb pull /sdcard/recording/myVrsFile.vrs /home/unixname/MyVRSFolder/  To download a single metadata file adb pull /sdcard/recording/myVrsFile.json /home/unixname/MyVRSFolder/  ","version":"Next","tagName":"h3"},{"title":"Streaming​","type":1,"pageTitle":"Project Aria Glasses Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#streaming","content":"Install the CLient SDK to stream Aria data. ","version":"Next","tagName":"h2"},{"title":"Access basic visualization​","type":1,"pageTitle":"Project Aria Glasses Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#access-basic-visualization","content":"Through the Desktop app, you can view streaming data, however it does not have the capabilities of the Client SDK and you can't subscribe to the data. When using the Desktop app, we recommend only using Profiles 12 and 18, which are optimized for streaming. Go to the Desktop app page for detailed instructions. ","version":"Next","tagName":"h3"},{"title":"Project Aria Desktop Companion App","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/desktop_companion_app","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Project Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#overview","content":"The Project Aria Desktop Companion App, provides the ability to record, transfer, process, validate and visualize Aria's data through a desktop interface. These instructions are only useful if you have Project Aria glasses. ","version":"Next","tagName":"h2"},{"title":"Features​","type":1,"pageTitle":"Project Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#features","content":"Handle and select between multiple plugged in Aria glassesSet the default recording profile for recordings initiated by the capture button or Desktop appVisualize a live stream of Aria's sensorsDisplay, extract, validate Aria VRS dataAccess Machine Perception Services (MPS), a cloud-based service to process Aria VRS dataVisualize Aria Data as well as MPS outputs.Direct access to documentation and guides ","version":"Next","tagName":"h3"},{"title":"Before you start - Device OS Update​","type":1,"pageTitle":"Project Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#before-you-start---device-os-update","content":"caution Before using the Desktop app, you must use the Aria Mobile Companion App to update your Aria glasses' OS. ","version":"Next","tagName":"h2"},{"title":"To install​","type":1,"pageTitle":"Project Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#to-install","content":"Follow the instructions in the ARK SW Downloads and Updates page to download and install the app (this is where you'll download updates as well). ","version":"Next","tagName":"h2"},{"title":"Login​","type":1,"pageTitle":"Project Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#login","content":"Read the Project Aria Research Community Guidelines and select AcceptLog in using the credentials that were sent to you (the same used to log into the Mobile Companion App). ","version":"Next","tagName":"h2"},{"title":"Dashboard​","type":1,"pageTitle":"Project Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#dashboard","content":"After accepting the guidelines and logging in, you should see the Aria Dashboard. The dashboard displays the device info for your Aria glasses and allow you to interact with your glasses, if they are plugged in, as well as links to further information about Aria.  ","version":"Next","tagName":"h2"},{"title":"Pairing​","type":1,"pageTitle":"Project Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#pairing","content":"The Desktop App will detect devices that are plugged in via USB. Once plugged in, your device will display as Aria and as Active with its serial number visible in the list of devices next to My Device. If you cannot see your device, make sure you have updated the firmware using the Aria Mobile Companion app. ","version":"Next","tagName":"h3"},{"title":"Machine Perception Services (MPS)​","type":1,"pageTitle":"Project Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#machine-perception-services-mps","content":"Through the Desktop App you can request MPS for eligible recordings. To request MPS: Go to the Recordings TabSelect ToolsSelect Request MPS in the Recordings toolbar Go to Request MPS for more information. ","version":"Next","tagName":"h2"},{"title":"Use Desktop App as CLI​","type":1,"pageTitle":"Project Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#use-desktop-app-as-cli","content":"The Desktop App can be run directly from the command line without needing a GUI. It can be found under C:\\Program Files\\Aria\\v3\\AriaHub.exe on Windows and /Applications/Aria.app/Contents/MacOS/AriaHub on Mac. You may print the app's help using the AriaHub --help command. ","version":"Next","tagName":"h2"},{"title":"health​","type":1,"pageTitle":"Project Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#health","content":"Usage : AriaHub health vrsFilePath.vrs  Use this to run validity checks on an Aria recording (VRS file). These checks are also run on the VRS file automatically, before the file gets uploaded for MPS processing. The results of those checks can be found under your home directory under ./aria/logs ","version":"Next","tagName":"h3"},{"title":"vrs​","type":1,"pageTitle":"Project Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#vrs","content":"Usage : AriaHub vrs vrsFilePath.vrs  Use it as a Swiss army knife utility to manipulate VRS files in different ways. Go to the VRS official documentation for a full list of commands. ","version":"Next","tagName":"h3"},{"title":"Download your data​","type":1,"pageTitle":"Project Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#download-your-data","content":"Once you've completed recordings with your Aria Glasses you can access and download these recordings using the Desktop Companion app, if you wish. tip While you can download recordings using the Desktop app, we recommend using ADB or MTP. Go to the Quickstart Guide for more information. Every recording will generate a VRS file and a .vrs.json file containing the recording's metadata. To download a recording: Go to the Recordings pageYou'll see thumbnails of all the recordings on your Aria glassesSelect Pull to download the data from your glassesIn the file explorer window, select where you want to save the recordingOnce you've selected a folder location, a dialog window will pop up indicating the current VRS file transfer ","version":"Next","tagName":"h2"},{"title":"Visualize your data​","type":1,"pageTitle":"Project Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#visualize-your-data","content":"Once files have been copied to your local directory, the VRS file can be visualized using the Desktop App. In the Recordings page, select ToolsSelect Play VRSWhen the file explorer window appears, select the VRS file you want to playOnce you've selected a file to open, the VRS player window will pop up and start playing your recording Once the visualizer is open, use Open (Path or URI) or Select (Explorer window) to visualize other recordings ","version":"Next","tagName":"h2"},{"title":"Playback Controls​","type":1,"pageTitle":"Project Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#playback-controls","content":"Play/pause/stop playback​ The Previous and Next Frame buttons will play at most one frame backward or forward for each visible stream. The Speed controls let you chose to play slower or faster. If there is too much data for your system to process, frames will be dropped. Overlay Selection​ The overlay selector lets you chose what information to display over the frames. The options are: Hide - there is no overlayTags - show stream tagsConfiguration, State or Data - show the metadata found in the last record. ","version":"Next","tagName":"h3"},{"title":"Tooltips​","type":1,"pageTitle":"Project Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#tooltips","content":"To know the duration of the image data, use the tooltip found over the time display. Note that the start and end times show the time range in which image or audio data was found. Streams that don't contain image or audio data are ignored, and only data records from image and audio streams are considered. So if a recording contains a single image stream that has a configuration record at timestamp 0 rather than just before the first data record (as is too often the practice), while the first data record is at timestamp 15, the playback start time will be 15. ","version":"Next","tagName":"h3"},{"title":"Menu Bar Commands​","type":1,"pageTitle":"Project Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#menu-bar-commands","content":"The Menu Bar offers functionality available only there, don't forget to look there! The Layout menu's top section let you save and manage presets. Save your favorite stream display configurations, including stream orientation, stream order, which streams are visible or hidden, using the Save Preset command. To arrange streams, see the Context Menu section below. The commands in the lower section let you control in how many row of how many views the streams will be arranged. Layout Frames 4x2 means using 2 rows with up to 4 streams each. The layout configurations offered depend on the number of image streams visible. Once at least one preset has been saved, you can recall or delete presets, which automatically get a keyboard shortcut for quick access.  ","version":"Next","tagName":"h3"},{"title":"Keyboard Playback Controls​","type":1,"pageTitle":"Project Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#keyboard-playback-controls","content":"Playback can be directly controlled from the keyboard: Use the space bar to play/pause replay.The backspace and the home keys will reset playback to the start of the file, much like the Stop button.The left and right arrow keys will read at most one frame per stream, in either direction.The up and down arrow keys will jump at most 10 frames, in either direction.The page-up and page-down keys will jump at most 100 frames, in either direction. When using the arrow keys, all frames are guaranteed to be read. Use this feature if you want to be sure to view every frame of your file. ","version":"Next","tagName":"h3"},{"title":"Stream a basic visualization​","type":1,"pageTitle":"Project Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#stream-a-basic-visualization","content":"You can use the Desktop app to visualize sensor data from your Aria glasses stream data over Wi-Fi. This visualization does not have the capabilities of the Client SDK and you can't subscribe to the data. When using the Desktop app, we recommend only using Profiles 12 and 18, which are optimized for streaming. To stream data, your Aria glasses need to be on the same network as your computer. You will need to use a &quot;non-corporate&quot; network Corporate/Institution networks are often protected by many layers of security and firewalls which will impede you from streaming. If you are at home, please make sure you're not connected to a VPN. ","version":"Next","tagName":"h2"},{"title":"Wi-Fi connection via Mobile Companion App​","type":1,"pageTitle":"Project Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#wi-fi-connection-via-mobile-companion-app","content":"You will have connected your Aria device to Wi-Fi when you paired your device with the Mobile Companion App and updated it. Your computer and Aria device need to be on the same Wi-Fi network If you need to change the Wi-Fi network your Aria is connected to: Open Mobile Companion AppIn the Paired Glasses section of the Dashboard, select Select Wi-FiSelect your preferred network and follow the prompts to connect You can also forget an existing network from the Wi-Fi menuMake sure it is a non-corporate network that is the same as your computer Once connected, the Wi-Fi network name will appear in the Desktop App under My Device underneath the Wi-Fi icon which will become blue. ","version":"Next","tagName":"h3"},{"title":"Wi-Fi connection via Desktop App​","type":1,"pageTitle":"Project Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#wi-fi-connection-via-desktop-app","content":"To connect the Aria glasses to a Wi-Fi network Select Wi-Fi on the Dashboard device toolbar under My Device. Make sure it is a non-corporate network that is the same as your computer Select your desired network and enter its passwordSelect Connect to confirm your selected network ","version":"Next","tagName":"h3"},{"title":"Start Streaming​","type":1,"pageTitle":"Project Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#start-streaming","content":"To start streaming, select Streaming on the Dashboard device toolbar under My Device. The streaming session will then start ","version":"Next","tagName":"h3"},{"title":"Streaming Visualization​","type":1,"pageTitle":"Project Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#streaming-visualization","content":"The Aria stream window will pop up showing Aria's sensor live data. The nature of the visualized data will be determined by the chosen recording profile. We recommend only using Profiles 12 and 18, which are optimized for streaming. ","version":"Next","tagName":"h3"},{"title":"Stop Streaming​","type":1,"pageTitle":"Project Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#stop-streaming","content":"To stop streaming, select Stop on the same device toolbar Closing the Aria Stream window will not stop the stream. You will need to effectively click on the Stop button for the stream to stop. Appendix A - Recording using the Desktop app While we recommend recording using the Capture button or Mobile Companion app, here are the instructions for recording with the Desktop Companion app ","version":"Next","tagName":"h3"},{"title":"Select a Recording Profile​","type":1,"pageTitle":"Project Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#select-a-recording-profile","content":"Before you can record using the Desktop app, you'll need to set a recording profile. You can do this via the Desktop or Mobile Companion App. To set the recording profile via Desktop app, select Profile in the device toolbar. Whenever a profile is selected, its description will be shown underneath. Select OK to confirm your selected profile. The profile you select will also be the profile used when you initiate recording using the capture button on your Aria device. ","version":"Next","tagName":"h3"},{"title":"Start Recording​","type":1,"pageTitle":"Project Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#start-recording","content":"Plug your Aria device into your computer with the provided cable.In the Dashboard device toolbar under My Device, select Record.Once the recording has started you can unplug your Aria device ","version":"Next","tagName":"h3"},{"title":"Stop Recording​","type":1,"pageTitle":"Project Aria Desktop Companion App","url":"/projectaria_tools/docs/ARK/desktop_companion_app#stop-recording","content":"Plug your Aria device into your computerSelect Stop ","version":"Next","tagName":"h3"},{"title":"Project Aria Frame Sizes","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/frame_sizing","content":"","keywords":"","version":"Next"},{"title":"How to find out your Aria Frame Size​","type":1,"pageTitle":"Project Aria Frame Sizes","url":"/projectaria_tools/docs/ARK/frame_sizing#how-to-find-out-your-aria-frame-size","content":"","version":"Next","tagName":"h2"},{"title":"Option 1 - Order Head Sizing Kit​","type":1,"pageTitle":"Project Aria Frame Sizes","url":"/projectaria_tools/docs/ARK/frame_sizing#option-1---order-head-sizing-kit","content":"Ask your Project Aria contact to order you a Head Sizing kit. ","version":"Next","tagName":"h3"},{"title":"Option 2 - DIY Head Sizing​","type":1,"pageTitle":"Project Aria Frame Sizes","url":"/projectaria_tools/docs/ARK/frame_sizing#option-2---diy-head-sizing","content":"Place your head snugly between two objects and then measure the difference.   If it's under 152mm, order a Small. If it's 152mm or more order a Large ","version":"Next","tagName":"h3"},{"title":"Option 3 - Check the frame size of existing glasses/sunglasses​","type":1,"pageTitle":"Project Aria Frame Sizes","url":"/projectaria_tools/docs/ARK/frame_sizing#option-3---check-the-frame-size-of-existing-glassessunglasses","content":"This may be less accurate than other methods, given normal glasses have stiffer arms that don't fit snugly around your head. But it can get you going faster if you don't want to do DIY Frame Sizing. Find a pair of glasses or sunglasses that fit you comfortablyMeasure the distance between the two armsIf it's under 152mm, order a Small. If it's 152mm or more order a Large ","version":"Next","tagName":"h3"},{"title":"Aria Glasses Fit and Comfort","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/glasses_manual/fit_and_comfort","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Aria Glasses Fit and Comfort","url":"/projectaria_tools/docs/ARK/glasses_manual/fit_and_comfort#overview","content":"There are a few adjustments you can make to your Project Aria glasses to get a more comfortable fit. Your browser does not support the video tag. Video of a person adjusting Project Aria glasses temple arm tips and nosepads. ","version":"Next","tagName":"h2"},{"title":"Ideal Fit​","type":1,"pageTitle":"Aria Glasses Fit and Comfort","url":"/projectaria_tools/docs/ARK/glasses_manual/fit_and_comfort#ideal-fit","content":"Proper frame width: The device should match the overall width of your head. When too small, the device will be compressing on the sides of your head creating marks. When too large, the temple arms will feel too loose and not secure.Proper nose pad fit: The device should rest comfortably on your nose without slipping down or causing marks. The Nose pad bracket can be adjusted for proper angle and pad placement.Proper temple arm fit: Temple arms reach all the way behind your ears and flex adjusted to fit. Your frames should align evenly with your eyes, no higher than your eyebrows. For eye-tracking, ensure that your eyes (pupils) are approximately located in between the dotted lines as shown in the picture.  ","version":"Next","tagName":"h2"},{"title":"Nose Pad Adjustments​","type":1,"pageTitle":"Aria Glasses Fit and Comfort","url":"/projectaria_tools/docs/ARK/glasses_manual/fit_and_comfort#nose-pad-adjustments","content":"","version":"Next","tagName":"h2"},{"title":"The nose pads feel too tight or pinching on the nose?​","type":1,"pageTitle":"Aria Glasses Fit and Comfort","url":"/projectaria_tools/docs/ARK/glasses_manual/fit_and_comfort#the-nose-pads-feel-too-tight-or-pinching-on-the-nose","content":"Solution: Move pads further apart Securing the frame eye wire with your hands, use your thumbs to spread the pad apart carefully. Make small incremental adjustments until desired fit is achieved.Using your fingers, you can make small incremental adjustments on the pad angle by moving/turning the pad arm carefully. An ideal fit is achieved by making sure the full surface of the soft nose pad is evenly placed on each side of your nose bridge. Your browser does not support the video tag. Video of the nosepads of Aria glasses being gently pushed outwards. ","version":"Next","tagName":"h3"},{"title":"Nose pads are sitting too low on the nose, or feel loose?​","type":1,"pageTitle":"Aria Glasses Fit and Comfort","url":"/projectaria_tools/docs/ARK/glasses_manual/fit_and_comfort#nose-pads-are-sitting-too-low-on-the-nose-or-feel-loose","content":"Solution: Move pads closer to each other Use your thumbs to push the nose pads closer together until they fit snugly against either side of your nose, being careful to not damage the Device. Make small incremental adjustments until desired fit is achieved.Using your fingers, you can make small incremental adjustments on the pad angle by moving/turning the pad arm carefully.An ideal fit is achieved by making sure the full surface of the soft nose pad is evenly placed on each side of your nose bridge. Your browser does not support the video tag. Video of the nosepads of Aria glasses being gently pushed inwards. ","version":"Next","tagName":"h3"},{"title":"Flexible Temple Arm Tips​","type":1,"pageTitle":"Aria Glasses Fit and Comfort","url":"/projectaria_tools/docs/ARK/glasses_manual/fit_and_comfort#flexible-temple-arm-tips","content":"IMPORTANT! The flexible temple arm tips are restricted to horizontal inward/outward movement only. DO NOT move them vertically up or down. The ends of Project Aria Device arms are flexible, allowing you to make small adjustments to increase wearing comfort. Using your hands, you can bend the tips inward to make it tighter behind the ears or outward to make it looser. Your browser does not support the video tag. Video of the tips of Project Aria glasses being bent inwards and outwards. ","version":"Next","tagName":"h2"},{"title":"Ear Hooks​","type":1,"pageTitle":"Aria Glasses Fit and Comfort","url":"/projectaria_tools/docs/ARK/glasses_manual/fit_and_comfort#ear-hooks","content":"You can install Ear Hooks at the end of each temple arm tip, for a more secure and stable grip behind the ears. ","version":"Next","tagName":"h2"},{"title":"Project Aria Glasses User Manual","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual","content":"","keywords":"","version":"Next"},{"title":"Proper Handling and Cleaning​","type":1,"pageTitle":"Project Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#proper-handling-and-cleaning","content":"Handle the Glasses with care &amp; avoid damage. The Glasses are sensitive electronic equipment and should be handled with care.Do not try to bend the temples arms. Attempting to fold them like normal glasses will damage them.Do not drop, strike, or shake your Glasses excessively. Do not use the Glasses if they are damaged.The Glasses are not intended for use as safety glasses or eye protection.Do not use with other head mounted displays, such as virtual reality headsets or other glasses.Storage. Store the Glasses in a clean, dry, temperature-controlled environment. Do not leave in an unattended vehicle where temperatures may reach hot or cold extremes.Water &amp; liquids. Your Glasses are resistant to water splashes but are not designed for submersion in water or extended exposure to water or other liquids. If water exposure occurs, dry your Glasses thoroughly and clear the charging areas of residue or other debris.Cleaning. Clean using a microfiber cloth or a lens cleaning wipe. Do not soak, rinse, or submerge. ","version":"Next","tagName":"h2"},{"title":"You can't fold the glasses, but you can adjust the tips of the temple arms​","type":1,"pageTitle":"Project Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#you-cant-fold-the-glasses-but-you-can-adjust-the-tips-of-the-temple-arms","content":"Your browser does not support the video tag. Video of temple arm tips being adjusted. Go to the Fit and Comfort page for more tips about adjusting your glasses. ","version":"Next","tagName":"h3"},{"title":"Detailed Cleaning Instructions​","type":1,"pageTitle":"Project Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#detailed-cleaning-instructions","content":"","version":"Next","tagName":"h2"},{"title":"To sanitize​","type":1,"pageTitle":"Project Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#to-sanitize","content":"Use 70% pre moistened alcohol lens wipes with a light touch. ","version":"Next","tagName":"h3"},{"title":"To clean​","type":1,"pageTitle":"Project Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#to-clean","content":"Clean the eyeglass lenses with a dry microfiber cloth or with a traditional lens cleaner available at any optical store. Try your best to only use a microfiber cloth for cleaning. Wash the microfiber cloth with soap and water at least weekly and air dry to ensure cleanliness of cloth. Do not use any paper products such as paper towels or Kleenex as it will damage the anti-reflective coating on the lenses. If a microfiber cloth is not available, you can use a 100% soft cotton cloth to wipe your lenses. Be mindful not to get the camera lenses and sensors wet as much as possible. It's best to spray the cloth first (soap+water mixture ok to use too) to dampen then wipe the lenses with it. ","version":"Next","tagName":"h3"},{"title":"Project Aria Glasses​","type":1,"pageTitle":"Project Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#project-aria-glasses","content":"  ","version":"Next","tagName":"h2"},{"title":"Charging​","type":1,"pageTitle":"Project Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#charging","content":"Your Aria glasses must be connected to a charger to upload data. It connects to USB via its magnetic connector on the right arm. If you see the LED flashing red on the inside of the right temple arm, your battery is depleted.  ","version":"Next","tagName":"h3"},{"title":"Powering On​","type":1,"pageTitle":"Project Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#powering-on","content":"Connect your glasses to the charger to automatically turn them on. or Hold the power button for 3-5 seconds.Continue holding the button when you see the LED flash green once. You can release the button once you see the LED turn solid blue. Your browser does not support the video tag. Video of Project Aria glasses being turned on. ","version":"Next","tagName":"h3"},{"title":"Powering Off​","type":1,"pageTitle":"Project Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#powering-off","content":"Hold the power button for 5 seconds and release It may take several seconds for the LED to turn off. Your browser does not support the video tag. Video of Project Aria glasses being turned off. ","version":"Next","tagName":"h3"},{"title":"Privacy Switch​","type":1,"pageTitle":"Project Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#privacy-switch","content":"The Privacy Switch, located underneath the right arm, toggles privacy mode on/off. Privacy mode is enabled when the switch is toggled toward the crossed out camera (i.e., &quot;back&quot; position relative to the wearer). When the privacy switch is engaged, the Aria glasses cannot record data.Any active recordings will be stopped and deleted when the switch is engaged. Your browser does not support the video tag. Video of privacy switch being toggled on and off. ","version":"Next","tagName":"h3"},{"title":"Capture Button​","type":1,"pageTitle":"Project Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#capture-button","content":"Located over the right frame.Can be used to start and stop an active recording.The capture and privacy switch are used in combination if you need to factory reset your device. Please note, it may take several seconds for your recording to start or stop. The larger the recording, the longer it will take to finish indexing and stop the recording.  ","version":"Next","tagName":"h3"},{"title":"Proximity Sensor​","type":1,"pageTitle":"Project Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#proximity-sensor","content":"Senses when the Aria glasses are being worn.  ","version":"Next","tagName":"h3"},{"title":"LED States​","type":1,"pageTitle":"Project Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#led-states","content":"","version":"Next","tagName":"h2"},{"title":"Right Arm LED​","type":1,"pageTitle":"Project Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#right-arm-led","content":"Low Battery: Flashing RedRecovery Mode: Alternating Blue and Red (may look violet because of the alternating speed)Powering On: Flashing Green oncePower On: Solid BluePower Off: OffCharge Loop: Solid Red (when the battery is too low to boot successfully)Data Uploading: Pulsing Blue 3 times, repeating this pattern (reminder, your device must be connected to power to upload data)  ","version":"Next","tagName":"h3"},{"title":"User Facing LED​","type":1,"pageTitle":"Project Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#user-facing-led","content":"Recording in Progress: Solid WhiteThermal Mitigation Reached: Solid Red, may appear Orange (when the Device temperature is too high) ","version":"Next","tagName":"h3"},{"title":"Bystander Facing LED​","type":1,"pageTitle":"Project Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#bystander-facing-led","content":"Recording in Progress: Solid White  ","version":"Next","tagName":"h3"},{"title":"Power Cycle/Reboot​","type":1,"pageTitle":"Project Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#power-cyclereboot","content":"Power cycle your device by holding down the power button down for about 10 seconds, until the nearby LED flashes green once and returns to solid blue. ","version":"Next","tagName":"h3"},{"title":"Factory Reset​","type":1,"pageTitle":"Project Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#factory-reset","content":"caution This will delete any recordings on your glasses. Engage the Privacy Switch (i.e. in the recordings disabled position).Tap (press but do not hold) the power button and the capture button at the same time.The device will reboot. After a while, you will see the LED near the power button flashing purple. Your browser does not support the video tag. Video of Project Aria glasses being factory reset. ","version":"Next","tagName":"h2"},{"title":"Device ID​","type":1,"pageTitle":"Project Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#device-id","content":"Aria glasses have a unique device_ID. Unlike the serial number, the device_ID is reset every time the device is a factory reset or the glasses are unpaired. Aria glasses must go through a factory reset before they can be paired to a new user, so the device_ID will always be associated with a single user account (although an individual user may have multiple devices associated with their account). Note: disconnecting your glasses does not trigger a factory reset, only unpairing them so that a new user may use the glasses. The serial number persists and is tied to the device, like any other product. You can find the Device ID for your glasses in the Mobile Companion App Device Settings (select the settings/gear icon next to your glasses on the dashboard). Health and Safety Information In addition to the Health and Safety information provided with your welcome kit, you can also read Aria Glasses health and safety information in the Mobile companion app: Open the Aria companion app on your phoneSelect SettingsSelect Health &amp; Safety ","version":"Next","tagName":"h2"},{"title":"Project Aria Glasses Recording Profile Guide","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/glasses_manual/profile_guide","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Project Aria Glasses Recording Profile Guide","url":"/projectaria_tools/docs/ARK/glasses_manual/profile_guide#overview","content":"This page provides an overview of Project Aria sensor recording profiles that are commonly used when collecting data with a Project Aria device, as well as the recording profiles that work with Machine Perception Services (MPS). Go to the Recording Profiles in Technical Specifications for more detailed information about each profile. Recording profiles enable researchers choosing which sensors to record with as well as what settings to use. Settings options include what camera resolution to use and whether the output is RAW (no encoding) or JPEG (compressed). If you would like more details about any recording profile and have a Project Aria device: Open the Mobile Companion App and select New RecordingSelect Profile and scroll down to see the sensor configuration ","version":"Next","tagName":"h2"},{"title":"General Guidance​","type":1,"pageTitle":"Project Aria Glasses Recording Profile Guide","url":"/projectaria_tools/docs/ARK/glasses_manual/profile_guide#general-guidance","content":"These are some sensor profiles researchers have found useful for particular kinds of research. Check recording profiles on your Mobile Companion App for more details. ","version":"Next","tagName":"h2"},{"title":"If you’re not sure what you want​","type":1,"pageTitle":"Project Aria Glasses Recording Profile Guide","url":"/projectaria_tools/docs/ARK/glasses_manual/profile_guide#if-youre-not-sure-what-you-want","content":"Profile 10 is interesting to explore, it gathers data with all sensors and the RGB Camera records at 10 fps. All sensor data is useful for exploring multimodal ML models. If you need high RGB Resolution (2880x2880 rather than 1408x1408), and 1FPS is sufficient shutter speed, use Profile 0. ","version":"Next","tagName":"h3"},{"title":"If you're streaming data​","type":1,"pageTitle":"Project Aria Glasses Recording Profile Guide","url":"/projectaria_tools/docs/ARK/glasses_manual/profile_guide#if-youre-streaming-data","content":"While you can use any recording profile when streaming, we recommend only using Profiles 12 and 18, which are optimized for streaming. ","version":"Next","tagName":"h3"},{"title":"If you need a high frame rate​","type":1,"pageTitle":"Project Aria Glasses Recording Profile Guide","url":"/projectaria_tools/docs/ARK/glasses_manual/profile_guide#if-you-need-a-high-frame-rate","content":"Use profiles 2, 9 or 15, depending on whether you want EyeTracking or GPS. Profile 2 does not have ET, profile 15 does not have GPS. ","version":"Next","tagName":"h3"},{"title":"If your research focuses on audio​","type":1,"pageTitle":"Project Aria Glasses Recording Profile Guide","url":"/projectaria_tools/docs/ARK/glasses_manual/profile_guide#if-your-research-focuses-on-audio","content":"Try profiles 4, 7 (no SLAM) or profile 10. ","version":"Next","tagName":"h3"},{"title":"If your research focuses on EyeTracking(ET)​","type":1,"pageTitle":"Project Aria Glasses Recording Profile Guide","url":"/projectaria_tools/docs/ARK/glasses_manual/profile_guide#if-your-research-focuses-on-eyetrackinget","content":"Try profile 5 for high resolution ET recordings at 20fps. ","version":"Next","tagName":"h3"},{"title":"To avoid image pre-processing​","type":1,"pageTitle":"Project Aria Glasses Recording Profile Guide","url":"/projectaria_tools/docs/ARK/glasses_manual/profile_guide#to-avoid-image-pre-processing","content":"In situations where you want to use RAW videos and skip the Image Sensor Processor (ISP) as much as possible, profile 7 is helpful. Please note, because profile 7 delivers RAW image files, not JPEGs the data is 8x more costly to store. This profile also uses more energy while recording and may heat up faster than others. ","version":"Next","tagName":"h3"},{"title":"Recording Profiles That Support MPS​","type":1,"pageTitle":"Project Aria Glasses Recording Profile Guide","url":"/projectaria_tools/docs/ARK/glasses_manual/profile_guide#recording-profiles-that-support-mps","content":"The table below shows which recording profiles have the necessary data to generate Trajectory or Eye Gaze data using MPS. More commonly used profiles are marked with bold text. Aria Recording Profile\tDescription\tTraj\tETProfile0\tDefault (all sensors)\tYes\tYes Profile2\tRGB and SLAM high frame rate\tYes\tNo Profile5\tEye tracking calibration (high res)\tNo\tYes Profile8\tNoise and Hearing mode\tYes\tYes Profile9\tContextualized AI\tYes\tNo Profile10\tAll sensors enabled with low fps\tYes\tYes Profile12\tStreaming mode with JPEG\tNo\tYes Profile14\tSLAM and ET high frame rate with RGB low frame rate\tYes\tYes Profile15\tAR Replay\tYes\tYes Profile16\tEye tracking high frame rate\tNo\tYes Profile17\tStreaming mode with H.265\tNo\tYes Profile18\tStreaming mode with JPEG and Spatial Audio\tNo\tYes Profile19\tDual capture\tYes\tNo Profile21\tNoise and hearing mode with high RGB fps (RGB 15fps 2MP, SLAM 15fps VGA, ET 30fps QVGA; Audio on; GPS, Wi-Fi and BLE off)\tNo\tYes ","version":"Next","tagName":"h2"},{"title":"Project Aria Mobile Companion App","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/mobile_companion_app","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Project Aria Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#overview","content":"The Project Aria Mobile Companion App, provides the ability to interact &amp; record with your Aria glasses via Bluetooth. This section covers: Getting Started (Update Your Glasses)RecordingMobile Companion App Screens These instructions are only useful if you have access to Aria glasses. Go to projectaria.com to find out how to become an academic research partner and gain access to the Aria Research Kit (ARK). ","version":"Next","tagName":"h2"},{"title":"Mobile Companion App features include:​","type":1,"pageTitle":"Project Aria Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#mobile-companion-app-features-include","content":"Fully wirelessCheck your glasses status (temperature, GPS, privacy switch etc..)Handle and select between multiple paired Aria glassesUpdate Aria glasses to the latest OS buildSelect a recording profile and start recording directly from the mobile appSet default recording profileData quality signals while recordingIn-session Eye Gaze CalibrationAccept, store and delete security certificates to enable the Client SDK and CLI Getting Started (Update Your Glasses) ","version":"Next","tagName":"h2"},{"title":"Download and Install​","type":1,"pageTitle":"Project Aria Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#download-and-install","content":"Follow the instructions in the ARK SW Downloads and Updates page to download and install the app (this is where you'll download updates as well). ","version":"Next","tagName":"h2"},{"title":"Sign in and pairing​","type":1,"pageTitle":"Project Aria Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#sign-in-and-pairing","content":"Plug your Project Aria glasses into their charger This will automatically turn your glasses Make sure the Privacy Switch is not engaged The Privacy Switch should be pushed forwards, towards the lenses Open the Companion app for the first time and sign in using the log in we gave you in your welcome email When launching the Companion App for the first time, you'll need to grant it certain permissions to work correctly, such as location services.Follow the prompts to agree to Project Aria Research Community Guidelines and read the Health and Safety informationSelect Get Started to begin setting up your glasses.The app will begin to look for nearby Project Aria glasses. When your glasses are discovered, they will be listed at the top of the screen alongside its serial number.After selecting your glasses, the Companion App will begin pairing with it.Once pairing completes, the app will ask to name your Aria glasses.Join a Wi-Fi network, your glasses must be plugged into a charger to complete the setup process.Once connected to Wi-Fi, the glasses will look for updates and update your glasses' OSOnce you have completed setup, you’ll see the Companion App Dashboard Page ","version":"Next","tagName":"h2"},{"title":"Set Default Recording Profile​","type":1,"pageTitle":"Project Aria Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#set-default-recording-profile","content":"The default recording profile determines determines which recording profile the New Recordings page starts with, as well as the profile used when initiating recording via the capture button or Desktop app. You can set the default recording profile via Mobile or Desktop Companion App. In the Mobile Companion App, you can change the Default Recording Profile at any time via Device Settings (select the settings/gear icon next to your glasses on the dashboard).  ","version":"Next","tagName":"h2"},{"title":"OS Update​","type":1,"pageTitle":"Project Aria Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#os-update","content":"Your Aria glasses's OS will also automatically update when plugged into power and connected to Wi-Fi with an internet connection. To manually update Aria glasses OS: In the Mobile Companion App, select Device SettingsScroll down to view your Aria Device's OS VersionSelect Check for UpdatesOnce your glasses have finished updating, they will reboot and the update will be complete. Pairing additional glasses Each pair of Aria glasses can only be paired with a single account. Multiple glasses can be paired with the one account, however. Plug your Project Aria glasses into their charger This will automatically turn your glasses Make sure the Privacy Switch is not engaged The Privacy Switch should be pushed forwards, towards the lenses Select the Add Glasses on the Aria dashboard. The app will then start looking for nearby Aria glasses.Follow steps 7-12 in Sign in and PairingSet the default recording profile for your glasses ","version":"Next","tagName":"h2"},{"title":"Terminology​","type":1,"pageTitle":"Project Aria Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#terminology","content":"","version":"Next","tagName":"h2"},{"title":"Pairing/Unpairing​","type":1,"pageTitle":"Project Aria Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#pairingunpairing","content":"Pair(ed): connecting Aria glasses to the Companion App for the first time.Unpair(ed): the process of unpairing Aria glasses from your Companion App. This will delete all data stored locally on the glasses. ","version":"Next","tagName":"h3"},{"title":"Connecting/Disconnecting​","type":1,"pageTitle":"Project Aria Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#connectingdisconnecting","content":"Connect(ed): is when the Companion App has active control over the glasses.Disconnect(ed): is when the Companion App no longer has active control over the glasses. ","version":"Next","tagName":"h3"},{"title":"Recording​","type":1,"pageTitle":"Project Aria Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#recording","content":"","version":"Next","tagName":"h2"},{"title":"To start recording​","type":1,"pageTitle":"Project Aria Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#to-start-recording","content":"Select New Recording Session on the Aria Dashboard.You have the option to configure your recording session before it begins: Name (optional): This will define the name of your VRS file. If you do not provide a name a random alphanumeric name will be givenNotes (optional): Notes are appended to your recordings ID. You can input a short or long text string in this field. Notes is a value that is provided in the vrs.json file associated with your recording.Sensors Used: Select Sensors Used to see details of the recording profile. From the Sensors Used menu, select Profile to choose different recording profiles to record with.If you’ve selected a default recording profile, that profile will be automatically populated, otherwise you will need to select a recording profile. Select Begin new recording While you're recording the Sensor Status field will let you know if there are any data quality issues Perform in-session Eye Gaze Calibration (optional) You only need to do this if you want Eye Gaze MPS with Calibrated dataGo To Eye Gaze Calibration for more information End your recording by selecting Complete Recording in the app or use the capture button on the top right side of the glasses Once you've completed or discarded a recording, you'll return to the New Recording screen, pre-populated with your previous details. ","version":"Next","tagName":"h3"},{"title":"Mobile Companion App Screens​","type":1,"pageTitle":"Project Aria Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#mobile-companion-app-screens","content":"","version":"Next","tagName":"h2"},{"title":"Dashboard​","type":1,"pageTitle":"Project Aria Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#dashboard","content":"The Aria Dashboard has several interactive elements: Tap New Recording Session to make a recordingTap Add glasses to pair additional devices with your Aria glasses One account can have multiple Aria glasses associated it, but each pair of glasses can only be associated with one accountMultiple glasses can be paired, but only one pair of glasses can be connected at a time Tap Select glasses to toggle between multiple paired Aria glasses or pair with new glasses This will replace Add glasses if more than one set of glasses is paired Tap the gear icon to go to Device SettingsTap the Bluetooth icon to disconnect your glassesTap the Wi-Fi icon to connect to different Wi-Fi network, forget a network or see the glasses' IP address  ","version":"Next","tagName":"h3"},{"title":"Aria Device Settings​","type":1,"pageTitle":"Project Aria Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#aria-device-settings","content":"On the main Aria Dashboard in the Mobile app, select the settings (gear) icon next to your Aria glasses to view your Aria glasses' settings.  The Device Settings page includes: Bluetooth, Wi-Fi, Battery, GPS and Temperature statusOS version and ability to check for OS updatesRemaining storage spaceMAC AddressSerial numberDevice IDCheck Device Mode (it should say Partner)Change the default recording profile ","version":"Next","tagName":"h3"},{"title":"Recordings menu​","type":1,"pageTitle":"Project Aria Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#recordings-menu","content":"In the Recordings menu, you'll be able to see recordings that are currently on your Aria glasses. Select a recording in the Recording menu to edit your recording's name and notes (notes will be stored in the vrs.json file) and see a range of details including: Recording durationRecording profile usedUp to 10 thumbnail images from your recording If you have not given your recording a name (such as when you initiate recording via the capture button) the VRS file will have a random alphanumeric name when it is downloaded. To name or edit the name of a VRS recording on your glasses: Go to the recordings tab of the Mobile appSelect a recordingSelect Edit on the top right corner ","version":"Next","tagName":"h3"},{"title":"Settings menu​","type":1,"pageTitle":"Project Aria Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#settings-menu","content":"The Settings menu shows the settings for the Aria app, not your Aria glasses. You'll be able to see the App version, but not the Aria glasses version. In the Advanced Settings menu there is the option to Clear Local Data, which can be helpful if you encounter issues that restarting your Mobile App does not resolve. Aria recordings are stored on the glasses, not on the phone, so it will not delete any of your recordings. ","version":"Next","tagName":"h3"},{"title":"Project Aria Machine Perception Services","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/mps","content":"","keywords":"","version":"Next"},{"title":"Current MPS offerings​","type":1,"pageTitle":"Project Aria Machine Perception Services","url":"/projectaria_tools/docs/ARK/mps#current-mps-offerings","content":"The following MPS can be requested, as long as the data has been recorded with a compatible Recording Profile. The Recording Profile Guide provides a quick list of compatible sensor profile,s or go to Recording Profiles in Technical Specifications for more granular information about each profile. MPS offerings are grouped into SLAM services (called Trajectory in the Desktop app) and Eye Gaze services. ","version":"Next","tagName":"h2"},{"title":"SLAM services​","type":1,"pageTitle":"Project Aria Machine Perception Services","url":"/projectaria_tools/docs/ARK/mps#slam-services","content":"To get these outputs the recording profile must have SLAM cameras + IMU enabled. ","version":"Next","tagName":"h2"},{"title":"6DoF trajectory​","type":1,"pageTitle":"Project Aria Machine Perception Services","url":"/projectaria_tools/docs/ARK/mps#6dof-trajectory","content":"MPS provides two types of high frequency (1kHz) trajectories: Open loop trajectory - local odometry estimation from visual-inertial odometry (VIO)Closed loop trajectory - created via batch optimization, using multi-sensors' input (SLAM, IMU, barometer, Wi-Fi and GPS), fully optimized and provides poses in a consistent frame of reference. ","version":"Next","tagName":"h3"},{"title":"Semi-dense point cloud​","type":1,"pageTitle":"Project Aria Machine Perception Services","url":"/projectaria_tools/docs/ARK/mps#semi-dense-point-cloud","content":"Semi-dense point cloud data supports researchers who need static scene 3D reconstructions, reliable 2D images tracks or a representative visualization of the environment. ","version":"Next","tagName":"h3"},{"title":"Online sensor calibration​","type":1,"pageTitle":"Project Aria Machine Perception Services","url":"/projectaria_tools/docs/ARK/mps#online-sensor-calibration","content":"The time-varying intrinsic and extrinsic calibrations of cameras and IMUs are estimated at the frequency of the SLAM (mono scene) cameras by our multi-sensor state estimation pipeline. ","version":"Next","tagName":"h3"},{"title":"Multi-SLAM​","type":1,"pageTitle":"Project Aria Machine Perception Services","url":"/projectaria_tools/docs/ARK/mps#multi-slam","content":"Multi-SLAM can be requested on two or more recordings. It creates all of the above SLAM output, in a shared co-ordinate frame. Multi-SLAM can only be requested using the MPS CLI SDK. ","version":"Next","tagName":"h3"},{"title":"Eye Gaze data​","type":1,"pageTitle":"Project Aria Machine Perception Services","url":"/projectaria_tools/docs/ARK/mps#eye-gaze-data","content":"Eye gaze is one of the most important indicators of human attention, eye gaze direction estimation with uncertainty is provided by MPS. Eye gaze estimation uses the data from the Eye Tracking (ET) cameras. These outputs can be generated for any recording that had ET cameras enabled. If you have made a recording with In-Session Eye Gaze Calibration, you will receive a second .csv file with calibrated eye gaze outputs. ","version":"Next","tagName":"h2"},{"title":"About MPS Data Loader APIs​","type":1,"pageTitle":"Project Aria Machine Perception Services","url":"/projectaria_tools/docs/ARK/mps#about-mps-data-loader-apis","content":"Please refer to our MPS data loader APIs (C++ and Python support) to load the MPS outputs into your application. The visualization guide shows how to visualize all the MPS outputs. ","version":"Next","tagName":"h2"},{"title":"Questions & Feedback​","type":1,"pageTitle":"Project Aria Machine Perception Services","url":"/projectaria_tools/docs/ARK/mps#questions--feedback","content":"If you have feedback you'd like to provide, be it overall trends and experiences or where we can improve, we'd love to hear from you. Go to our Support page for different ways to get in touch. ","version":"Next","tagName":"h2"},{"title":"Project Aria In-Session Eye Gaze Calibration","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/mps/eye_gaze_calibration","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Project Aria In-Session Eye Gaze Calibration","url":"/projectaria_tools/docs/ARK/mps/eye_gaze_calibration#overview","content":"This page provides an overview of what In-Session Eye Gaze Calibration does, and how to collect In-Session Eye Gaze Calibration with Project Aria glasses. Not required to request Eye Gaze MPS In-Session Eye Gaze Calibration provides an additional personalized Eye Gaze output, but is not required for requesting Eye Gaze MPS. ","version":"Next","tagName":"h2"},{"title":"What In-Session Eye Gaze Calibration does​","type":1,"pageTitle":"Project Aria In-Session Eye Gaze Calibration","url":"/projectaria_tools/docs/ARK/mps/eye_gaze_calibration#what-in-session-eye-gaze-calibration-does","content":"In-Session Eye Gaze Calibration enables researchers to improve the eye gaze estimations in Eye Gaze MPS outputs, enabling researchers to more accurately determine where wearers are looking during the recordings. When you request Eye Gaze Machine Perception Services (MPS) and the file has an in-session Eye Gaze Calibration as part of the VRS file, you will receive two outputs: general_eye_gaze.csv - based on the standard eye gaze configurationpersonalized_eye_gaze.csv - personalized eye gaze data based on the calibration data collected in the recording Every person is unique in terms of how they move their eyes and look at objects, so the personalized_eye_gaze estimation is expected to be more accurate for the individual. Further resources: Eye Gaze Data FormatsHow to Request MPSRecording Profiles That Support MPSMachine Perception Services Tutorial - includes sample output from a recording where good in-session Eye Gaze Calibration data was collected note Eye gaze calibration is not the same as Aria device calibration. For information about Aria device calibration, go to Project Aria Device Calibration. ","version":"Next","tagName":"h2"},{"title":"HW & SW Requirements​","type":1,"pageTitle":"Project Aria In-Session Eye Gaze Calibration","url":"/projectaria_tools/docs/ARK/mps/eye_gaze_calibration#hw--sw-requirements","content":"","version":"Next","tagName":"h2"},{"title":"Android​","type":1,"pageTitle":"Project Aria In-Session Eye Gaze Calibration","url":"/projectaria_tools/docs/ARK/mps/eye_gaze_calibration#android","content":"Hardware​ ARCore Depth API (https://developers.google.com/ar/devices) support Software​ Google Play Services for AR (https://play.google.com/store/apps/details?id=com.google.ar.core) For phones that have ARCore Depth API capabilities, it's normally installed by defaultYou'll see the error message &quot;Please check that Google Play Services for AR is installed&quot; if you don't have ARCore Depth API installed. ","version":"Next","tagName":"h3"},{"title":"iOS​","type":1,"pageTitle":"Project Aria In-Session Eye Gaze Calibration","url":"/projectaria_tools/docs/ARK/mps/eye_gaze_calibration#ios","content":"Hardware​ TrueDepth camera (iPhone X or later) Software​ iOS 14 or later ","version":"Next","tagName":"h3"},{"title":"How to Collect In-Session Eye Gaze Calibration​","type":1,"pageTitle":"Project Aria In-Session Eye Gaze Calibration","url":"/projectaria_tools/docs/ARK/mps/eye_gaze_calibration#how-to-collect-in-session-eye-gaze-calibration","content":"In-Session Eye Gaze Calibration can only be initiated via the Mobile Companion App. Follow these steps for any recording where you may wish to generate Calibrated Eye Gaze MPS. In the Mobile Companion app, create a new recording using a profile that includes ET and RGB cameras (such as Profile 15 or 25)Once your recording has started, close the recording window Select X on the top left of the screen Go to Device Settings (select the gear next to your glasses)Select Eye Tracking CalibrationConfirm that you’d like to run it during the current recording sessionFollow the prompts to calibrate your glasses ","version":"Next","tagName":"h2"},{"title":"Multiple Users Within the One Recording​","type":1,"pageTitle":"Project Aria In-Session Eye Gaze Calibration","url":"/projectaria_tools/docs/ARK/mps/eye_gaze_calibration#multiple-users-within-the-one-recording","content":"It’s possible for multiple users to do an in-session calibration within the one recording. When a new user gets the glasses, the first thing they should do is the in-session Eye Tracking Calibration. See Eye Gaze Data Format for how multiple users are tracked. ","version":"Next","tagName":"h3"},{"title":"Eye Gaze Calibration tips​","type":1,"pageTitle":"Project Aria In-Session Eye Gaze Calibration","url":"/projectaria_tools/docs/ARK/mps/eye_gaze_calibration#eye-gaze-calibration-tips","content":"","version":"Next","tagName":"h2"},{"title":"Things to avoid​","type":1,"pageTitle":"Project Aria In-Session Eye Gaze Calibration","url":"/projectaria_tools/docs/ARK/mps/eye_gaze_calibration#things-to-avoid","content":"❌ Do not wear a face covering during eye calibration. ❌ Choose an area with ample and even lighting; do not face a bright light, window or reflective surface. ❌ Do not set your phone screen brightness too high compared to your surroundings. ❌ Do not fully extend your arm(s) during eye calibration. Your elbows should be bent so that the phone is roughly 1 ft (30 cm) away from your face. ","version":"Next","tagName":"h3"},{"title":"Things to do​","type":1,"pageTitle":"Project Aria In-Session Eye Gaze Calibration","url":"/projectaria_tools/docs/ARK/mps/eye_gaze_calibration#things-to-do","content":"✅ The phone should be held straight in front of your face, so that you shouldn't look up or down to see the screen. Hold the phone plumb (90 degrees) vertically to the ground.  ✅ The &quot;Leveler&quot; stage appears if the position of your phone isn't within specifications for the calibration process. Adjust the phone in front of you and its distance by bending your elbow until the smaller, black circle turns into a green disk with a check mark. ✅ Once the &quot;Leveler&quot; stage is successfully completed, do your best to keep your phone in exactly the same position throughout the full eye calibration process. If your phone is moved to a position no longer suited to calibrate your device, the app will return to the &quot;Leveler&quot; stage. ✅ During eye calibration stages 1 to 10, move your nose towards the direction indicated by the arrow. If you're only following the direction with your gaze without moving your head, the calibration stage will time out and fail. However, please make sure to keep your eyes fixed on the number within the dot the whole time.  ","version":"Next","tagName":"h3"},{"title":"MPS Data Lifecycle","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/mps/mps_processing","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"MPS Data Lifecycle","url":"/projectaria_tools/docs/ARK/mps/mps_processing#overview","content":"Researchers can upload data collected by Project Aria glasses to Meta for cloud-based Machine Perception Services (MPS) processing. This page provides information about how all Aria sequences submitted to Meta for MPS are processed, handled and stored. Go to Machine Perception Services to find out more about the dataGo to How to Request MPS for how to get your data processed ","version":"Next","tagName":"h2"},{"title":"How sequences are processed​","type":1,"pageTitle":"MPS Data Lifecycle","url":"/projectaria_tools/docs/ARK/mps/mps_processing#how-sequences-are-processed","content":"Raw Aria sequences (VRS files) are uploaded to secure cloud storage via the Desktop Companion appThe data is only uploaded to Meta servers to serve MPS requests and is deleted after 30 daysThe MPS output is saved in the cloud User account that requested the MPS gets the token necessary to access MPS outputsThis derived data is persisted in the cloud Raw data is deleted from the cloud Meta’s data management processes mandate that this raw data cannot be stored for more than 30 days Figure 1: MPS Processing Lifecycle ","version":"Next","tagName":"h2"},{"title":"Data storage and use​","type":1,"pageTitle":"MPS Data Lifecycle","url":"/projectaria_tools/docs/ARK/mps/mps_processing#data-storage-and-use","content":"Partner data is only used to serve MPS requests. Partner data is not available to Meta researchers or Meta’s affiliates.Raw partner data (VRS files) is stored for no more than 30 days.The whole process is automated and only engineers in the core MPS team can access the pipeline.All MPS output (trajectories, semi-dense point clouds, gaze vectors etc.) continue to be stored in secure cloud storage, so that users can re-download the data at any time. MPS output is not available to Meta researchers or Meta’s affiliates. Only the user account that requested the MPS output gets the token necessary to download the derived dataOur goal is for this data to always be available to the user who requested it, however, if the Desktop App’s local cache is cleared you may no longer have the token necessary to access the data. The MPS pipeline generates statistics about how the algorithms are performing as well as the console logs from processing. These aggregated statistics are used by the Project Aria MPS team to help improve our offerings. ","version":"Next","tagName":"h2"},{"title":"Request MPS Using the Desktop Companion App","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/mps/request_mps/desktop_mps","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Request MPS Using the Desktop Companion App","url":"/projectaria_tools/docs/ARK/mps/request_mps/desktop_mps#overview","content":"This page provides information about how academic research partners who have access to the Aria Research Kit (ARK) can request Machine Perception Services (MPS) using the Desktop Companion App. Try the MPS CLI! We recommend using the MPS CLI to submit requests as it contains additional capabilities, such as resumable uploads and Multi-SLAM MPS  ","version":"Next","tagName":"h2"},{"title":"How to Request MPS​","type":1,"pageTitle":"Request MPS Using the Desktop Companion App","url":"/projectaria_tools/docs/ARK/mps/request_mps/desktop_mps#how-to-request-mps","content":"To request MPS processing on Aria VRS files: Go to the Recordings section of the Desktop App and select VRS ToolsSelect Request MPS in the Recordings toolbarSelect the VRS file from the native file explorerSelect what types of MPS you'd like to requestSelect UploadYour file's progress and the downloadable MPS outputs will become visible under Remote Uploads It may take several minutes for your file to appear in the Uploads tab, especially if it is large Once your data has been successfully processed, you can download the derived data zip file, from the Recordings toolbar. Please refer to the MPS Loader APIs on how to load the output in Python/C++ and Python visualization, C++ Visualization guides. on how to run the rich visualization tools to visualize all the MPS outputs. ","version":"Next","tagName":"h2"},{"title":"Status Types​","type":1,"pageTitle":"Request MPS Using the Desktop Companion App","url":"/projectaria_tools/docs/ARK/mps/request_mps/desktop_mps#status-types","content":"How long it takes for a file to progress from one state to the next will depend on how big the file is and how many other requests are being processed at that time. Unrequested: This type of derived data was not requestedWaiting: MPS request has been received and is in the queue for processingProcessing: This data is currently being processedComplete: Data Processing is Complete, you can now download your filesError: VRS file did not have the data required for processing, please use a Supported Recording Profile.An error occurred while processing data with a supported recording profile. There are many variables that can impact MPS output. For Trajectory data you can download the summary.json file to get more information. See MPS Trajectory Documentation for more information.If you've checked your recording profile and that is not the issue and you are unable to debug the issue using summary.json, please emailAriaOps@meta.comwith a bug report. Make sure you include the Transaction ID in your report and summary.json if you have one. Screenshots and screen recordings are always welcome. ","version":"Next","tagName":"h2"},{"title":"MPS CLI","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"MPS CLI","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli#overview","content":"The Project Aria MPS Command Line Interface (MPS CLI) is part of Project Aria Tools and it is the preferred way to request Machine Perception Services (MPS). While research partners can use the Desktop Companion app to request MPS, the MPS CLI provides more features and uploads can resume if interrupted. MPS CLI features: Request SLAM (trajectory, point cloud, online sensor calibration) and eye gaze dataRequest multi-sequence SLAM dataAuto run health checks prior to upload Recordings will not be uploaded if they are not valid for any of the MPS services requested Resumable uploads Uploads, interrupted for any reason, can be resumed within 24 hrsParticularly useful for large VRS files Concurrent processing CLI takes a list of directories/files as input and recursively finds all .vrs files to process concurrentlyThe number of concurrent processes for each stage can be modified in Settings Automatically downloads outputs once processing is completeRecordings are processed once If the output directory already exists and contains the MPS results, processing is skippedIf the recording was processed in the past, the results will be downloaded without submitting the recording for processing again (unless --force is passed as an input) Uploaded data is stored for 30 days Additional MPS can be requested without needing to upload againData can be reprocessed without needing to upload again CLI can be integrated into your automated workflows This includes ensuring that all commands and processes work without using the UISee the Command Line Reference for more details CLI documentation: Getting StartedMPS CLI Guide Further resources: About Project Aria Machine Perception ServicesMPS Data FormatsVisualize MPS Using PythonVisualize MPS Using C++ ","version":"Next","tagName":"h2"},{"title":"Getting Started With the MPS CLI","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_getting_started","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Getting Started With the MPS CLI","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_getting_started#overview","content":"The Project Aria Machine Perceptions Services Command Line Interface (MPS CLI) is a command line tool used to request and receive Machine Perception Services. While you can use the Desktop Companion app to request MPS, the MPS CLI provides more features and more robust file uploading. This page provides basic information to get you started with the MPS CLI, go to the MPS CLI Guide for more details. The MPS inputs can be a file or directory, and multiple inputs can be listed in a single command. The MPS CLI has two modes: Single Process each recording individuallyOutput is always saved next to the input VRS fileThe most common way to request MPS Multi Process multiple recordings to generate multi-sequence SLAM dataUser must provide a directory for the outputs Non-UI options available This tutorial uses the the MPS CLI UI, but all processes can also work can without using the UI and can be integrated into automated workflows. See the Command Line Reference in the User Guide for more details. ","version":"Next","tagName":"h2"},{"title":"Getting Started​","type":1,"pageTitle":"Getting Started With the MPS CLI","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_getting_started#getting-started","content":"","version":"Next","tagName":"h2"},{"title":"Install Project Aria Tools​","type":1,"pageTitle":"Getting Started With the MPS CLI","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_getting_started#install-project-aria-tools","content":"Project Aria MPS CLI is only available if you install the pip installation version of Project Aria Tools. This installation has been designed to be simple to use, even if you are not familiar with programming languages. Install Project Aria Tools To return to your installation of Project Aria Tools at any time, restart the virtual environment using the following command: source $HOME/projectaria_tools_python_env/bin/activate  ","version":"Next","tagName":"h3"},{"title":"Download the MPS CLI sample dataset​","type":1,"pageTitle":"Getting Started With the MPS CLI","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_getting_started#download-the-mps-cli-sample-dataset","content":"To try out the following commands on VRS files: Download the sample files: Sample 1 - single VRS fileSample 2 - single VRS file Move them to a directory called Example in your downloads directory info You may also wish to use your own recordings. ","version":"Next","tagName":"h3"},{"title":"Request MPS for all VRS files in the “Example” directory and it’s subdirectories:​","type":1,"pageTitle":"Getting Started With the MPS CLI","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_getting_started#request-mps-for-all-vrs-files-in-the-example-directory-and-its-subdirectories","content":"aria_mps single -i ~/Downloads/Example/  You'll be prompted to enter your username and password. Use the Project Aria credentials you use to sign into the Mobile Companion app.  Once the request has been processed, the MPS output will be downloaded next to the original VRS file. In this example, a recording in the Example directory called recording1.vrs was used to generate MPS.  └── Example folder ├── mps_recording1_vrs │ ├── eye_gaze │ │ ├── general_eye_gaze.csv │ │ └── summary.json │ ├── slam │ │ ├── closed_loop_trajectory.csv │ │ ├── online_calibration.jsonl │ │ ├── open_loop_trajectory.csv │ │ ├── semidense_observations.csv.gz │ │ ├── semidense_points.csv.gz │ │ └── summary.json │ ├── vrs_health_check.json │ └── vrs_health_check_slam.json └── recording1.vrs  Go to MPS Data Format Basics for more details about the folder structure. ","version":"Next","tagName":"h3"},{"title":"Exit the MPS CLI​","type":1,"pageTitle":"Getting Started With the MPS CLI","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_getting_started#exit-the-mps-cli","content":"To quit the MPS CLI, press CTRL + Q. The CLI will ask for confirmation before quitting. If you quit the request tool while the files are uploading the uploads will stop.If you resubmit the request the uploads will resume where they left off, progress won’t be lost. If you quit the request tool once the files have been uploaded, the MPS processes will continue. Once processing is complete, and the request tool is open, MPS files will be automatically downloaded to your VRS files’ location. ","version":"Next","tagName":"h3"},{"title":"Multi Sequence MPS Requests​","type":1,"pageTitle":"Getting Started With the MPS CLI","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_getting_started#multi-sequence-mps-requests","content":"When you request MPS using multi mode, MPS will process a group of recordings together to generate Multi-SLAM MPS. Multi-SLAM MPS creates SLAM MPS outputs in a shared co-ordinate frame. Once the request has been processed, the MPS output will be downloaded to the directory you defined. aria_mps multi -i ~/Downloads/Example/ -o ~/Documents/multi_slam_output   ","version":"Next","tagName":"h3"},{"title":"Working with MPS data​","type":1,"pageTitle":"Getting Started With the MPS CLI","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_getting_started#working-with-mps-data","content":"You may find the following resources helpful when working with MPS data: MPS Data FormatsVisualize MPS Using PythonVisualize MPS Using C++ ","version":"Next","tagName":"h2"},{"title":"Project Aria Client SDK","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/sdk","content":"","keywords":"","version":"Next"},{"title":"Client SDK features​","type":1,"pageTitle":"Project Aria Client SDK","url":"/projectaria_tools/docs/ARK/sdk#client-sdk-features","content":"After connecting your Project Aria glasses and PC via USB or WiFi, you will be able to: Retrieve device information, status and sensor calibration dataStart and stop recordingConfigure, start and stop streaming from the glassesSubscribe to streaming data from the glasses ","version":"Next","tagName":"h3"},{"title":"Project Aria CLI​","type":1,"pageTitle":"Project Aria Client SDK","url":"/projectaria_tools/docs/ARK/sdk#project-aria-cli","content":"In addition to APIs and code samples, the SDK also provides Aria CLI which allows you to control an Project Aria glasses and to stream its data without touching any code. info The current SDK supports one pair of Aria glasses connected to one computer. ","version":"Next","tagName":"h3"},{"title":"This documentation covers:​","type":1,"pageTitle":"Project Aria Client SDK","url":"/projectaria_tools/docs/ARK/sdk#this-documentation-covers","content":"Setup Guide PrerequisitesDownload and install the Client SDKPair your Aria glasses to the computerExtract SDK samples Code samples to demonstrate using the SDK to control Project Aria glasses and build you own real-time applications. Connection: connect an Aria device to a computer, fetch the device information and status.Recording: start and stop a recording via USB and Wi-Fi.Streaming Subscription: subscribe to and unsubscribe from a streaming deviceStreaming and Visualizing All Live Sensor Data: programmatically start and stop a streaming session, add callbacks to visualize and manipulate the streamed dataStreaming Undistorted RGB Image Using Calibration: programmatically start and stop a streaming session, access sensor calibration and undistort an RGB live stream Core Concepts Sensor Profiles: access information about Aria recording profiles via the CLI or SDKStreaming Internals: deeper dives into how data is streamed API ReferencesCLI documentationSDK Troubleshooting If you run into any issues, please check out the Troubleshooting page, post to Academic Partners Feedback and Support, post to our Academic Partners Discord group or email to AriaOps@meta.com. ","version":"Next","tagName":"h3"},{"title":"Aria CLI","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/sdk/cli","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Aria CLI","url":"/projectaria_tools/docs/ARK/sdk/cli#overview","content":"This page provides an overview of Aria CLI features and a few useful commands, go to the Command Reference page for a full list. ","version":"Next","tagName":"h2"},{"title":"About the CLI​","type":1,"pageTitle":"Aria CLI","url":"/projectaria_tools/docs/ARK/sdk/cli#about-the-cli","content":"Aria CLI is a simple command line binary available on Linux and Mac which allows users to control their Project Aria glasses via CLI commands. The CLI currently provides the ability to: Pair the glasses via USB or Wi-FiConnect to the glasses via USB or Wi-FiRetrieve the device status and informationControl Aria recording capabilitiesControl Aria streaming capabilities caution To be able to use these commands, follow the setup instructions to install the CLI, and pair your glasses. ","version":"Next","tagName":"h2"},{"title":"Help​","type":1,"pageTitle":"Aria CLI","url":"/projectaria_tools/docs/ARK/sdk/cli#help","content":"aria --help  will show you available options and subcommands. ","version":"Next","tagName":"h2"},{"title":"Connecting​","type":1,"pageTitle":"Aria CLI","url":"/projectaria_tools/docs/ARK/sdk/cli#connecting","content":"You can connect to Aria glasses via Wi-Fi or USB. Wi-Fi: you will need to specify the option --device-ip Pair your Aria glasses with the Mobile Companion app to get their IP address by tapping Wi-Fi in the DashboardGo to the Mobile Companion app page for screenshots and further instructions USB: you don't need to manually input the device IP when it is plugged in via USB, the CLI will automatically connect Use the option --serial, if more than one device is plugged in via USB ","version":"Next","tagName":"h2"},{"title":"Aria Client SDK API Reference","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/sdk/api_reference","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#overview","content":"This page provides a list of Project Aria Client SDK's APIs. ","version":"Next","tagName":"h2"},{"title":"Global Functions & Methods​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#global-functions--methods","content":"Function\tType\tDescriptionset_log_level(level: Level)\tNone\tSets the SDK logging verbosity ","version":"Next","tagName":"h2"},{"title":"Classes​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#classes","content":"","version":"Next","tagName":"h2"},{"title":"aria.sdk.Error​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkerror","content":"Property\tType\tDescriptioncode\tint\tError code message\tstr\tHuman readable error message ","version":"Next","tagName":"h3"},{"title":"aria.sdk.RecordingConfig​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkrecordingconfig","content":"Property\tType\tDescriptionprofile_name\tstr\tSensors profile name for recording ","version":"Next","tagName":"h3"},{"title":"aria.sdk.StreamingSecurityOptions​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkstreamingsecurityoptions","content":"Property\tType\tDescriptionuse_ephemeral_certs\tbool\tUse ephemeral certs instead of persistent ones local_certs_root_path\tstr\tLocal directory path where streaming certificates are stored ","version":"Next","tagName":"h3"},{"title":"aria.sdk.StreamingConfig​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkstreamingconfig","content":"Property\tType\tDescriptionsecurity_options\tStreamingSecurityOptions\tSecurity options used to start streaming topic_prefix\tstr\tUse this as a unique string to prefix all streamed data messages profile_name\tstr\tSensors profile name used to start streaming streaming_interface\tStreamingInterface\tNetwork interface used to start streaming ","version":"Next","tagName":"h3"},{"title":"aria.sdk.StreamingSubscriptionConfig​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkstreamingsubscriptionconfig","content":"Property\tType\tDescriptionsecurity_options\tStreamingSecurityOptions\tSecurity options used to subscribe to an existing live secure stream subscriber_data_type\tStreamingDataType\tData types to subscribe to message_queue_size\tint\tSize for the message queue. A shorter queue size may be useful if the processing callback is always slow and you wish to process more recent data subscriber_name\tstr\tRetrieve the subscriber name subscriber_topic_prefix\tstr\tRetrieve the topic used to prefix the existing live stream ","version":"Next","tagName":"h3"},{"title":"aria.sdk.DeviceInfo​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkdeviceinfo","content":"Property\tType\tDescriptionboard\tstr\tDevice board name bootloader\tstr\tBootloader version brand\tstr\tDevice brand name manufacturer\tstr\tManufacturer name model\tstr\tModel name product\tstr\tProduct name serial\tstr\tSerial number time\tint\tOS build timestamp ","version":"Next","tagName":"h3"},{"title":"aria.sdk.DeviceStatus​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkdevicestatus","content":"Property\tType\tDescriptionbattery_level\tint\tBattery level charger_connected\tbool\tUSB charger cable state charging\tbool\tCharging state wifi_enabled\tbool\tWiFi activation state wifi_configured\tbool\tWiFi configuration state wifi_connected\tbool\tWiFi connection state wifi_ip_address\tstr\tWiFi IP address wifi_device_name\tstr\tWiFi device name wifi_ssid\tstr\tWiFi SSID name logged_in\tbool\tCompanion App user login state developer_mode\tbool\tDeveloper mode state adb_enabled\tbool\tADB state thermal_mitigation_triggered\tbool\tIndicate max level temperature has been reached triggering throttling skin_temp_celsius\tfloat\tDevice temperature default_recording_profile\tstr\tDefault recording profile used when pressing the top right HW button is_recording_allowed\tbool\tRecording capability state device_mode\tDeviceMode\tDevice mode ","version":"Next","tagName":"h3"},{"title":"aria.sdk.DeviceClientConfig​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkdeviceclientconfig","content":"Property\tType\tDescriptionip_v4_address\tstr\tIP v4 address to use for connecting to the device via Wi-Fi device_serial\tstr\tDevice serial number used when connecting to the device via USB (only necessary if multiple devices are plugged in) adb_path\tstr\tSpecify your own custom ADB version reconnection_attempts\tint\tNumber of reconnection attempts before time out. Defaults to 2 ","version":"Next","tagName":"h3"},{"title":"aria.sdk.DeviceClient​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkdeviceclient","content":"Method\tType\tDescriptioncreate(config: DeviceClientConfig)\tDeviceClient\tCreate DeviceClient instance authenticate()\tNone\tAuthenticate your client using the specified config connect()\tDevice\tConnect to device via Wifi or via USB using the specified config. Setting both ip address and adb path will lead to the adb path being ignored disconnect(device: Device)\tNone\tDisconnect Device instance ","version":"Next","tagName":"h3"},{"title":"aria.sdk.Device​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkdevice","content":"Method\tType\tDescriptionrecording_manager()\tRecordingManager\tAccess recording capabilities streaming_manager()\tStreamingManager\tAccess streaming capabilities info()\tDeviceInfo\tRetrieve device information such as device name and serial number status()\tDeviceStatus\tRetrieve device status such as battery level and device temperature factory_calibration_json()\tstr\tRetrieve device factory calibration as JSON string ","version":"Next","tagName":"h3"},{"title":"aria.sdk.RecordingManager​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkrecordingmanager","content":"Method\tType\tDescriptionstart_recording()\tNone\tStart recording stop_recording()\tNone\tStop recording sensors_calibration()\tstr\tRetrieve the device calibration computed from the sensors profile used to record Property\tType\tDescriptionrecording_config\tRecordingConfig\tUsed to configure recording parameters such as the sensors profile recording_state\tRecordingState\tDetermine current recording state recording_profiles\tList[str]\tReturns a list of existing profile names to use to start recording ","version":"Next","tagName":"h3"},{"title":"aria.sdk.StreamingManager​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkstreamingmanager","content":"Method\tType\tDescriptionstart_streaming()\tNone\tStart streaming stop_streaming()\tNone\tStop streaming sensors_calibration()\tstr\tRetrieve the device calibration computed from the sensors profile used to stream Property\tType\tDescriptionstreaming_config\tStreamingConfig\tUsed to configure streaming parameters related to network interface, security, and sensors profile streaming_state\tStreamingState\tDetermine current streaming state streaming_profiles\tList[str]\tReturns a list of existing profile names to use to start streaming ","version":"Next","tagName":"h3"},{"title":"aria.sdk.BaseStreamingClientObserver​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkbasestreamingclientobserver","content":"Method\tType\tDescriptionon_streaming_client_failure(reason: ErrorCode, message: str)\tNone\tRetrieve streaming failure on_image_received(image_and_record: projectaria_tools.core.sensor_data.ImageDataAndRecord)\tNone\tRetrieve image data streamed from rgb, slam1, slam2 or eye tracking camera sensors on_audio_received(audio_and_record: projectaria_tools.core.sensor_data.AudioDataAndRecord)\tNone\tRetrieve audio data streamed from microphone sensors on_imu_received(motion_data: List[projectaria_tools.core.sensor_data.MotionData], imu_idx: int)\tNone\tRetrieve imu data streamed from IMU1 and IMU2 sensors. Use imu_idx to determine the IMU on_magneto_received(magneto_data: projectaria_tools.core.sensor_data.MotionData)\tNone\tRetrieve magnetometer data streamed from magnetometer sensor on_baro_received(baro_data: projectaria_tools.core.sensor_data.BarometerData)\tNone\tRetrieve barometer data streamed from barometer sensor ","version":"Next","tagName":"h3"},{"title":"aria.sdk.StreamingClient​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkstreamingclient","content":"Method\tType\tDescriptioncreate()\tStreamingClient\tCreate StreamingClient instance subscribe()\tNone\tSubscribe to data streamed from Aria unsubscribe()\tNone\tUnsubscribe to data streamed from Aria set_streaming_client_observer(observer: StreamingClientObserver)\tNone\tSets the observer to subscribe to the data streamed from Aria is_subscribed()\tbool\tReturns streaming subscription state ","version":"Next","tagName":"h3"},{"title":"Enums​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#enums","content":"","version":"Next","tagName":"h2"},{"title":"aria.sdk.Level​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdklevel","content":"Name\tDescriptionDisabled\tDisable all logs Error\tPrint only error logs Warning\tPrint warning and error logs Info\tPrint info, warning and error logs (default) Debug\tPrint debug, info, warning and error logs Trace\tPrint all logs ","version":"Next","tagName":"h3"},{"title":"aria.sdk.CameraId​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkcameraid","content":"Name\tDescriptionSlam1\tSlam camera 1 sensor Slam2\tSlam camera 2 sensor Rgb\tRgb camera sensor EyeTrack\tEye tracking camera sensors Invalid\tUnknown camera sensor ","version":"Next","tagName":"h3"},{"title":"aria.sdk.RecordingState​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkrecordingstate","content":"Name\tDescriptionNotStarted\tRecording not started Started\tRecording stopped Streaming\tRecording in progress Stopped\tRecording stopped ","version":"Next","tagName":"h3"},{"title":"aria.sdk.StreamingState​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkstreamingstate","content":"Name\tDescriptionNotStarted\tStreaming not started Started\tStreaming stopped Streaming\tStreaming in progress Stopped\tStreaming stopped ","version":"Next","tagName":"h3"},{"title":"aria.sdk.StreamingInterface​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkstreaminginterface","content":"Name\tDescriptionWifiStation\tStream through WiFi router Usb\tStream through USB cable ","version":"Next","tagName":"h3"},{"title":"aria.sdk.StreamingDataType​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkstreamingdatatype","content":"Name\tDescriptionUnknown Rgb\tRgb sensor data Slam\tSlam1 and Slam2 sensors data EyeTrack\tEye tracking sensors data Audio\tMicrophones data Imu\tImu sensors data Magneto\tMagnetometer sensor data Baro\tBarometer sensor data ","version":"Next","tagName":"h3"},{"title":"aria.sdk.DeviceMode​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkdevicemode","content":"Name\tDescriptionResearch\tResearch mode Partner\tPartner mode Prototype\tPrototype mode ","version":"Next","tagName":"h3"},{"title":"Project Aria Machine Perception Services CLI Guide","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_guide","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Project Aria Machine Perception Services CLI Guide","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_guide#overview","content":"The Project Aria MPS CLI Guide provides detailed information about how to use this tool. The guide contain: About the MPS CLIHow the MPS CLI works Go to MPS Data Lifecycle for more details about how sequences are processed on the server MPS CLI SettingsMPS CLI Command Line ReferenceTroubleshooting and Error Codes Go to Getting Started With the MPS CLI for a quick tutorial showing basic commands using sample data.   ","version":"Next","tagName":"h2"},{"title":"About​","type":1,"pageTitle":"Project Aria Machine Perception Services CLI Guide","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_guide#about","content":"The Project Aria MPS Command Line Interface (MPS CLI), part of Project Aria Tools, is the preferred way to request Machine Perception Services (MPS). While you can use the Desktop Companion app to request MPS, the MPS CLI provides more features and ease of upload. The MPS CLI has two modes: Single Process each recording individuallyThe input can be a file and/or directory, so you can batch process multiple recordings with a single commandOutput is always saved next to the input fileThe most common way to request MPS Multi Process multiple recordings to generate multi-sequence SLAM dataUser must provide a directory for the outputs Go to MPS CLI Overview for a full breakdown of all the features available.   ","version":"Next","tagName":"h2"},{"title":"How the MPS CLI works​","type":1,"pageTitle":"Project Aria Machine Perception Services CLI Guide","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_guide#how-the-mps-cli-works","content":"The MPS CLI enables you to upload VRS files from your computer to the Meta servers for processing. The outputs are then saved to your local directory. The MPS CLI will try to process multiple recordings concurrently. The concurrency for various stages can be controlled via Settings.  Once you submit your request, the MPS CLI for the selected mode will open and show the status of your requests. See the Getting Started or the Command Line Reference below for how to submit a request. Once authenticated, the request tool checks with the server to see if this recording was previously processed. We use unique IDs (Hashing stage) to check if this is a new or a previously known recording.If it is a known recording, we skip processing and directly download the results (Download Results stage) or show the error code.If this is a new recording, we run health checks on the recording (HealthCheck stage), to minimize the chances that it will fail during processing. While this check catches obvious errors, like gaps in data and ensures presence of the right sensor streams, but server side processing may still fail.If the health check passes, the recording is encrypted on your machine (Encrypting stage).After encryption, the recordings are uploaded to the MPS servers (Uploading stage) for processing. Uploads are resumable.Interrupted uploads can be resumed within 24 hours. Data is processed on MPS servers (Processing stage). The MPS CLI periodically checks the MPS request's status on the server.It is safe to close the MPS Request tool once the data is processing. When the MPS CLI is reopened, it will check the status of your data and progress to Downloading if it is ready. If you get an error code The server re-attempts processing multiple times before it stops and provides an error message Check Error Codes to see what the error was.We encourage you to send a bug report with log files to Aria User Support if it is not an error code 1xx. By default, logs are stored in /tmp/logs/projectaria/mps/. Once the processing is complete, and the tool is open, outputs are automatically downloaded (Downloading stage).Recordings in the MPS CLI UI will show the Success status once the outputs been successfully downloaded. ","version":"Next","tagName":"h2"},{"title":"Logs​","type":1,"pageTitle":"Project Aria Machine Perception Services CLI Guide","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_guide#logs","content":"Each run will write the console logs to a log file on the local drive. Since the MPS CLI can run concurrently on multiple recordings, these logs are useful for debugging purposes. The logs are named by the current time when the request was initiated via CLI. By default, logs are stored in /tmp/logs/projectaria/mps/. The location can be modified in settings.   ","version":"Next","tagName":"h3"},{"title":"CLI Settings​","type":1,"pageTitle":"Project Aria Machine Perception Services CLI Guide","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_guide#cli-settings","content":"Project Aria MPS CLI settings can be customized via the mps.ini file. This file is located in the $HOME/.projectaria/mps.ini Setting\tDescription\tDefault Value General settings log_dir\tWhere log files are saved for each run. The filename is the timestamp from when the request tool started running.\t/tmp/logs/projectaria/mps/ status_check_interval\tHow long the MPS CLI waits to check the status of data during the Processing stage.\t30 secs HASH concurrent_hashes\tMaximum number of files that can be concurrently hashed\t4 chunk_size\tChunk size to use for hashing\t10MB Encryption chunk_size\tChunk size to use for encryption\t50MB concurrent_encryptions\tMaximum number of files that can be concurrently encrypted\t4 delete_encrypted_files\tWhether to delete the encrypted files after upload is done. If you set this to false local disk usage will double, due to an encrypted copy of each file.\tTrue. Health Check concurrent_health_checks\tMaximum number of VRS file healthchecks that can be run concurrently\t2 Uploads backoff\tThe exponential back off factor for retries during failed uploads. The wait time between successive retries will increase with this factor.\t1.5 interval\tBase delay between retries.\t20 secs retries\tMaximum number of retries before giving up.\t10 concurrent_uploads\tMaximum number of concurrent uploads.\t4 max_chunk_size\tMaximum chunk size that can be used during uploads.\t100 MB min_chunk_size\tThe minimum upload chunk size.\t5 MB smoothing_window_size\tSize of the smoothing window to adjust the chunk size. This value defines the number of uploaded chunks that will be used to determine the next chunk size.\t10 target_chunk_upload_secs\tTarget time to upload a single chunk. If the chunks in a smoothing window take longer, we reduce the chunk size. If it takes less time, we increase the chunk size.\t3 secs GraphQL (Query the MPS backend for MPS Status) backoff\tThis the exponential back off factor for retries for failed queries. The wait time between successive retries will increase with this factor\t1.5 interval\tBase delay between retries\t4 secs retries\tMaximum number of retries before giving up\t3 Download backoff\tThis the exponential back off factor for retries during failed downloads. The wait time between successive retries will increase with this factor.\t1.5 interval\tBase delay between retries\t20 secs retries\tMaximum number of retries before giving up\t10 chunk_size\tThe chunk size to use for downloads\t10MB concurrent_downloads\tNumber of concurrent downloads\t10 delete_zip\tThe server will send the results in a zip file. This flag controls whether to delete the zip file after extraction or not\tTrue   ","version":"Next","tagName":"h2"},{"title":"Command line reference​","type":1,"pageTitle":"Project Aria Machine Perception Services CLI Guide","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_guide#command-line-reference","content":"The MPS CLI has two distinct modes: single (process each recording individually) and multi (SLAM outputs for multiple recordings in a shared co-ordinate frame). aria_mps single &lt;options&gt;  or aria_mps multi &lt;options&gt;  ","version":"Next","tagName":"h2"},{"title":"Help​","type":1,"pageTitle":"Project Aria Machine Perception Services CLI Guide","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_guide#help","content":"To see the available options and subcommands, use: --help  or -h  ","version":"Next","tagName":"h3"},{"title":"Authentication​","type":1,"pageTitle":"Project Aria Machine Perception Services CLI Guide","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_guide#authentication","content":"Log in​ The first time you use the MPS CLI, you’ll be prompted to enter your username and password. Use the Project Aria credentials you use to sign into the Mobile Companion app. You can also supply the username and password via CLI input. This option is more suited for running MPS as part of a batch script or other automated workflows. The authentication token gets saved in the $HOME/.projectaria directory. To log in using the CLI -u USERNAME -p PASSWORD  or --username USERNAME --password PASSWORD  Token storage​ The login token is saved onto your machine by default. If you request MPS using --no-ui , you'll have the option to pass --no-save-token. This means the token won't be saved. Once processing is complete the MPS CLI will also logout and invalidate any existing tokens. Log out​ Use the following command to log out the authentication token and delete it from the system. Next time you run the CLI, it will ask for username and password again. aria_mps logout  ","version":"Next","tagName":"h3"},{"title":"Request commands for any mode​","type":1,"pageTitle":"Project Aria Machine Perception Services CLI Guide","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_guide#request-commands-for-any-mode","content":"These options are shared between both modes. Define input path​ Provides the path for a directory or file that will be uploaded for processing. Where a directory is provided, all subdirectories will be scanned for VRS files. At least one input file must be provided. There is no limit of how many files or folders can be included in a single request. -i INPUT  or --input INPUT  Force the provided files to be reprocessed​ Force the server to reprocess all of the provided files, regardless of their current state on the server. --force  Automatically retry processing if it fails​ By default the MPS server will retry processing data multiple times before generating a failure code. By adding this flag requests automatically retries again if the processing fails. This command is generally only worth using if you’ve done some debugging to warrant it. --retry-failed  note If you retry 30 days after the recording was uploaded, you'll also need to re-upload the data. Don’t show the UI​ Instead of the MPS CLI UI, you’ll see the raw outputs and processes in the command line. --no-ui  ","version":"Next","tagName":"h3"},{"title":"Single Recording mode​","type":1,"pageTitle":"Project Aria Machine Perception Services CLI Guide","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_guide#single-recording-mode","content":"Select the MPS you wish to generate​ By default, Eye Gaze and SLAM MPS are generated. Use the features option to select a single service, which can speed up processing. --features {EYE_GAZE,SLAM}  ","version":"Next","tagName":"h3"},{"title":"Multi-Recording mode​","type":1,"pageTitle":"Project Aria Machine Perception Services CLI Guide","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_guide#multi-recording-mode","content":"Define the output directory​ Define the output directory where the results will be stored. Setting the output path is required. The output directory must be empty before processing starts. -o OUTPUT_DIR  Or --output OUTPUT_DIR    ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"Project Aria Machine Perception Services CLI Guide","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_guide#troubleshooting","content":"","version":"Next","tagName":"h2"},{"title":"Authentication issue on macOS​","type":1,"pageTitle":"Project Aria Machine Perception Services CLI Guide","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_guide#authentication-issue-on-macos","content":"If you encounter a permissions error while trying to sign in to the MPS CLI, try: ln -s /etc/ssl/* /Library/Frameworks/Python.framework/Versions/3.9/etc/openssl  ","version":"Next","tagName":"h3"},{"title":"Terminal display issues in macOS​","type":1,"pageTitle":"Project Aria Machine Perception Services CLI Guide","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_guide#terminal-display-issues-in-macos","content":"Some users have reported that the MPS CLI UI has display issues in Terminal. Using a different terminal app may help. Why doesn't Textual look good on macOS provides further details about this issue. ","version":"Next","tagName":"h3"},{"title":"Error codes​","type":1,"pageTitle":"Project Aria Machine Perception Services CLI Guide","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_guide#error-codes","content":"Error Code\tDescription\tAll 1xx codes are local client errors 100\tMulti-Recording processing error. Another recording in the group had a failure, so the processing was halted. We need the full group of recordings to successfully process and generate multi-slam MPS output. The recording that fails will show its own error code, processing then stops and and sets the other recordings to this error code. 101\tSomething failed on your computer. When you see this error, we suggest inspecting the local log for this run. If you need to reach out to support, please include this log file. 102\tThere were health check failures during processing. Check the vrs_health_check.json and vrs_health_check_slam.json for more information about what went wrong. 103\tMulti-Recording processing error. Duplicate file detected. Duplicate recordings are not allowed for multi-SLAM processing. 104\tMPS CLI failed to encrypt the file. Check error logs for more information. 105\tInput-output mismatch error. This error happens when you try to run multi-SLAM processing but pass an output directory that already contains output or intermediate artifacts from a different multi-SLAM job. 106\tThe output directory used for multi-SLAM processing is not empty. Non-empty output directory is only allowed if the output directory contains output or artifacts from a previous run of the same group of recordings. 108\tThe server failed to query the status of the request. Retrying should usually fix this issue. 109\tError during computing the hash id of the file. Check error logs for more information. 110\tError when checking existing outputs on the disk. Check error logs for more information. 111\tError when checking previously submitted MPS requests for a file. Check error logs for more information. 112\tError when checking if the recording was previously uploaded. Check error logs for more information. 113\tError uploading the VRS file. Check error logs for more information. 114\tError submitting MPS request to the server. Check error logs for more information. Retrying should fix the issue. 115\tError when checking the processing status of the MPS request. Check error logs for more information. 116\tError when downloading the MPS results. Check error logs for more information. All other error codes\tServer side failure. Reach out to support with the error code and log file. ","version":"Next","tagName":"h3"},{"title":"Aria CLI Command References","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Aria CLI Command References","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference#overview","content":"This page provides all of the commands available when using the Project Aria CLI (installed as part of the Client SDK). You can also use ADB to interact with Project Aria glasses. For any command with [OPTIONS], use --help or -h to find out about the options and subcommands available. ","version":"Next","tagName":"h2"},{"title":"Authentication​","type":1,"pageTitle":"Aria CLI Command References","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference#authentication","content":"Manage device pairing with external client over Wi-Fi and USB aria auth [OPTIONS] SUBCOMMAND  ","version":"Next","tagName":"h2"},{"title":"pair​","type":1,"pageTitle":"Aria CLI Command References","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference#pair","content":"Pair device with client aria auth pair  ","version":"Next","tagName":"h3"},{"title":"unpair​","type":1,"pageTitle":"Aria CLI Command References","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference#unpair","content":"Unpair device from client aria auth unpair  ","version":"Next","tagName":"h3"},{"title":"Device​","type":1,"pageTitle":"Aria CLI Command References","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference#device","content":"Control device. aria device [OPTIONS] SUBCOMMAND  ","version":"Next","tagName":"h2"},{"title":"list​","type":1,"pageTitle":"Aria CLI Command References","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference#list","content":"List connected USB devices. aria device list  ","version":"Next","tagName":"h3"},{"title":"info​","type":1,"pageTitle":"Aria CLI Command References","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference#info","content":"Get device info. aria device info  ","version":"Next","tagName":"h3"},{"title":"status​","type":1,"pageTitle":"Aria CLI Command References","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference#status","content":"Get device status. aria device status  ","version":"Next","tagName":"h3"},{"title":"default-profile​","type":1,"pageTitle":"Aria CLI Command References","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference#default-profile","content":"Manage default profile. aria device default-profile [OPTIONS] SUBCOMMAND  Subcommands include​ --status  Get default profile status. --set  Set default profile. ","version":"Next","tagName":"h3"},{"title":"Recording​","type":1,"pageTitle":"Aria CLI Command References","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference#recording","content":"Control device recording. aria recording [OPTIONS] SUBCOMMAND  ","version":"Next","tagName":"h2"},{"title":"start​","type":1,"pageTitle":"Aria CLI Command References","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference#start","content":"Start recording. aria recording start [OPTIONS]  Options​ --profile (string)  Recording profile name. ","version":"Next","tagName":"h3"},{"title":"stop​","type":1,"pageTitle":"Aria CLI Command References","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference#stop","content":"Stop recording. aria recording stop  ","version":"Next","tagName":"h3"},{"title":"profiles​","type":1,"pageTitle":"Aria CLI Command References","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference#profiles","content":"List recording profiles. aria recording profiles [OPTIONS]  Options​ --save-json (string)  Save profiles as JSON files. --json-dir (string)  Location of directory where profiles will be saved as JSON files. ","version":"Next","tagName":"h3"},{"title":"Streaming​","type":1,"pageTitle":"Aria CLI Command References","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference#streaming","content":"Control device streaming. aria streaming [OPTIONS] SUBCOMMAND  ","version":"Next","tagName":"h2"},{"title":"start​","type":1,"pageTitle":"Aria CLI Command References","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference#start-1","content":"Start streaming aria streaming start [OPTIONS]  Options​ --profile (string)  Streaming profile name. --topic-prefix (string)  Streaming topic prefix. --interface (string)  Streaming interface name. Values can be:[usb] - Streaming over USB. [wifi] - Streaming over Wi-Fi network. [hotspot] - Streaming over Aria Wi-Fi hotspot. --use-ephemeral-certs (flag)  Use ephemeral streaming certs on both device and local host (see aria streaming install-certs --help) --local-certs-dir (string)  Local streaming certificates directory. Use to specify the local directory where local streaming certificates are installed on local host. ","version":"Next","tagName":"h3"},{"title":"Stop​","type":1,"pageTitle":"Aria CLI Command References","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference#stop-1","content":"Stop streaming. aria streaming stop [OPTIONS]  ","version":"Next","tagName":"h3"},{"title":"Options​","type":1,"pageTitle":"Aria CLI Command References","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference#options-3","content":"--remove-ephemeral-certs (flag)  Remove ephemeral streaming certs from local host. Use aria streaming uninstall-certs --help for commands to uninstall ephemeral certs on Aria glasses and local host. info Ephemeral certificates should automatically expire and be removed when streaming stops. --local-certs-dir (string)  Local streaming certificates directory. Use to specify the local directory where local streaming certificates are installed on local host. ","version":"Next","tagName":"h3"},{"title":"profiles​","type":1,"pageTitle":"Aria CLI Command References","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference#profiles-1","content":"List streaming profiles. aria streaming profiles [OPTIONS]  Options​ --save-json (flag)  Save profiles as JSON files. --json-dir (string)  Location of directory where profiles will be saved as JSON files. ","version":"Next","tagName":"h3"},{"title":"install-certs​","type":1,"pageTitle":"Aria CLI Command References","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference#install-certs","content":"Installs streaming certificates on Aria glasses and on the local host. aria streaming install-certs [OPTIONS]  Options​ --local-certs-dir (string)  Local streaming certificates directory. Use to specify the local directory where local streaming certificates are installed on local host. ","version":"Next","tagName":"h3"},{"title":"uninstall-certs​","type":1,"pageTitle":"Aria CLI Command References","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference#uninstall-certs","content":"Removes streaming certificates on Aria glasses and on the local host. aria streaming uninstall-certs [OPTIONS]  Options​ --local-certs-dir (string)  Local streaming certificates directory. Use to specify the local directory where local streaming certificates are installed on local host. ","version":"Next","tagName":"h3"},{"title":"Connect to the glasses and retrieve device status","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/sdk/samples/device_connection","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Connect to the glasses and retrieve device status","url":"/projectaria_tools/docs/ARK/sdk/samples/device_connection#overview","content":"The device_connect example shows how to connect to your Project Aria device using the Client SDK and retrieve: Device status, such as such as battery level, Wi-Fi SSID or Wi-Fi IP addressDevice information, such as the device serial number or device model ","version":"Next","tagName":"h2"},{"title":"Running the sample​","type":1,"pageTitle":"Connect to the glasses and retrieve device status","url":"/projectaria_tools/docs/ARK/sdk/samples/device_connection#running-the-sample","content":"In your terminal, from the samples directory, run: python -m device_connect  You should then see: [AriaSdk:DeviceControllerImpl][INFO]: Connecting to device &lt;serial_number&gt; using ADB [AriaSdk:DeviceClientImpl][INFO]: Connection established with device &lt;serial_number&gt; Aria Device Status: battery level 100, wifi ssid &lt;xxxxxxxx&gt; , wifi ip &lt;192.168.xx.xx&gt;, mode DeviceMode.Partner Aria Device Info: model Aria, serial &lt;serial_number&gt; Aria Device Connected, disconnecting  ","version":"Next","tagName":"h2"},{"title":"Code walkthrough​","type":1,"pageTitle":"Connect to the glasses and retrieve device status","url":"/projectaria_tools/docs/ARK/sdk/samples/device_connection#code-walkthrough","content":"1. Create and configure a Device Client​ DeviceClient allow you to connect to Project Aria glasses over Wi-Fi or USB. device_client = aria.DeviceClient()  By default, DeviceClient connects to Aria glasses over USB. To connect to glasses over Wi-Fi, configure the DeviceClient by creating a DeviceClientConfig, setting ip_v4_address and setting the config. client_config = aria.DeviceClientConfig() if args.ip_address: client_config.ip_v4_address = args.ip_address device_client.set_client_config(client_config)  info Get your Aria glasses' IP address from the Mobile Companion App by tapping Wi-Fi on the Dashboard. 2. Connect to a Device​ Connect to the Aria glasses and retrieve a Device instance. An Exception will be thrown if the connection is not successful. device = device_client.connect()  3. Fetch device status and information​ We can then obtain the device status and information: status = device.status print( &quot;Aria Device Status: battery level {0}, wifi ssid {1}, wifi ip {2}, mode {3}&quot;.format( status.battery_level, status.wifi_ssid, status.wifi_ip_address, status.device_mode, ) )  Check out the full status list. info = device.info print( &quot;Aria Device Info: model {}, serial {}, manufacturer {}&quot;.format( info.model, info.serial, info.manufacturer ) )  Check out the full information list. 4. Disconnect​ Once all operations have been completed, you can disconnect from your glasses to release any held resources. device_client.disconnect(device)  ","version":"Next","tagName":"h3"},{"title":"Streaming Internals","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/sdk/concepts/streaming_internals","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Streaming Internals","url":"/projectaria_tools/docs/ARK/sdk/concepts/streaming_internals#overview","content":"This page provides details about how Project Aria data is streamed using the Client SDK. This is followed by how to stream and subscribe to Aria glasses’ sensor streams. ","version":"Next","tagName":"h2"},{"title":"Concepts​","type":1,"pageTitle":"Streaming Internals","url":"/projectaria_tools/docs/ARK/sdk/concepts/streaming_internals#concepts","content":"","version":"Next","tagName":"h2"},{"title":"Introduction​","type":1,"pageTitle":"Streaming Internals","url":"/projectaria_tools/docs/ARK/sdk/concepts/streaming_internals#introduction","content":"Project Aria glasses can be configured to stream sensor data over a network to one or more devices running the Client SDK. You can stream data from the glasses over Wi-Fi or USB. There are two aspects to streaming; configuring the glasses to stream data and subscribing to the data. You may configure the device for streaming via the SDK, the CLI or by changing the default recording set by the Companion App. During configuration, you can select a profile that determines which sensors stream data and at what rate and resolution. Once streaming has started, the glasses will start capturing data according to the set profile, but no data will be transmitted over the network until a computer running the SDK subscribes to the data. A computer running the SDK may then subscribe to all or a subset of the sensor streams from the glasses. The device or process subscribing does not need to be the same as the device that configured the sensors or started the streaming. ","version":"Next","tagName":"h3"},{"title":"Streaming security​","type":1,"pageTitle":"Streaming Internals","url":"/projectaria_tools/docs/ARK/sdk/concepts/streaming_internals#streaming-security","content":"The sensor data streams are secured via a certificate pair. A subscribing device must have a valid certificate matching the one installed on the glasses to successfully subscribe to the sensor streams. note Streaming certificates are not the same as the certificates used to pair the SDK with the glasses. You may use ephemeral or persistent streaming certificates. Persistent certificates​ Persistent certificates are generated once and are valid for a year. They may be used to subscribe to streaming data from several different computers. Persistent certificates may be generated and installed on the glasses via the CLI using: aria streaming install-certs  Future streaming sessions will use these certificates by default. The certificates may be used on another machine by copying the certificates from the following directory to the same directory on another machine. ~/.aria/streaming-certs/persistent  Ephemeral certificates​ Ephemeral certificates are generated on the fly for each streaming session and are only valid for that streaming session. These are most useful when you are configuring streaming and subscribing from the same machine. To start streaming with ephemeral certificates from the CLI use aria streaming start *--use-ephemeral-certs*  ","version":"Next","tagName":"h3"},{"title":"Streaming interfaces​","type":1,"pageTitle":"Streaming Internals","url":"/projectaria_tools/docs/ARK/sdk/concepts/streaming_internals#streaming-interfaces","content":"Data may be streamed over Wi-Fi or a USB network interface. Streaming over W-Fi​ Streaming over Wi-Fi provides more flexibility in terms of mobility, but the bandwidth and latency is dependent on the strength of the Wi-Fi connection and contention from other network users. Streaming over USB​ Streaming over USB gives higher bandwidth and a more stable connection. Since the glasses are powered over USB the duration of the streaming session is not limited by the battery.  ","version":"Next","tagName":"h3"},{"title":"Streaming backend​","type":1,"pageTitle":"Streaming Internals","url":"/projectaria_tools/docs/ARK/sdk/concepts/streaming_internals#streaming-backend","content":"We use eProsima implementation of Fast DDS to support streaming between glasses and the SDK. DDS enables automatic discovery of glasses over the network. To support streaming, the following requirements must be met on the network: Multicast UDP must be supported across the networkUDP ports in the range 7000-8000 should be open on your machine Each sensor stream is published over a DDS topic. If multiple Aria glasses are present on the network, a separate topic prefix should be used for each pair of glasses. ","version":"Next","tagName":"h3"},{"title":"Starting streaming​","type":1,"pageTitle":"Streaming Internals","url":"/projectaria_tools/docs/ARK/sdk/concepts/streaming_internals#startingstreaming","content":"","version":"Next","tagName":"h2"},{"title":"With the CLI​","type":1,"pageTitle":"Streaming Internals","url":"/projectaria_tools/docs/ARK/sdk/concepts/streaming_internals#with-the-cli","content":"The Subscribe to Aria Streaming Data code sample shows how to initiate streaming using the CLI over USB or Wi-Fi. ","version":"Next","tagName":"h3"},{"title":"With the SDK​","type":1,"pageTitle":"Streaming Internals","url":"/projectaria_tools/docs/ARK/sdk/concepts/streaming_internals#with-the-sdk","content":"Setting up the glasses for streaming is configured via StreamingManager. There you can configure which sensors should be streamed by setting the profile and configure the security settings for subscribing to the data. Go to the Streaming code example for how to start streaming using the SDK. ","version":"Next","tagName":"h3"},{"title":"Subscribing to data with the SDK​","type":1,"pageTitle":"Streaming Internals","url":"/projectaria_tools/docs/ARK/sdk/concepts/streaming_internals#subscribing-to-data-with-the-sdk","content":"Go to Subscribe to Aria Streaming Data code samples for a code walkthrough of the streaming subscription setup and configuration steps described below. ","version":"Next","tagName":"h2"},{"title":"Setting up the streaming client​","type":1,"pageTitle":"Streaming Internals","url":"/projectaria_tools/docs/ARK/sdk/concepts/streaming_internals#setting-up-the-streaming-client","content":"You can subscribe to data from the glasses using a StreamingClient object. If you have started streaming from the SDK you can obtain a preconfigured StreamingClient from StreamingManager using streamingManager.streamingClient(). If you set up streaming from another CLI or another machine, you will need to set the StreamingSubscriptionConfig appropriately. You may control which sensor streams you subscribe to using the subscriber_data_type configuration option. If you are only using a subset of the streams, this can significantly reduce the bandwidth required for stable streaming and reduce the power consumption of the glasses, extending the length of the streaming session.  ","version":"Next","tagName":"h3"},{"title":"Message queue size​","type":1,"pageTitle":"Streaming Internals","url":"/projectaria_tools/docs/ARK/sdk/concepts/streaming_internals#message-queue-size","content":"For each data type you may set a custom queue size, controlling how many messages will be queued if the message callback is slow. The best setting for this value is application-dependent. If the processing time is bursty and it is important not to drop messages you may wish to set a long queue length. Conversely if processing is known to be slow and you are only interested in the most recent data (for example visualizing the image stream) you may wish to set the queue depth to 1. ","version":"Next","tagName":"h3"},{"title":"Adding an observer to receive data​","type":1,"pageTitle":"Streaming Internals","url":"/projectaria_tools/docs/ARK/sdk/concepts/streaming_internals#adding-an-observer-to-receive-data","content":"Streaming data is provided via callbacks on an observer object you provide to the streaming client. Please be aware that the callbacks will not be invoked from the main thread and may be invoked from a different thread from message to message. If the callback runs slowly, subsequent data on that topic will be queued up to the queue size you have configured. If the queue is full, older data will be dropped. ","version":"Next","tagName":"h3"},{"title":"Access Sensor Profiles Using the CLI or SDK","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/sdk/concepts/sdk_sensor_profiles","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Access Sensor Profiles Using the CLI or SDK","url":"/projectaria_tools/docs/ARK/sdk/concepts/sdk_sensor_profiles#overview","content":"Project Aria glasses have multiple recording profiles, so that you can choose what sensors record or stream data, as well as the settings those sensors use. This page shows how to use the Client SDK or CLI to download profile information and set which profile is used. If a value is not set, Aria glasses will use the default recording profile. If a default recording profile is not set, it will use profile8 when making recordings and profile18 when streaming data. Go to the Recording Profiles page for more information about possible sensor configurations. You can also view all available recording profile details when you select Create a Recording in the Mobile Companion app. In addition to sensor setup, recording profile information can provide insights into how battery and thermal levels can be affected when streaming or recording. ","version":"Next","tagName":"h2"},{"title":"Retrieve list of available sensor profiles​","type":1,"pageTitle":"Access Sensor Profiles Using the CLI or SDK","url":"/projectaria_tools/docs/ARK/sdk/concepts/sdk_sensor_profiles#retrieve-list-of-available-sensor-profiles","content":"The CLI or SDK can be used to retrieve the available sensor profiles for recording or streaming stored on your Aria glasses. To make sure you have access to all possible profiles, make sure your Aria glasses OS are up to date. The available profiles for recording or streaming are the same, but profile12 and profile18 were created specifically for streaming data. ","version":"Next","tagName":"h2"},{"title":"Save streaming/recording profiles​","type":1,"pageTitle":"Access Sensor Profiles Using the CLI or SDK","url":"/projectaria_tools/docs/ARK/sdk/concepts/sdk_sensor_profiles#save-streamingrecording-profiles","content":"The following commands export all profiles as JSON files. There is one JSON file per recording, profile12.json etc. SDKCLI import aria.sdk as aria device_client = aria.DeviceClient() device = device_client.connect() # sensor profiles for recording recording_manager = device.recording_manager recording_profiles = recording_manager.recording_profiles print(*recording_profiles) # sensor profiles for streaming streaming_manager = device.streaming_manager streaming_profiles = streaming_manager.streaming_profiles print(*streaming_profiles)  ","version":"Next","tagName":"h3"},{"title":"Example output​","type":1,"pageTitle":"Access Sensor Profiles Using the CLI or SDK","url":"/projectaria_tools/docs/ARK/sdk/concepts/sdk_sensor_profiles#example-output","content":"Each JSON file will show which sensors are used and how each sensor is configured. profile12.json will contain the following: { &quot;name&quot;: &quot;profile12&quot;, &quot;description&quot;: &quot;Streaming mode with JPEG (RGB 10fps 2MP, SLAM 10fps, ET 10fps QVGA, JPEG; Audio, GPS, Wi-Fi and BLE off)&quot;, &quot;imu1&quot;: { &quot;enabled&quot;: true, &quot;dataRateHz&quot;: 1000 }, &quot;imu2&quot;: { &quot;enabled&quot;: true, &quot;dataRateHz&quot;: 800 }, &quot;magnetometer&quot;: { &quot;enabled&quot;: true, &quot;dataRateHz&quot;: 10 }, &quot;barometer&quot;: { &quot;enabled&quot;: true, &quot;dataRateHz&quot;: 50 }, &quot;audio&quot;: { &quot;enabled&quot;: false, &quot;numChannels&quot;: 0, &quot;sampleRateHz&quot;: &quot;0&quot;, &quot;periodSize&quot;: 0 }, &quot;gps&quot;: { &quot;enabled&quot;: false, &quot;dataRateHz&quot;: 0 }, &quot;ble&quot;: { &quot;enabled&quot;: false, &quot;scanDurationMs&quot;: 0 }, &quot;wifi&quot;: { &quot;enabled&quot;: false, &quot;scanDurationMs&quot;: 0, &quot;wifiScanModeActive&quot;: false, &quot;wifiMinDwellTimeMs&quot;: 0, &quot;wifiMaxDwellTimeMs&quot;: 0 }, &quot;slamCameras&quot;: { &quot;enabled&quot;: true, &quot;width&quot;: 640, &quot;height&quot;: 480, &quot;fps&quot;: 10, &quot;autoExposureEnabled&quot;: true, &quot;exposureMinUs&quot;: &quot;0&quot;, &quot;exposureMaxUs&quot;: &quot;0&quot;, &quot;gainMin&quot;: 0, &quot;gainMax&quot;: 0, &quot;exposureUs&quot;: &quot;0&quot;, &quot;gain&quot;: 0, &quot;irLedEnabled&quot;: false, &quot;imageFormat&quot;: &quot;JPEG&quot;, &quot;jpegEncoderType&quot;: &quot;HARDWARE&quot;, &quot;jpegQuality&quot;: 80, &quot;videoEncoderQp&quot;: 0, &quot;videoCodecType&quot;: &quot;H264&quot;, &quot;targetIntensity&quot;: 0 }, &quot;etCamera&quot;: { &quot;enabled&quot;: true, &quot;width&quot;: 320, &quot;height&quot;: 240, &quot;fps&quot;: 10, &quot;autoExposureEnabled&quot;: false, &quot;exposureMinUs&quot;: &quot;0&quot;, &quot;exposureMaxUs&quot;: &quot;0&quot;, &quot;gainMin&quot;: 0, &quot;gainMax&quot;: 0, &quot;exposureUs&quot;: &quot;1000&quot;, &quot;gain&quot;: 1, &quot;irLedEnabled&quot;: true, &quot;imageFormat&quot;: &quot;JPEG&quot;, &quot;jpegEncoderType&quot;: &quot;HARDWARE&quot;, &quot;jpegQuality&quot;: 80, &quot;videoEncoderQp&quot;: 0, &quot;videoCodecType&quot;: &quot;H264&quot;, &quot;targetIntensity&quot;: 0 }, &quot;rgbCamera&quot;: { &quot;enabled&quot;: true, &quot;width&quot;: 1408, &quot;height&quot;: 1408, &quot;fps&quot;: 10, &quot;autoExposureEnabled&quot;: true, &quot;exposureMinUs&quot;: &quot;0&quot;, &quot;exposureMaxUs&quot;: &quot;0&quot;, &quot;gainMin&quot;: 0, &quot;gainMax&quot;: 0, &quot;exposureUs&quot;: &quot;0&quot;, &quot;gain&quot;: 0, &quot;irLedEnabled&quot;: false, &quot;imageFormat&quot;: &quot;JPEG&quot;, &quot;jpegEncoderType&quot;: &quot;HARDWARE&quot;, &quot;jpegQuality&quot;: 80, &quot;videoEncoderQp&quot;: 0, &quot;videoCodecType&quot;: &quot;H264&quot;, &quot;targetIntensity&quot;: 0 }, &quot;attention&quot;: { &quot;enabled&quot;: false } }  ","version":"Next","tagName":"h3"},{"title":"Use a recording/sensor profile​","type":1,"pageTitle":"Access Sensor Profiles Using the CLI or SDK","url":"/projectaria_tools/docs/ARK/sdk/concepts/sdk_sensor_profiles#use-a-recordingsensor-profile","content":"Once retrieved a profile can be used to start recording or streaming. In this example, we use the sensor profile profile12 obtained from the previous step to start streaming: SDKCLI streaming_manager = device.streaming_manager streaming_config = aria.RecordingConfig() streaming_config.profile_name = &quot;profile12&quot; streaming_manager.streaming_config = streaming_config streaming_manager.start_streaming()  ","version":"Next","tagName":"h2"},{"title":"Project Aria Client SDK Code Samples","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/sdk/samples","content":"Project Aria Client SDK Code Samples This section provides code samples and walkthroughs for using the Aria Client SDK to interact with the Project Aria glasses. Connection Connect to the Aria glasses using USB and Wi-FiFetch device information, such as the device serial numberFetch device status, such as battery level or Wi-Fi SSID. Recording Start and stop recording on an Aria device via USB and Wi-FiSet a recording profile for a recording Streaming Subscription Subscribe to and unsubscribe from a streaming sessionVisualize the live stream Streaming and Visualizing All Live Sensor Data Start and stop streaming from an Aria device to a computer via USB and Wi-FiAdd callbacks to visualize and manipulate the streamed data Streaming Undistorted RGB Image Using Calibration Start and stop streaming from an Aria device to a computer via USB and Wi-FiAccess sensor calibration and undistort an RGB live stream Make sure you have finished the Setup Guide. We recommend starting with device_connect.py to validate that your Aria glasses have connected successfully.","keywords":"","version":"Next"},{"title":"Make a Recording Using the Client SDK","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/sdk/samples/device_recording","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Make a Recording Using the Client SDK","url":"/projectaria_tools/docs/ARK/sdk/samples/device_recording#overview","content":"This device_record example shows how to set a recording profile, start and stop recording using the Project Aria Client SDK. ","version":"Next","tagName":"h2"},{"title":"Running the sample​","type":1,"pageTitle":"Make a Recording Using the Client SDK","url":"/projectaria_tools/docs/ARK/sdk/samples/device_recording#running-the-sample","content":"","version":"Next","tagName":"h2"},{"title":"Example 1: Start and stop recording over USB​","type":1,"pageTitle":"Make a Recording Using the Client SDK","url":"/projectaria_tools/docs/ARK/sdk/samples/device_recording#example-1-start-and-stop-recording-over-usb","content":"Use the SDK to establish a USB connection, start a 10 second recording using the default recording profile, then stop the recording. Plug your Aria glasses into your computer, using the provided USB cableFrom the samples directory in Terminal, run: python -m device_record  The recording LED will show on your Aria glassesAfter 10 seconds the recording will stop and be stored in your Aria glasses View the recording metadata in the Recordings tab of the Mobile Companion appRun adb shell ls -l /sdcard/recording and you should see your newly recorded fileRun adb pull /sdcard/recording/myVrsFile.vrs myLocalFolder/ to download the VRS fileRun adb pull /sdcard/recording/myVrsFile.json myLocalFolder/ to download the VRS metadataVRS files can be visualized using the Aria Viewer ","version":"Next","tagName":"h3"},{"title":"Example 2: Start and stop recording using a custom sensor profile​","type":1,"pageTitle":"Make a Recording Using the Client SDK","url":"/projectaria_tools/docs/ARK/sdk/samples/device_recording#example-2-start-and-stop-recording-using-a-custom-sensor-profile","content":"Use Project Aria Recording Profiles to choose among different sensor configurations. To set a specific profile, run: python -m device_record --profile profile15  ","version":"Next","tagName":"h3"},{"title":"Example 3: Start and stop recording over Wi-Fi​","type":1,"pageTitle":"Make a Recording Using the Client SDK","url":"/projectaria_tools/docs/ARK/sdk/samples/device_recording#example-3-start-and-stop-recording-over-wi-fi","content":"Recording over Wi-Fi is similar with Example 1, with a few extra steps. Connect your Aria glasses and computer to the same compatible Wi-Fi network: Check Requirements for details about compatible routers Open the Mobile Companion app and tap Wi-Fi on the Dashboard to see your glasses' IP addressFrom the samples directory in Terminal, run: python -m device_record --device-ip &lt;Glasses IP&gt;  Make sure you replace &lt;Glasses IP&gt; with the IP address you got from the Mobile Companion app The recording LED will show on your Aria glassesAfter 10 seconds the recording will stop and be stored in your Aria glasses. Follow the same step in Example 1 to pull the data to your computer ","version":"Next","tagName":"h3"},{"title":"Code walkthrough​","type":1,"pageTitle":"Make a Recording Using the Client SDK","url":"/projectaria_tools/docs/ARK/sdk/samples/device_recording#code-walkthrough","content":"1. Retrieve RecordingManager instance​ Use RecordingManager to start and stop a recording and get access to the calibration data. recording_manager = device.recording_manager  2. Configure recording settings​ Set the profile name for the recording in RecordingConfig or you'll use your glasses' default recording profile. If a default recording profile has not been set, you'll record with profile8. Go to the Sensor Profiles page for supported profiles and their specs. recording_config = aria.RecordingConfig() # If set, profile_name takes precedence over the default profile recording_config.profile_name = args.profile_name recording_manager.recording_config = recording_config  3. Start recording for specific amount of time​ recording_manager.start_recording() time.sleep(args.recording_duration) recording_manager.stop_recording()  info You can also stop recording using the Capture button on the glasses, Mobile Companion app or CLI. ","version":"Next","tagName":"h3"},{"title":"Useful links​","type":1,"pageTitle":"Make a Recording Using the Client SDK","url":"/projectaria_tools/docs/ARK/sdk/samples/device_recording#useful-links","content":"Glasses Quickstart Guide: explore other ways to make recordings. ","version":"Next","tagName":"h2"},{"title":"Subscribe to Aria Streaming Data","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/sdk/samples/streaming_subscribe","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Subscribe to Aria Streaming Data","url":"/projectaria_tools/docs/ARK/sdk/samples/streaming_subscribe#overview","content":"This streaming_subscribe example shows how to subscribe to and unsubscribe from a streaming session as well as visualize the live stream, using the Project Aria Client SDK. ","version":"Next","tagName":"h2"},{"title":"Stream and subscribe examples​","type":1,"pageTitle":"Subscribe to Aria Streaming Data","url":"/projectaria_tools/docs/ARK/sdk/samples/streaming_subscribe#stream-and-subscribe-examples","content":"","version":"Next","tagName":"h2"},{"title":"Example 1: Stream and subscribe over USB​","type":1,"pageTitle":"Subscribe to Aria Streaming Data","url":"/projectaria_tools/docs/ARK/sdk/samples/streaming_subscribe#example-1-stream-and-subscribe-over-usb","content":"In this example, the CLI is used to initiate streaming and the Client SDK is used to subscribe to the stream. To find out how to start streaming using the SDK, go to Streaming Sensor Data. Plug your Aria glasses into your computer, using the provided cableFrom the samples directory in Terminal, run: aria streaming start --interface usb --use-ephemeral-certs  Wait for the stream to start then run: python -m streaming_subscribe  You should then see:  There are several ways you can stop streaming: Press q or ESC to quit the appUse Ctrl-C to terminate in terminalPress the Capture button on your glasses ","version":"Next","tagName":"h3"},{"title":"Example 2: Using Wi-Fi​","type":1,"pageTitle":"Subscribe to Aria Streaming Data","url":"/projectaria_tools/docs/ARK/sdk/samples/streaming_subscribe#example-2-using-wi-fi","content":"To use Wi-Fi to initiate streaming or to stream data, alter the aria streaming start --interface usb --use-ephemeral-certs command. To stream data over Wi-Fi, use --interface wifiTo initiate streaming over Wi-Fi, add --device-ip &lt;glasses IP&gt; Open the Mobile Companion app and tap Wi-Fi on the Dashboard to see your glasses' IP address ","version":"Next","tagName":"h3"},{"title":"Code walkthrough​","type":1,"pageTitle":"Subscribe to Aria Streaming Data","url":"/projectaria_tools/docs/ARK/sdk/samples/streaming_subscribe#code-walkthrough","content":"1. Configure the subscription​ Use subscriber_data_type attribute to set the type of data the client subscribes to. config = streaming_client.subscription_config config.subscriber_data_type = ( aria.StreamingDataType.Rgb | aria.StreamingDataType.Slam )  2. Set message queue size​ The message queue size determines how many recent frames will be retained. A smaller queue size is utilized to process only the most recent data. config.message_queue_size[aria.StreamingDataType.Rgb] = 1 config.message_queue_size[aria.StreamingDataType.Slam] = 1  3. Set streaming security options​ Security options are set to use ephemeral certificates through a StreamingSecurityOptions instance. Go to the Streaming Internals page for various aspects of streaming security and how certificates are used. options = aria.StreamingSecurityOptions() options.use_ephemeral_certs = True config.security_options = options  4. Create an StreamingClient observer and attach it​ Find more description of observer in the streaming code sample class StreamingClientObserver: def __init__(self): self.images = {} def on_image_received(self, image: np.array, record: ImageDataRecord): self.images[record.camera_id] = image observer = StreamingClientObserver() streaming_client.set_streaming_client_observer(observer)  5. Start subscribing and listen to the live stream​ The client begins listening for incoming streaming data from the subscribed data types. streaming_client.subscribe()  6. Visualize the live stream​ RGB and SLAM images are visualized in separate windows using OpenCV. The images are processed and displayed the streaming stops or the application quit. We rotate the image and stack the SLAM images so that they are shown in a single window. while not quit_keypress(): # Render the RGB image if aria.CameraId.Rgb in observer.images: rgb_image = np.rot90(observer.images[aria.CameraId.Rgb], -1) rgb_image = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2RGB) cv2.imshow(rgb_window, rgb_image) del observer.images[aria.CameraId.Rgb] # Stack and display the SLAM images if ( aria.CameraId.Slam1 in observer.images and aria.CameraId.Slam2 in observer.images ): slam1_image = np.rot90(observer.images[aria.CameraId.Slam1], -1) slam2_image = np.rot90(observer.images[aria.CameraId.Slam2], -1) cv2.imshow(slam_window, np.hstack((slam1_image, slam2_image))) del observer.images[aria.CameraId.Slam1] del observer.images[aria.CameraId.Slam2]  note Cameras on Aria glasses are installed sideways. The visualizer rotates the raw image data for a more natural view. ","version":"Next","tagName":"h3"},{"title":"7. Unsubscribe from the stream and free resources​","type":1,"pageTitle":"Subscribe to Aria Streaming Data","url":"/projectaria_tools/docs/ARK/sdk/samples/streaming_subscribe#7-unsubscribe-from-the-stream-and-free-resources","content":"Unsubscribing stops the client from listening to streaming data and cleans up resources. streaming_client.unsubscribe()  ","version":"Next","tagName":"h3"},{"title":"Client SDK & CLI Troubleshooting & Known Issues","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/sdk/sdk_troubleshooting","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Client SDK & CLI Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/sdk/sdk_troubleshooting#overview","content":"This page provides troubleshooting information for using Project Aria Client SDK or CLI. If you cannot find a solution for your problem on this page, go to the Support page for how to contact our team. ","version":"Next","tagName":"h2"},{"title":"Aria Doctor​","type":1,"pageTitle":"Client SDK & CLI Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/sdk/sdk_troubleshooting#aria-doctor","content":"The Project Aria Doctor utility can help detect and resolve common issues connecting and streaming from the glasses. Run the utility using the following command and follow the prompts to resolve any issues. aria-doctor  ","version":"Next","tagName":"h2"},{"title":"Connection and pairing issues​","type":1,"pageTitle":"Client SDK & CLI Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/sdk/sdk_troubleshooting#connection-and-pairing-issues","content":"","version":"Next","tagName":"h2"},{"title":"Computer can't find Aria Glasses​","type":1,"pageTitle":"Client SDK & CLI Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/sdk/sdk_troubleshooting#computer-cant-find-aria-glasses","content":"It may be that the battery is drained, make sure your Aria Glasses are correctly charging (there should be a blue LED on the right arm) and wait ten minutes. On Linux, this may be due to USB driver issues. Run adb kill-server &amp;&amp; adb start-server and aria-doctor, then try connecting to Aria glasses again. ","version":"Next","tagName":"h3"},{"title":"Mobile app doesn't receive authorization​","type":1,"pageTitle":"Client SDK & CLI Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/sdk/sdk_troubleshooting#mobile-app-doesnt-receive-authorization","content":"If you use aria auth pair and don't receive an authorization prompt in the Mobile Companion app, try the following steps. Make sure the Mobile Companion app is in the foreground and try againRestart the Mobile Companion app and try againRevoke any existing certificates (via Device Settings) and try againMake sure you're using venv as your virtual environment Users have experienced difficulties using the Client SDK in other virtual environments, such as Conda If you've paired multiple Aria glasses to the one account​ If you've paired multiple Aria glasses to the one account, the wrong glasses may be connected to the app. Tap Select other on the top right of the dashboard If you see Add glasses instead, only one set of Aria glasses is connected to this account Tap the glasses connected to your computer If you're not sure which glasses you're using: Go to Device Info in the Mobile Companion app to find out the serial number of the glasses that are currently connected to the Mobile Companion appTo find out the serial number of the glasses connected to your computer The serial number is printed on the right arm of the glasses, near the privacy switch (go to the Glasses Manual for screenshots of where it is)In Terminal, use adb devices (ADB is part of Android Studio) The device number returned is the serial number ","version":"Next","tagName":"h3"},{"title":"Streaming or Recording Issues​","type":1,"pageTitle":"Client SDK & CLI Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/sdk/sdk_troubleshooting#streaming-or-recording-issues","content":"","version":"Next","tagName":"h2"},{"title":"Lost Internet Connection (MacOS)​","type":1,"pageTitle":"Client SDK & CLI Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/sdk/sdk_troubleshooting#lost-internet-connection-macos","content":"If you lose internet connection on MacOS while streaming, run aria-doctor in a separate terminal. ","version":"Next","tagName":"h3"},{"title":"Can't start streaming/recording. RuntimeError: (9) Failed to read data from socket: Operation canceled​","type":1,"pageTitle":"Client SDK & CLI Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/sdk/sdk_troubleshooting#cant-start-streamingrecording-runtimeerror-9-failed-to-read-data-from-socket-operation-canceled","content":"You may encounter this error message if you: Tried to start streaming/recording and the Privacy Switch was engagedTurned off the Privacy SwitchImmediately tried to start recording again The Aria glasses were still switching modes. Please try again. ","version":"Next","tagName":"h3"},{"title":"Streaming is laggy/ only some streaming visualizations appear/ visualizer is blank​","type":1,"pageTitle":"Client SDK & CLI Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/sdk/sdk_troubleshooting#streaming-is-laggy-only-some-streaming-visualizations-appear-visualizer-is-blank","content":"This issue can occur for several reasons. Corporate or VPN interference (even if streaming via ADB)​ This issue may occur if you're on a VPN or corporate network. Even if you're only streaming via ADB, some security protocols may interfere. Aria Glasses and Computer on different Wi-Fi networks​ info This can occur if you know you've set both devices to the same network! Devices will sometimes preferentially switch back to Wi-Fi connections with stronger signal strength, so you may need to forget a corporate network on the Aria Glasses or computer. How to adjust Wi-Fi settings via the mobile app​ Open the Aria Mobile Companion AppIn the Paired Glasses section of the Dashboard, select Select Wi-FiSelect your preferred network and follow the prompts to connect You can also forget an existing network from the Wi-Fi menuMake sure it is a non-corporate network that is the same as your computerThe glasses Wi-Fi network is independent of your phone's Wi-Fi network Once connected, the Wi-Fi network name will appear in the Desktop App under My Device underneath the WiFi icon which will become blue. Resources are tied up​ You may have a previous streaming session running. Follow the device_stream.py or streaming_subscribe.py instructions to stop the stream and free resources. Linux connection issue​ If streaming on Linux does not show any data, try the following steps to address connection issues: Make sure that you have run aria-doctorMake sure that the UDP ports used for streaming are not blocked by your machine firewall. You can add an iptable entry to open these ports with the following commands sudo iptables -A INPUT -p udp -m udp --dport 7000:8000 -j ACCEPT  Ensure that during streaming, USB Ethernet tab in the &quot;Network Settings Window&quot; has Aria selected ","version":"Next","tagName":"h3"},{"title":"Other Useful Links​","type":1,"pageTitle":"Client SDK & CLI Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/sdk/sdk_troubleshooting#other-useful-links","content":"Aria Glasses User Manual for general information about using your glasses.Aria Research Kit (ARK) Troubleshooting and Known Issues page for general Aria Glasses troubleshooting information. ","version":"Next","tagName":"h2"},{"title":"Streaming Undistorted RGB Image Using Calibration","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/sdk/samples/undistort_rgb_image","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Streaming Undistorted RGB Image Using Calibration","url":"/projectaria_tools/docs/ARK/sdk/samples/undistort_rgb_image#overview","content":"This page shows how to run the code sample undistort_rgb_image to: Access a Project Aria Tools type device calibration objectUse core data utilities in projectaria_tools to undistort streamed camera data ","version":"Next","tagName":"h2"},{"title":"Run undistort_rgb_image​","type":1,"pageTitle":"Streaming Undistorted RGB Image Using Calibration","url":"/projectaria_tools/docs/ARK/sdk/samples/undistort_rgb_image#run-undistort_rgb_image","content":"Plug your Aria glasses into your computer, using the provided USB cableFrom the samples directory in Terminal, run: python -m undistort_rgb_image --interface usb --update_iptables  info Use --interface wifi to stream over Wi-FI  ","version":"Next","tagName":"h2"},{"title":"Code walkthrough​","type":1,"pageTitle":"Streaming Undistorted RGB Image Using Calibration","url":"/projectaria_tools/docs/ARK/sdk/samples/undistort_rgb_image#code-walkthrough","content":"The code walkthrough for undistort_rgb_image.py is similar to device_stream.py, but has 2 key differences: 1. Access sensor calibration​ Once the sensors have been configured, the recording manager can provide the sensor calibration data for those settings. sensors_calib_json = streaming_manager.sensors_calibration()  2. Use Project Aria Tools for calibration operations​ A Project Aria Tools type device calibration object can then be retrieved by using the device_calibration_from_json_string function: from projectaria_tools.core.calibration import ( device_calibration_from_json_string, distort_by_calibration, get_linear_camera_calibration, ) sensors_calib = device_calibration_from_json_string(sensors_calib_json) rgb_calib = sensors_calib.get_camera_calib(&quot;camera-rgb&quot;)  Get a linear camera calibration object to be used in RGB image undistortion: dst_calib = get_linear_camera_calibration(512, 512, 150, &quot;camera-rgb&quot;)  To find out more about how to use sensor calibration, go to the Accessing Sensor Calibration page. 3. Undistort and visualize the live RGB image stream​ Unlike device_stream.py that uses custom streaming client observer class, undistort_rgb_image.py uses a simple streaming client observer class to define callbacks: class StreamingClientObserver: def __init__(self): self.rgb_image = None def on_image_received(self, image: np.array, record: ImageDataRecord): self.rgb_image = image  Undistort the RGB image using distort_by_calibration and visualize it in a while loop. The camera RGB image and the undistorted RGB image are visualized in separate windows using OpenCV. The images are processed and displayed the streaming stops or the application quit. while not (quit_keypress() or ctrl_c): if observer.rgb_image is not None: rgb_image = cv2.cvtColor(observer.rgb_image, cv2.COLOR_BGR2RGB) cv2.imshow(rgb_window, np.rot90(rgb_image, -1)) # Apply the undistortion correction undistorted_rgb_image = distort_by_calibration( rgb_image, dst_calib, rgb_calib ) # Show the undistorted image cv2.imshow(undistorted_window, np.rot90(undistorted_rgb_image, -1)) observer.rgb_image = None  note Cameras on Aria glasses are installed sideways. The visualizer rotates the raw image data for a more natural view. ","version":"Next","tagName":"h3"},{"title":"Project Aria Client SDK and CLI Setup Guide","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/sdk/setup","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Project Aria Client SDK and CLI Setup Guide","url":"/projectaria_tools/docs/ARK/sdk/setup#overview","content":"The page provides instructions about how to get started with the Project Aria Client SDK, covering: Hardware and software requirementsDownloading and installing the SDK Installing projectaria_client_sdk via pip will also add the Aria CLI to your PATH Running Project Aria Doctor to setup your computer and fix common issuesPairing your Aria GlassesExtracting and exploring the sample apps  ","version":"Next","tagName":"h2"},{"title":"Requirements​","type":1,"pageTitle":"Project Aria Client SDK and CLI Setup Guide","url":"/projectaria_tools/docs/ARK/sdk/setup#requirements","content":"","version":"Next","tagName":"h2"},{"title":"Hardware​","type":1,"pageTitle":"Project Aria Client SDK and CLI Setup Guide","url":"/projectaria_tools/docs/ARK/sdk/setup#hardware","content":"Project Aria glasses that have: Completed full device setup using the Aria Mobile Companion AppLatest up-to-date OS If you want to stream over Wi-Fi, you'll need a router, such as Asus, Netgear or TP-Link, that has: No firewallSupports Wi-Fi 6 So that the glasses can connect to the 5GHz band when streaming over Wi-Fi danger The Client SDK does not currently support streaming over corporate, university or public networks. Those networks are protected by many layers of security and firewalls. We recommend using one of the recommended routers listed above to stream over Wi-Fi. ","version":"Next","tagName":"h3"},{"title":"Platforms​","type":1,"pageTitle":"Project Aria Client SDK and CLI Setup Guide","url":"/projectaria_tools/docs/ARK/sdk/setup#platforms","content":"The codebase is supported on the following platforms: x64 Linux distributions of: Fedora 36 or newerUbuntu jammy (22.04) or newer Mac Intel or Mac ARM-based (M1) with MacOS 11 (Big Sur) or newer ","version":"Next","tagName":"h3"},{"title":"Software​","type":1,"pageTitle":"Project Aria Client SDK and CLI Setup Guide","url":"/projectaria_tools/docs/ARK/sdk/setup#software","content":"Python 3 with versions &gt;= 3.8.10 and &lt;= 3.11 Python 3.9+ if you want to use the device_stream code sample due to the fastplotlib dependencyPython 3 download pageTo check which version of Python 3 you have, use python3 --version ADB (optional) In addition to the CLI, you can use ADB to interact with Aria glassesADB is one of the ways that you can download Aria data  ","version":"Next","tagName":"h3"},{"title":"Environment Setup​","type":1,"pageTitle":"Project Aria Client SDK and CLI Setup Guide","url":"/projectaria_tools/docs/ARK/sdk/setup#environment-setup","content":"","version":"Next","tagName":"h2"},{"title":"Step 1: Install SDK from PyPi​","type":1,"pageTitle":"Project Aria Client SDK and CLI Setup Guide","url":"/projectaria_tools/docs/ARK/sdk/setup#step-1-install-sdk-from-pypi","content":"","version":"Next","tagName":"h2"},{"title":"Create a virtual environment​","type":1,"pageTitle":"Project Aria Client SDK and CLI Setup Guide","url":"/projectaria_tools/docs/ARK/sdk/setup#create-a-virtual-environment","content":"When using pip, it is best practice to use a virtual environment. This will keep all the modules under one folder and will not break your local environment. Use the following command with your version of Python3. python3 -m venv ~/venv  ","version":"Next","tagName":"h3"},{"title":"Install the Client SDK and CLI​","type":1,"pageTitle":"Project Aria Client SDK and CLI Setup Guide","url":"/projectaria_tools/docs/ARK/sdk/setup#install-the-client-sdk-and-cli","content":"Install projectaria_client_sdk with pip source ~/venv/bin/activate python -m pip install projectaria_client_sdk --no-cache-dir  ","version":"Next","tagName":"h3"},{"title":"Step 2: Run Project Aria Doctor utility​","type":1,"pageTitle":"Project Aria Client SDK and CLI Setup Guide","url":"/projectaria_tools/docs/ARK/sdk/setup#step-2-run-project-aria-doctor-utility","content":"The Project Aria Doctor utility can help detect and resolve common issues connecting and streaming from the glasses. Run the utility and follow the prompts to resolve any issues. aria-doctor  info If you're on MacOS and lose internet connection while streaming, run aria-doctor again.  ","version":"Next","tagName":"h2"},{"title":"Step 3: Pair Aria Glasses with your computer​","type":1,"pageTitle":"Project Aria Client SDK and CLI Setup Guide","url":"/projectaria_tools/docs/ARK/sdk/setup#step-3-pair-aria-glasses-with-your-computer","content":"Pairing your Aria glasses to your computer allows the Client SDK and CLI to control the glasses. A pair of Aria glasses can be paired to multiple computers. Turn on your Aria glasses and connect it to your computer using the provided USB cableOpen the Mobile Companion app on your phoneOn your computer, run: aria auth pair  A prompt should then appear in the Mobile app, tap Approve to pair your glasses The hash in the terminal and the app should be the sameView (or revoke) certificates by going to Device SettingsThe Client SDK Certificate will remain valid until you manually revoke it or factory reset your glasses  info At this point, you can now use the Aria CLI to interact with you Aria glasses.  ","version":"Next","tagName":"h2"},{"title":"Step 4: Extract and explore the sample apps​","type":1,"pageTitle":"Project Aria Client SDK and CLI Setup Guide","url":"/projectaria_tools/docs/ARK/sdk/setup#step-4-extract-and-explore-the-sample-apps","content":"Extract the Client SDK code samples (here to your home directory) python -m aria.extract_sdk_samples --output ~  Navigate to the sample folder cd ~/projectaria_client_sdk_samples  Install necessary dependencies: pip install -r requirements.txt  Go to Code Samples to explore Aria Client SDK features. Connection: connect an Aria device to a computer, fetch the device information and status.Recording: start and stop a recording via USB and Wi-Fi.Streaming Subscription: subscribe to and unsubscribe from a streaming deviceStreaming and Visualizing All Live Sensor Data: programmatically start and stop a streaming session, add callbacks to visualize and manipulate the streamed dataStreaming Undistorted RGB Image Using Calibration: programmatically start and stop a streaming session, access sensor calibration and undistort an RGB live stream Go to Streaming Internals to understand how streaming works and how to configure your own streaming setup. If you encounter any issues please run aria-doctor in a separate terminal or check out troubleshooting. info You can check your Aria glasses' recording or streaming status in the Mobile Companion app. danger The Client SDK does not currently support streaming over corporate, university or public networks. Those networks are protected by many layers of security and firewalls. We recommend using one of the recommended routers listed above to stream over Wi-Fi. ","version":"Next","tagName":"h2"},{"title":"Useful Links​","type":1,"pageTitle":"Project Aria Client SDK and CLI Setup Guide","url":"/projectaria_tools/docs/ARK/sdk/setup#useful-links","content":"Streaming InternalsSDK API Reference - full list of APIsCLI Command ReferenceSDK &amp; CLI Troubleshooting ","version":"Next","tagName":"h2"},{"title":"Streaming and Visualizing All Live Sensor Data","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/sdk/samples/device_stream","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Streaming and Visualizing All Live Sensor Data","url":"/projectaria_tools/docs/ARK/sdk/samples/device_stream#overview","content":"This page shows how to run the code sample device_stream to: Start and stop streaming data from Aria glasses over USB and Wi-FiVisualize the sensor streams from Aria's RGB, Mono Scene (SLAM), ET cameras and IMU sensors note These code samples require Python 3.9+ because of a fastplotlib dependency ","version":"Next","tagName":"h2"},{"title":"Run device_stream​","type":1,"pageTitle":"Streaming and Visualizing All Live Sensor Data","url":"/projectaria_tools/docs/ARK/sdk/samples/device_stream#run-device_stream","content":"","version":"Next","tagName":"h2"},{"title":"Example 1: Start and stop streaming over USB​","type":1,"pageTitle":"Streaming and Visualizing All Live Sensor Data","url":"/projectaria_tools/docs/ARK/sdk/samples/device_stream#example-1-start-and-stop-streaming-over-usb","content":"Plug your Aria glasses into your computer, using the provided USB cableFrom the samples directory in Terminal, run: python -m device_stream --interface usb --update_iptables  You should then see:  note Cameras on Aria glasses are installed sideways. The visualizer rotates the raw image data for a more natural view. There are several ways you can stop streaming: Press q or ESC to quit the appUse Ctrl-C to terminate in terminalPress the Capture button on your glasses ","version":"Next","tagName":"h3"},{"title":"Example 2: Start and stop streaming over Wi-Fi​","type":1,"pageTitle":"Streaming and Visualizing All Live Sensor Data","url":"/projectaria_tools/docs/ARK/sdk/samples/device_stream#example-2-start-and-stop-streaming-over-wi-fi","content":"Streaming over Wi-Fi is similar, with a few extra steps. Connect your Aria Glasses and Computer to the same Wi-Fi compatible Wi-Fi network: Check Requirements for details about compatible routers Open the Mobile Companion app and tap Wi-Fi on the Dashboard to see your glasses' IP addressFrom the samples directory in Terminal, run: python -m device_stream --interface wifi --device-ip &lt;Glasses IP&gt; --update_iptables  Make sure you replace &lt;Glasses IP&gt; with the IP address you got from the Mobile Companion app If you're on MacOS and lose internet connection, run aria-doctor in a separate terminal. Stop streaming using any of the methods listed in Example 1 ","version":"Next","tagName":"h3"},{"title":"Code walkthrough​","type":1,"pageTitle":"Streaming and Visualizing All Live Sensor Data","url":"/projectaria_tools/docs/ARK/sdk/samples/device_stream#code-walkthrough","content":"1. Get StreamingManager instance​ Use StreamingManager to start and stop streaming. streaming_manager = device.streaming_manager  2. Configure Streaming Settings (optional)​ Specify the profile name for the streaming in StreamingConfig, otherwise you'll use your glasses' default recording profile. If a default recording profile has not been set, you'll stream using profile18. Go to the Sensor Profiles page for supported profiles and their specs. streaming_config = StreamingConfig() streaming_config.profile_name = args.profile_name  Use ephemeral streaming certificates to protect your streamed data from being eavesdropped by other computer without certificate. This needs to be set to true when no persistent certificates were pre-installed. Go to the Streaming Internals page on the various aspects of streaming security and how certificates are used. streaming_config.security_options.use_ephemeral_certs = True streaming_manager.streaming_config = streaming_config  3. Start Streaming​ Start streaming using all the previously set configurations. streaming_manager.start_streaming()  4. Write callbacks for each sensor data stream​ Create an observer class derived from BaseStreamingClientObserver defined in visualizer.py. Find the complex example named AriaVisualizerStreamingClientObserver in visualizer.py, which is used in this sample app to fetch and visualize data from each sensor. Find a simpler example in undistort_rgb_image.py to fetch RGB data from the stream: class StreamingClientObserver: def __init__(self): self.rgb_image = None def on_image_received(self, image: np.array, record: ImageDataRecord): self.rgb_image = image  5. Register callbacks in the streaming client​ Once your observer class is defined, you will need to register it with a stream client object. # get streaming client from the streaming manager streaming_client = streaming_manager.streaming_client # create custom visualizer and streaming client observer class instances aria_visualizer = AriaVisualizer() aria_visualizer_streaming_client_observer = AriaVisualizerStreamingClientObserver(aria_visualizer) # register callbacks streaming_client.set_streaming_client_observer(aria_visualizer_streaming_client_observer)  6. Visualize live stream​ Subscribe to the live stream data and enter the rendering loop to visualize the streaming data until the window is closed. streaming_client.subscribe() aria_visualizer.render_loop()  7. Stop the stream &amp; free resources​ Once you've finished streaming, unsubscribe from StreamingClient instance, so that any held resources can be released, stop streaming and disconnect the device. streaming_client.unsubscribe() streaming_manager.stop_streaming() device_client.disconnect(device)  ","version":"Next","tagName":"h3"},{"title":"How to Capture Logs for the Aria Desktop App","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/troubleshooting/desktop_app_logs","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"How to Capture Logs for the Aria Desktop App","url":"/projectaria_tools/docs/ARK/troubleshooting/desktop_app_logs#overview","content":"This page provides information about how to gather logs from the Aria Desktop Companion app. Logs are not necessary prior to filing a bug report or seeking technical support, but they can be helpful. People are most likely to use this page if they have been asked for log files by Project Aria User Support (AriaOps@meta.com). You turn on logging when you run the app, you cannot gather logs retrospectively. You will need to use these commands every time you want to run the Desktop app with logging enabled. ","version":"Next","tagName":"h2"},{"title":"Enabling logs​","type":1,"pageTitle":"How to Capture Logs for the Aria Desktop App","url":"/projectaria_tools/docs/ARK/troubleshooting/desktop_app_logs#enabling-logs","content":"On MacOS or Windows, enable logging by closing the Desktop app and then relaunching it via the command line with logging turned on. Logs will be added to aria_output.log, which will be created in your user home directory. If you don't fully quit the app, there is a risk your system may open an existing instance with logging turned off, instead of opening a new instance with logging enabled ","version":"Next","tagName":"h2"},{"title":"MacOS​","type":1,"pageTitle":"How to Capture Logs for the Aria Desktop App","url":"/projectaria_tools/docs/ARK/troubleshooting/desktop_app_logs#macos","content":"Quit the Desktop app, if it is runningOpen your terminal and run: open /Applications/Aria.app --args --log-output The Aria Desktop app should then open with logging enabledTo view your logs, go to your user home folder in Finder (Shift + Command + H) where you will find aria_output.logLogs will continue to be added to this file until you quit the appIf you generate logs at a later time, they will be appended to the end of these logs ","version":"Next","tagName":"h3"},{"title":"Windows​","type":1,"pageTitle":"How to Capture Logs for the Aria Desktop App","url":"/projectaria_tools/docs/ARK/troubleshooting/desktop_app_logs#windows","content":"Quit the Desktop app completely Close the app by right-clicking the Aria logo (it looks like glasses) in the right end side of the taskbar, and selecting &quot;Quit&quot; orOpen Task Manager and end the &quot;AriaHub.exe&quot; running task. Use the run command (Windows + R) or Windows terminal to run: &quot;C:\\Program Files\\Aria\\v3\\AriaHub.exe&quot; --log-output The Aria Desktop app should then open with logging enabled.To view your logs, open File Explorer and go to C:\\Users\\&lt;myusername&gt;. There you will find aria_output.logLogs will continue to be added to this file until you quit the appIf you generate logs at a later time, they will be appended to the end of these logs ","version":"Next","tagName":"h3"},{"title":"How to Clear the Desktop App's Cache","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/troubleshooting/clear_cache","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"How to Clear the Desktop App's Cache","url":"/projectaria_tools/docs/ARK/troubleshooting/clear_cache#overview","content":"This page provides information about how to clear Project Aria's Desktop Companion App's cache. Users may wish to do this if they're encountering issues signing into the app, experiencing instabilities in the app or are directed to do so by User Support to resolve an issue. Please note, clearing the cache will clear all your history in Recordings &gt; Tools, so you will no longer be able to download previously generated MPS. ","version":"Next","tagName":"h2"},{"title":"Clear the Cache​","type":1,"pageTitle":"How to Clear the Desktop App's Cache","url":"/projectaria_tools/docs/ARK/troubleshooting/clear_cache#clear-the-cache","content":"Use the following commands: ","version":"Next","tagName":"h2"},{"title":"MacOS​","type":1,"pageTitle":"How to Clear the Desktop App's Cache","url":"/projectaria_tools/docs/ARK/troubleshooting/clear_cache#macos","content":"rm -f /Users/unixname/Library/Preferences/com.meta.Aria.plist &amp;&amp; killall -u unixname cfprefsd  ","version":"Next","tagName":"h3"},{"title":"Windows​","type":1,"pageTitle":"How to Clear the Desktop App's Cache","url":"/projectaria_tools/docs/ARK/troubleshooting/clear_cache#windows","content":"Remove-Item -Path 'HKCU:\\SOFTWARE\\Meta\\Aria\\'  Or open the Registry Editor and go to HKEY_CURRENT_USER-&gt;SOFTWARE-&gt;Meta then delete Aria ","version":"Next","tagName":"h3"},{"title":"Linux​","type":1,"pageTitle":"How to Clear the Desktop App's Cache","url":"/projectaria_tools/docs/ARK/troubleshooting/clear_cache#linux","content":"rm -f /home/unixname/.config/Meta/Aria.conf  ","version":"Next","tagName":"h3"},{"title":"How to Reduce VRS File Size","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/troubleshooting/reduce_vrs_file_size","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"How to Reduce VRS File Size","url":"/projectaria_tools/docs/ARK/troubleshooting/reduce_vrs_file_size#overview","content":"If an MPS request times out, you may need to reduce the size of the VRS file or try using the MPS CLI Finding a faster internet connection may also help. Because VRS files store each sensor stream separately, you can use VRS tooling to create copies of VRS files that do not include specific sensor streams. The MPS output can then be used in combination with the original VRS file (with all sensor streams). MPS does not need the following sensor streams that contain a lot of data: RGB Sensor Streams (214-1) Should more than halve your file sizeJust removing this stream may be sufficient Microphone Sensor Streams (231-1), contains 8 audio channels ","version":"Next","tagName":"h2"},{"title":"Instructions​","type":1,"pageTitle":"How to Reduce VRS File Size","url":"/projectaria_tools/docs/ARK/troubleshooting/reduce_vrs_file_size#instructions","content":"","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"How to Reduce VRS File Size","url":"/projectaria_tools/docs/ARK/troubleshooting/reduce_vrs_file_size#prerequisites","content":"These instructions use the VRS tools to create a VRS file without a specific sensor stream. Install and Build VRS ","version":"Next","tagName":"h3"},{"title":"Command​","type":1,"pageTitle":"How to Reduce VRS File Size","url":"/projectaria_tools/docs/ARK/troubleshooting/reduce_vrs_file_size#command","content":"In this example, a copy of the VRS file is created that does not include RGB Sensor Streams (214-1) vrs copy &lt;path/to/recording.vrs&gt; --to &lt;path/to/recording_norgb.vrs&gt; - 214-1 ","version":"Next","tagName":"h3"},{"title":"Aria Research Kit User Support","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/troubleshooting/get_support","content":"Aria Research Kit User Support If you need further support, have feedback or feature requests we encourage you to: Post to the Project Aria Academic Partners Workplace groupEmail AriaOps@meta.com To find out more about the Aria Research Kit (ARK), go to projectaria.com.","keywords":"","version":"Next"},{"title":"Project Aria Research Kit Release Notes","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/sw_release_notes","content":"","keywords":"","version":"Next"},{"title":"Announcing Project Aria MPS CLI Availability and New MPS Feature​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#announcing-project-aria-mps-cli-availability-and-new-mps-feature","content":"February 28 2024 Dear Academic Partners, We are pleased to announce Project Aria MPS CLI and new MPS capability multi-trajectory map alignment! As part of our ongoing commitment to empowering researchers, the Aria MPS CLI allows you to submit jobs to Machine Perception Services programmatically. This will hopefully bring a new level of efficiency and flexibility to your workflow. ","version":"Next","tagName":"h2"},{"title":"⭐️Key features for MPS CLI​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#️key-features-for-mps-cli","content":"Request the following MPS services Single and multi-sequence SLAM SLAM Open/Closed loop trajectoriesSemi-Dense point clouds and observationsOnline sensor calibration Eye gaze GeneralPersonalized Recording Upload VrsHealthCheck runs automatically before uploading. Upload is skipped if there is a failure.Unfinished uploads can be resumed for up to 24 hrs Processing Improved efficiency from concurrent processing and upload of multiple recordingsAutomatic download of outputs for immediate accessEnables re-processing of data within a set time-frame without the need for re-upload Workflow integration Support Run MPS CLI as part of a script of other workflow to run MPS on recordings via automation ","version":"Next","tagName":"h3"},{"title":"How to get started with CLI​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#how-to-get-started-with-cli","content":"To experience the power of Aria MPS CLI, go to the Project Aria MPS CLI page for how to use it. Videos of this feature are available in the Project Aria Academic Partner Announcements, Feedback &amp; Support workplace group ","version":"Next","tagName":"h3"},{"title":"⭐️MPS: MULTI-SLAM/MULTI-SEQUENCE MAP ALIGNMENT​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#️mps-multi-slammulti-sequence-map-alignment","content":"This addition brings the ability to generate SLAM output in a common frame of reference and support for multi-sequence SLAM outputs in a common frame of reference. Please go to Multi-SLAM page for more information about the feature and go to the MPS CLI page for how to access it. ","version":"Next","tagName":"h3"},{"title":"⭐️Feedback and Support​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#️feedback-and-support","content":"If you encounter any problems using Aria MPS CLI, there are several ways to get user support: Post to Project Aria Academic Partner Announcements, Feedback &amp; Support workplace groupPost to Project Aria DiscordEmail AriaOps@meta.com and you can always request a meeting through this email if you need higher bandwidth discussion.Additional resources are available at Project Aria Github. ","version":"Next","tagName":"h3"},{"title":"Aria Mobile App v150 is now available​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#aria-mobile-app-v150-is-now-available","content":"February 6 2024 Dear Academic Partners, The Aria Mobile App v150 is now available. Go to the Downloads page to get the latest version for Android. For iOS, select Update in the TestFlight app. Please update the app. ","version":"Next","tagName":"h2"},{"title":"BUG FIXES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes","content":"Various bug fixes to improve the overall usage. ","version":"Next","tagName":"h3"},{"title":"PROJECT ARIA LATEST OS​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#project-aria-latest-os","content":"The latest Project Aria OS build 4967277.730.70 was released on 01/25/2024. ","version":"Next","tagName":"h3"},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback","content":"Go to our new Support page for ways to get in touch. ","version":"Next","tagName":"h3"},{"title":"Project Aria Updates: Aria Mobile App v140 and changes to MPS​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#project-aria-updates-aria-mobile-app-v140-and-changes-to-mps","content":"December 4 2023 Dear Academic Partners, The Aria Mobile App v140 is now available. Go to the Downloads page to get the latest version for Android. For iOS, select Update in the Lighthouse app.  ","version":"Next","tagName":"h2"},{"title":"NEW & UPDATED FEATURES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new--updated-features","content":"MPS file outputs have been renamed​ Machine Perception Services (MPS) outputs have been renamed, so that they more clearly communicate what is in the outputs: Trajectory global_points.csv.gz -&gt; semidense_points.csv.gz Eye Gaze generalized_eye_gaze.csv -&gt; general_eye_gaze.csvcalibrated_eye_gaze.csv -&gt; personalized_eye_gaze.csv ","version":"Next","tagName":"h3"},{"title":"KNOWN ISSUES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#known-issues","content":"The MPS examples in Getting Started, Google Colab and the Visualization guide use the old naming conventions for MPS. ","version":"Next","tagName":"h3"},{"title":"BUG FIXES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes-1","content":"Various bug fixes improving the overall usage, including: Client SDK/CLI issue resolved. Device Settings tab now automatically refreshes after you’ve allowed Client SDK pairing in the mobile app. ","version":"Next","tagName":"h3"},{"title":"PROJECT ARIA LATEST OS​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#project-aria-latest-os-1","content":"The latest Project Aria OS build 4965072.660.70 was released on 12/01/2023. ","version":"Next","tagName":"h3"},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback-1","content":"Go to our new Support page for ways to get in touch. ","version":"Next","tagName":"h3"},{"title":"Announcing,  Project Aria Updates (ARK V1.5 & Project Aria Tools V1.2)​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#announcing-project-aria-updates-ark-v15--project-aria-tools-v12","content":"October 24, 2023 We’re delighted to announce Project Aria Research Kit V1.5, a point release encompassing new features for every aspect of Project Aria Research Kit (ARK), from a Client SDK and CLI, enabling you to directly interact with the glasses; to new Calibrated Eye Gaze, enabling you to achieve more accurate eye gaze estimations. Plus, you can now download the Mobile Companion app on iOS! In this release: Project Aria Client SDK V1.0.2Aria Mobile Companion app V135Project Aria OS build 4965072.660.70Project Aria Tools V1.2 ","version":"Next","tagName":"h2"},{"title":"To access the new ARK features​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#to-access-the-new-ark-features","content":"Update the Mobile Companion app and Aria glasses OS to the latest version: Follow the instructions in ARK SW Downloads and Updates to download and update the Aria Mobile Companion App on Android or iOSConnect your Aria Glasses to Wi-Fi and power and they will automatically update You can manually trigger an update from the Mobile Companion app’s Device Settings The Aria Research Kit parts of this release, support research partners with access to Project Aria glasses. ","version":"Next","tagName":"h3"},{"title":"To access new Open Science features​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#to-access-new-open-science-features","content":"Update Project Aria Tools: Python users: Update projectaria_tools python packageC++ users: Update projectaria_tools git repository Open Science releases can be helpful for users with or without access to Aria glasses. ","version":"Next","tagName":"h3"},{"title":"Aria Research Kit Release Notes​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#aria-research-kit-release-notes","content":"Project Aria Client SDK with CLI​ projectaria_client_sdk is a new Python package/client library that enables you to directly interact with your Project Aria glasses from a client machine. This enables you to stream Aria data and run MP algorithms on that data, in real-time. The Client SDK currently offers the ability to: Connect and disconnect from the device via USB and WiFiRetrieve detailed device information and its current statusControl Aria streaming and recording capabilitiesAccess calibration data for the sensors you're recording withSubscribe and listen to Aria sensors dataVisualize streaming data A CLI, installed as part of the SDK, provides the ability to: Pair the glasses via USB or Wi-FiConnect to the glasses via USB or Wi-FiRetrieve the device status and informationControl Aria recording and streaming capabilities To support this new feature, we’ve added a Client SDK section to ARK Documentation in Project Aria Tools, including: Project Aria Client SDKSetup GuideCode Sample Examples and Walkthroughs New &amp; Improved Machine Perception Service: Calibrated Eye Gaze​ You can now generate Calibrated Eye Gaze MPS for sequences captured on Project Aria glasses that have in-session Eye Gaze Calibration. This enables you to capture more data about how Aria wearers look at objects, and improve the eye gaze estimations for any given recording. When you request Eye Gaze Machine Perception Services (MPS) and the file has an in-session Eye Gaze Calibration as part of the VRS file, you will receive two outputs: generalized_eye_gaze.csv - based on the standard eye gaze configurationcalibrated_eye_gaze.csv - calibrated eye gaze data based on the calibration data collected in the recording. Additional improvements to Eye Gaze MPS include: Summary.json files are also now available for Eye Gaze MPS outputWe’ve created a new visualization tool, MPS Replay Viewer (C++), to render static scene and dynamic elements. See below for more details: How to collect in-session eye gaze calibration dataEye Gaze Data Format documentation - updated to reflect new outputs Mobile Companion App improvements (and now available on iOS)​ The Aria Mobile Companion App allows users to interact with their Aria glasses via mobile device. With this release, users will now be able to download the app on iOS as beta testing software via Testflight. Installing or updating the Mobile Companion app on Android just became easier. You no longer need to sign in to the partner portal to ARK specific software. You can download it as a direct link from the ARK SW Downloads and Updates page or from an update prompt in the app. Additionally, you’ll now have the ability to change the default Recording Profile when you make a recording using the Mobile Companion app. See below for more details: ARK SW Downloads and Updates Aria Mobile Companion App documentation ","version":"Next","tagName":"h3"},{"title":"ARK Bug Fixes​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#ark-bug-fixes","content":"Minor bug fixes and general quality of life improvements. ","version":"Next","tagName":"h3"},{"title":"Known Issues​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#known-issues-1","content":"Client SDK/CLI​ Unlike Project Aria Tools, the Client SDK is not available on Ubuntu 20.04Device Settings tab does not automatically refresh after you’ve paired your glasses with SDK. If you’re in the Device Settings tab when you select “aria auth pair”, navigate away from Device Settings and back again to view your certificate. Further resources: ARK Troubleshooting and Known IssuesClient SDK &amp; CLI Troubleshooting &amp; Known Issues Getting Support​ If you encounter any problems using ARK resources, there are several ways approved Academic Research Partners can access user support: Report or view issues on GitHub - can be used to report ARK or Open Science issuesPost to Project Aria discord - best for discussion, feedback or user supportPost to Academic Partners Feedback and Support workplace group - discussion, feedback or user supportEmail AriaOps@meta.com - for feedback or user support PROJECT ARIA LATEST OS​ The latest Project Aria OS build 4965072.660.70 was released on October 17th, 2023. ","version":"Next","tagName":"h3"},{"title":"Open Science Updates (for everybody)​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#open-science-updates-for-everybody","content":"The Aria Research Kit, builds on top of projectaria_tools, which is available to anyone wishing to work with Aria data. New features contained within projectaria_tools V1.2 include: Google Colab Notebooks You can now explore Project Aria Tools without needing to install it on your own machine! Dataprovider Quickstart TutorialMachine Perception Services Tutorial Python binding for the Sophus Library Sophus PyBind provides access to SO3, SE3, interpolate and iterativeMean featuresThis Python binding has been submitted to Sophus and will be officially supported by the Sophus Library GitHub repo soon Python binding type hinting Python type hinting/stubs are now automatically generated if you’ve installed the tools using pipChange the type of hinting in generate_stubs.py MPS Replay Viewer (C++) This new visualization tool renders static scene and dynamic elements: 2D/3D observations raysEye gaze data Output image data from data_provider now contains information about which camera it came from cameraId has been added to ImageDataRecord Update Project Aria Tools to access these features: Python users: Update projectaria_tools python packageC++ users: Update projectaria_tools git repository Documentation updates and improvements for this release include: How to Convert VRS to MP4Updates to Machine Perception Services documentation For more details, please go to Project Aria Tools Release notes. Open Science Bug Fixes​ Bug fixes include: When camera intrinsics are updated, all sensor calibration is now updated The Sophus API has been updated, if you encounter issues, please update to v1.2 of Project Aria Tools and update your code This fix resolves: TypeError: 'unsupported operand type(s) for @: '_core_pybinds.sophus.SE3d' and '_core_pybinds.sophus.SE3d'AttributeError: '_core_pybinds.sophus.SE3d' object has no attribute 'to_matrix' The API has been corrected so that the calibration data will match the sensor and device access point: get_sensor_calibration(stream_id).camera_calibration() and provider.get_device_calibration().get_camera_calib(name) will now match Go to Project Aria Tools Issues on GitHub for more information. If you encounter any problems working with Open Science tooling or data, we encourage you to post the issue on Github. Open Science Known Issues​ The Sophus API has been updated, if you encounter issues, please update to v1.2 of Project Aria Tools Further resources Project Aria Tools Troubleshooting and Known IssuesReport or view issues on GitHub ","version":"Next","tagName":"h3"},{"title":"August Release - Project Aria Mobile App v125 is now available​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#august-release---project-aria-mobile-app-v125-is-now-available","content":"8/22/2023 Dear Academic Partners, Download the Aria Mobile App v125 to ensure you have all the latest features and capabilities. Sign into the Aria Web Portal on your Android device to download v125. ","version":"Next","tagName":"h2"},{"title":"NEW & UPDATED FEATURES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new--updated-features-1","content":"Ability to name VRS files via the Mobile Companion appThe default recording profile (if set via Mobile or Desktop app) will now also be the default profile when initiating recordings via the Mobile appAdded a feature that requires the app to be updated if the app build is flagged as severely buggy. Ability to name VRS files​ Historically, the VRS files were given random alphanumeric names. This made the recordings difficult to distinguish from one another in the Desktop app and after downloading them. With this new release, the name given to a recording when creating a custom recording defines the VRS file name. You’ll get a warning if there are two recordings with the same name on your Aria device. To access this feature: In the mobile app, go to Settings to check that you have v125 Sign into portal.projectaria.com on your phone to get the latest recommended build Go to device settings (select the gear icon next to your glasses) to check that you have 4963656.310.70 of the OS Aria glasses will automatically update if they’re connected to Wi-Fi and powerSelect Check for Updates in Device Settings to make sure you have the most recent version of the OS How to name a recording: Set the recording’s name when you create a custom recording, or edit the file name of any recording on your glasses. To edit the name or to give an existing recording a name: Go to the recording tabSelect a recordingSelect Edit on the top right corner Default recording profile applied to new recordings​ With this change, the default recording profile selected for the glasses also applies to each recording started from the app. Previously, the default recording profile only applied to recordings started by the capture button on the glasses. Please double-check that the correct profile is selected before you start each recording, as the behavior has changed. Previously, the app kept its own last-used setting; with this change, the default will flip back to the default recording profile of the glasses on connection. This default recording profile can be updated via Device Settings in the mobile app. Disable severely buggy builds​ This feature is a contingency plan, in case a severely broken app build is rolled out by mistake. In this situation, the app would show a full-screen dialog that requires it to be updated in order to proceed. ","version":"Next","tagName":"h3"},{"title":"BUG FIXES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes-2","content":"Various bug fixes implemented to improve the overall usage of the App. ","version":"Next","tagName":"h3"},{"title":"PROJECT ARIA LATEST OS​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#project-aria-latest-os-3","content":"The latest Project Aria OS build 4963656.310.70 was released on August 22, 2023. ","version":"Next","tagName":"h3"},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback-2","content":"Contact Aria User Support by posting in Project Aria Academic Partner Announcements, Feedback &amp; Support or email AriaOps@meta.com. ","version":"Next","tagName":"h3"},{"title":"July Release - new documentation site, Aria Desktop App v37 and Mobile App v120​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#july-release---new-documentation-site-aria-desktop-app-v37-and-mobile-app-v120","content":"7/11/23 Dear Academic Partners, With this July release we’re pleased to announce a new documentation site on GitHub, Desktop Companion App v37, and Aria Mobile Companion App v120. V37 of the Desktop Companion App updates the app to the new Project Aria branding, the ability to run the Desktop app from the command line and users will now be able to request Semi-Dense Point Cloud MPS. Project Aria Tools is our new documentation site on GitHub. Information that used to be at projectaria.com is now in the Aria Research Kit sectionof the site and no login is required to access the documentation. Download the updated companion apps from the Aria Web Portal. If you sign in on your desktop computer, you’ll get the Desktop Companion app. If you sign in on your Android mobile device, you’ll download the Mobile Companion app. ","version":"Next","tagName":"h2"},{"title":"NEW AND UPDATED FEATURES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new-and-updated-features","content":"","version":"Next","tagName":"h3"},{"title":"Documentation now on GitHub​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#documentation-now-on-github","content":"The documentation previously hosted at projectaria.com will now be hosted in Project Aria Tools. Project Aria Tools is the new comprehensive website for all your documentation needs. It contains over 60 pages, covering technical specifications, data formats, data utilities you can use with your data, information and tooling for open data we’ve released and technical insights for deeper dives. The Aria Research Kit section contains information specific to Academic partners who have access to Aria glasses and MPS. We’ve added a few extra pages and updated the documentation for this release, updates include adding: Get the Right Size Glasses - sizing information if you’re ordering glasses (our small is surprisingly large!)How to Join the Academic Partners Workplace GroupAria Glasses User Manual - a combination of old and new information. Proper Handling and Cleaning information is now at the top. There is a lot of new documentation to explore, including: Aria Data Utilities - including new Jupyter notebook tutorialsMore detailed information about recording profiles (scroll the table sideways to see all the columns)More data format information (including coordinate conventions)New Open Datasets with tooling, Aria Digital Twin Dataset and Aria Synthetic Environments Dataset. ","version":"Next","tagName":"h3"},{"title":"Desktop App v37 - Semi-dense point cloud​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#desktop-app-v37---semi-dense-point-cloud","content":"From V37 of the Desktop app onwards, users will be able to request semi-dense point clouds as an addition to trajectory MPS services. The same sensor profile requirements for generating trajectory data apply to semi-dense point cloud. Semi-dense point clouds are used by researchers who need static scene 3D reconstructions, reliable 2D images tracks or a representative visualization of the environment. About Aria Machine Perception Services (MPS)How to Request MPSMPS Output - Semi-Dense Point Cloud ","version":"Next","tagName":"h3"},{"title":"Improved UI & UX​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#improved-ui--ux","content":"The Desktop app has been updated to match Aria’s new brandThe recordings view has been revamped, users will now see a thumbnail for each recording and the upload ID can be copiedThe Desktop app can now be run directly from the command line without needing a GUI. It currently includes two utilities as subcommands: Further documentation about how to run the Desktop app from the command line will be added to the ARK wiki soonhealth: use this to run validity checks on an Aria recording (VRS file) It can be run from the command line as follows: AriaHub health vrsFilePath.vrsThese checks are also run on the VRS file automatically, before the file gets uploaded for MPS processing. The results of those checks can be found under your home directory under ./aria/logs vrs: a Swiss army knife utility to manipulate VRS files in different ways It can be run from the command line as follows: AriaHub vrs vrsFilePath.vrsGo to VRS official documentation for a full list of commands ","version":"Next","tagName":"h3"},{"title":"Mobile App v120 - Quality Screens Feature update​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#mobile-app-v120---quality-screens-feature-update","content":"Some additional features have been added to the quality screen feature that was introduced with v115. You’ll now be able to see timecodes and the overall score percentage for the sensors. ","version":"Next","tagName":"h3"},{"title":"BUG FIXES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes-3","content":"Various bug fixes improving the overall usage of the Apps. ","version":"Next","tagName":"h3"},{"title":"PROJECT ARIA LATEST DEVICE OS​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#project-aria-latest-device-os","content":"The latest Project Aria device OS build 4962591.230.70 was released on June 30, 2023. ","version":"Next","tagName":"h3"},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback-3","content":"Contact Aria User Support by posting in Project Aria Academic Partner Announcements, Feedback &amp; Support or email AriaOps@meta.com.   ","version":"Next","tagName":"h3"},{"title":"Aria Mobile App v115 is now available​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#aria-mobile-app-v115-is-now-available","content":"Dear Academic Partners, The Aria Mobile App v115 for Android is now available for download from the Aria Web Portal (accessed via your Android internet browser). ","version":"Next","tagName":"h2"},{"title":"NEW & UPDATED FEATURES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new--updated-features-2","content":"Sensor data quality signalsIn-Session personalized eye gaze calibrationDifferentiate paired glasses in the pairing screen When you select Add glasses, the available Aria devices will be split into Paired Glasses and Other Glasses so that it’s easy to tell which glasses are already paired with the app. Sensor Data Quality Signals​ While making recordings, researchers will be able to see whether there are any sensor data quality issues (for example, due to thermal mitigation). To check for any issues, in the Mobile Companion app, view the &quot;Sensor Status&quot; section in the Recording Status Screen. Tap on the row to see full details. If you’ve initiated recording via the Capture button, access the Recording Status by selecting Recording in progress on the Mobile Companion app’s main dashboard. In-Session Personalized Eye Gaze Calibration​ Users will be able to record personalized eye gaze calibrations within an ongoing recording. The eye gaze calibration section of the sequence can be used to improve the eye gaze estimations in the rest of the recording. In the future, Machine Perception Services will be able to consume these recordings and output more accurate gaze information. In the Mobile Companion app, create a new recording using a profile that includes ET and RGB cameras (such as Profile 15 or 25)Once your recording has started, close the recording window Select X on the top left of the screen Go to Device Settings (select the gear next to your glasses)Select Eye Tracking CalibrationConfirm that you’d like to run the during the current recording sessionFollow the prompts to calibrate your glasses Eye Gaze Calibration tips​ Things to Avoid​ ❌ Do not wear a face covering during eye calibration. ❌ Choose an area with ample and even lighting; do not face a bright light, window or reflective surface. ❌ Do not set your phone screen brightness too high compared to your surroundings. ❌ Do not fully extend your arm(s) during eye calibration. Your elbows should be bent so that the phone is roughly 1 ft (30 cm) away from your face. Helpful Tips​ ✅ The phone should be held straight in front of your face, so that you shouldn't look up or down to see the screen. Hold the phone plumb (90 degrees) vertically to the ground. ✅ The &quot;Leveler&quot; stage appears if the position of your phone isn't within specifications for the calibration process. Adjust the phone in front of you and its distance by bending your elbow until the smaller, black circle turns into a green disk with a check mark. ✅ Once the &quot;Leveler&quot; stage is successfully completed, do your best to keep your phone in exactly the same position throughout the full eye calibration process. If your phone is moved to a position no longer suited to calibrate your device, the app will return to the &quot;Leveler&quot; stage. ✅ The eye calibration stages 1 to 10 move your nose towards the direction indicated by the arrow. If you're only following the direction with your gaze without moving your head, the calibration stage will time out and fail. However, you make sure to keep your eyes fixed on the number within the dot the whole time. ","version":"Next","tagName":"h3"},{"title":"BUG FIXES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes-4","content":"Various bug fixes to improve the overall usage of the Companion App. ","version":"Next","tagName":"h3"},{"title":"PROJECT ARIA LATEST OS​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#project-aria-latest-os-4","content":"","version":"Next","tagName":"h3"},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback-4","content":"Contact Aria User Support by posting here in Project Aria Academic Partner Announcements, Feedback &amp; Support or emailing AriaOps@meta.com. ","version":"Next","tagName":"h3"},{"title":"Aria Mobile App v110 is now available​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#aria-mobile-app-v110-is-now-available","content":"Dear Academic Partners,The Aria Mobile App v110 for Android is now available for download from the Aria Web Portal (accessed from your Android internet browser). Here are the updates this new version brings. ","version":"Next","tagName":"h2"},{"title":"NEW & UPDATED FEATURES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new--updated-features-3","content":"App icons and splash screens for Android companion apps have been updated to Aria’s new branding. When you update the app it will now look like this!From v110 onwards, users will be more easily able to tell if their Aria device’s OS is out of date and see prompts to update their devices.If the glasses are significantly (currently set as 2 months) out of date, the app will disable recording until they are updated.As before, glasses automatically update when connected to power and Wi-Fi.Users will get an in-app prompt to update the app if the app build is over 8 weeks old. ","version":"Next","tagName":"h3"},{"title":"BUG FIXES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes-5","content":"Fixed a bug that occasionally prevented partner mode glasses from being set up for over-the-air software updates properly. Please double-check that your glasses are able to update to a recent build (April 2023 or later). If your glasses are not updating, please reach out to the Aria team.After glasses are unpaired, the app prevents re-pairing with the glasses until they finish rebooting.Other minor bug fixes ","version":"Next","tagName":"h3"},{"title":"PROJECT ARIA LATEST OS​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#project-aria-latest-os-5","content":"The latest Project Aria OS build was released on May 2, 2023. ","version":"Next","tagName":"h3"},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback-5","content":"Contact Aria User Support by posting here in Project Aria Academic Partner Announcements, Feedback &amp; Support or emailing AriaOps@meta.com. ","version":"Next","tagName":"h3"},{"title":"May 3, 2023 Aria Mobile App v110 is now available​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#may-3-2023-aria-mobile-app-v110-is-now-available","content":"Dear Academic Partners, The Aria Mobile App v110 for Android is now available for download from theAria Web Portal (accessed from your Android internet browser). Here are the updates this new version brings. ","version":"Next","tagName":"h2"},{"title":"NEW & UPDATED FEATURES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new--updated-features-4","content":"App icons and splash screens for Android companion apps have been updated to Aria's new branding.From v110 onwards, users will be more easily able to tell if their Aria device's OS is out of date and see prompts to update their devices.If the glasses are significantly (currently set as 2 months) out of date, the app will disable recording until they are updated.As before, glasses automatically update when connected to power and Wi-Fi.Users will get an in-app prompt to update the app if the app build is over 8 weeks old. ","version":"Next","tagName":"h3"},{"title":"BUG FIXES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes-6","content":"Fixed a bug that occasionally prevented partner mode glasses from being set up for over-the-air software updates properly. Please double-check that your glasses are able to update to a recent build (April 2023 or later). If your glasses are not updating, please reach out to the Aria team.After glasses are unpaired, the app prevents re-pairing with the glasses until they finish rebooting.Other minor bug fixes ","version":"Next","tagName":"h3"},{"title":"PROJECT ARIA LATEST OS​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#project-aria-latest-os-6","content":"The latest Project Aria OS build 4961244.1190.70 was released on May 2, 2023 ","version":"Next","tagName":"h3"},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback-6","content":"Contact Aria User Support by posting in Project Aria Academic Partner Announcements, Feedback &amp; Support or emailing AriaOps@meta.com. ","version":"Next","tagName":"h3"},{"title":"April 3, 2023 Aria Desktop App v36 is now available​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#april-3-2023--aria-desktop-app-v36-is-now-available","content":"","version":"Next","tagName":"h2"},{"title":"IMPORTANT NOTICE​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#important-notice","content":"v36 will clear the app's cache when you start it for the first time. Please make sure you download all of your MPS artifacts (Trajectory, Eye Gaze) before installing and starting v36. Dear Academic Partners, The Aria Desktop App v36 for Mac and Windows is now available for download from the Aria Web Portal including the brand new Linux version. Here are the updates this new version brings: ","version":"Next","tagName":"h3"},{"title":"NEW FEATURES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new-features","content":"Aria for Linux is now available as a debian package for Ubuntu, more precisely the 22.04 LTS version. It is important to note the app was only tested for that specific version under Gnome 42.5 and X11 (X Server) as well as Wayland. Any other debian distribution (Ubuntu 22.04 fork such as Kubuntu, Mint etc..) or environment may or may not work. Find updated instructions in the Aria For Linux Installer section of the Desktop App page to find out how to install Aria on Ubuntu. ","version":"Next","tagName":"h3"},{"title":"IMPROVEMENTS​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#improvements","content":"Reduced app size bundle (both pre and post install)Reduced app startup time ","version":"Next","tagName":"h3"},{"title":"BUG FIXES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes-7","content":"Various bug fixes improving the overall usage of the Desktop App. ","version":"Next","tagName":"h3"},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback-7","content":"Contact Aria User Support by posting in Project Aria Academic Partner Announcements, Feedback &amp; Support or emailing AriaOps@meta.com.  ","version":"Next","tagName":"h3"},{"title":"March 24, 2023 Aria Mobile App v105 is now available​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#march-24-2023-aria-mobile-app-v105-is-now-available","content":"Dear Academic Partners, The Aria Mobile App v105 for Android is now available for download from the Aria Web Portal (accessed from your Android internet browser). Here are the updates this new version brings. ","version":"Next","tagName":"h2"},{"title":"NEW & UPDATED FEATURES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new--updated-features-5","content":"The default recording profile, engaged when starting a recording by pressing the Capture button on the Project Aria device directly, can now be set and viewed on the Mobile Aria App on Android (see screenshot below). This feature used to be only available on the Desktop Aria App.Tapping “Unpair Glasses” mentions the number of on-device recordings (not uploaded) the unpairing will delete through factory reset.The Device ID is now listed in the app, below the serial number in the device settings page. Go to the Device Info page for more information about Device IDs. ","version":"Next","tagName":"h3"},{"title":"BUG FIXES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes-8","content":"On Android only, a bug causing difficulties with switching between more than 8 paired glasses has been fixed.The recording setup screen is no longer dismissed if the recording fails to start.Minor bug fixes ","version":"Next","tagName":"h3"},{"title":"PROJECT ARIA LATEST OS​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#project-aria-latest-os-7","content":"The latest Project Aria OS build 4959822.780.70 was released on February 22, 2023. ","version":"Next","tagName":"h3"},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback-8","content":"Contact Aria User Support by posting in Project Aria Academic Partner Announcements, Feedback &amp; Support or emailing AriaOps@meta.com. ","version":"Next","tagName":"h3"},{"title":"February 22, 2023​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#february-22-2023","content":"","version":"Next","tagName":"h2"},{"title":"Aria Mobile App v100 is now available​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#aria-mobile-app-v100-is-now-available","content":"Dear Academic Partners, The Aria Mobile App v100 for Android is now available for download from the Aria Web Portal (accessed from your Android internet browser). Here are the updates this new version brings. ","version":"Next","tagName":"h3"},{"title":"NEW & UPDATED FEATURES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new--updated-features-6","content":"The on-screen status message stating a recording is starting or completing used to render the whole page temporarily unresponsive until the recording was fully started or saved. Now, an equivalent status message shows up on top of the screen (not as an overlay message in the center), allowing the user to dismiss or interact with the page at any time.Several UI adjustments were made to improve visibility and ease of interaction with various app sections and buttons. ","version":"Next","tagName":"h3"},{"title":"BUG FIXES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes-9","content":"The issue causing the recording status to occasionally stay on-screen instead of being dismissed when completing a recording by pressing the Capture button has been fixed.Minor bug fixes ","version":"Next","tagName":"h3"},{"title":"PROJECT ARIA LATEST OS​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#project-aria-latest-os-8","content":"The latest Project Aria OS build 4959822.780.70 was released on February 22, 2023. ","version":"Next","tagName":"h3"},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback-9","content":"Contact Aria User Support by posting in Project Aria Academic Partner Feedback &amp; Support or emailing AriaOps@meta.com.   ","version":"Next","tagName":"h3"},{"title":"February 9, 2023, Aria Desktop App v35 is now available​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#february-9-2023--aria-desktop-app-v35-is-now-available","content":"Dear Academic Partners, The Aria Desktop App v35 for Mac and Windows is now available for download from the Aria Web Portal. Here are the updates this new version brings. ","version":"Next","tagName":"h2"},{"title":"NEW FEATURES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new-features-1","content":"We revamped the Recordings view to simplify the distinction between accessing Project Aria's device storage and the tools tailored for processing VRS files on the local host with a dedicated VRS Tools tab. Every file operation on the local host (Mac/Windows) is now done using the native file explorer (Finder for Mac, File Explorer for Windows). Find updated instructions in Desktop App instructions, in the Device Storage section, to know how to copy locally VRS files from the Project Aria device storage.Find updated instructions in Desktop App wiki page, in the Playback section, to know how to read your locally copied VRS files.Find updated instructions in MPS wiki page for how to request MPS. ","version":"Next","tagName":"h3"},{"title":"BUG FIXES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes-10","content":"Various bug fixes improving the overall usage. ","version":"Next","tagName":"h3"},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback-10","content":"Contact Aria User Support by posting in Project Aria Academic Partner Feedback &amp; Support or emailing AriaOps@meta.com.   ","version":"Next","tagName":"h3"},{"title":"January 20, 2023, Aria Mobile Companion App v95 is now available​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#january-20-2023--aria-mobile-companion-app-v95-is-now-available","content":"Dear Academic Partners, The Aria Mobile Companion App v95 for Android is now available for download. Find it in the Aria Web Portal by visiting it directly from your Android internet browser. Here are the updates this new version brings. ","version":"Next","tagName":"h2"},{"title":"NEW FEATURES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new-features-2","content":"Backend Update for User Accounts​ Over the next few weeks, we will be changing the way we create user accounts to log into the Aria Mobile App on Android. There should be no perceived difference after this change, even with older versions of the app. If you run into an issue, please contact us immediately. Mobile App Update Prompt​ When you launch the Mobile Companion app, it should trigger a notification prompting you to download the latest version from Aria Web Portal. If dismissed, the prompt will not show up again within the same day unless you log out and back in. Otherwise, the same notification prompt will show up the following day when launching the app. ","version":"Next","tagName":"h3"},{"title":"BUG FIXES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes-11","content":"Various bug fixes ","version":"Next","tagName":"h3"},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback-11","content":"Contact Aria User Support by posting in Project Aria Academic Partner Feedback &amp; Support or emailing AriaOps@meta.com. ","version":"Next","tagName":"h3"},{"title":"December 16, 2022 Major Feature Release 🚀(Dec 2022)​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#december-16-2022-major-feature-release-dec-2022","content":"","version":"Next","tagName":"h2"},{"title":"New Feature Summary​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new-feature-summary","content":"1) New Machine Perception Services (i.e. MPS): These features are available through the updated version of Desktop App. Improved Trajectory - We will now provide additional Trajectory output, including 1 khz open loop trajectory (instead of low frequency 10 hz previously), 1 khz closed loop trajectory, online calibration at camera frame rate and more robust error messaging for scenarios where Trajectory processing fails.Local Eye Gaze - Provides unit vectors and associated uncertainties for each ET frame. The gaze vectors are expressed in central pupil frame (CPF). We also provide CPF to device frame 6DOF transformation. 2) Aria Data Tools: Open Source tools that provide C++ and Python3 tools to interact with Project Aria data.Read and visualize Project Aria sequences and sensor dataRetrieve calibration data and interact with Aria camera modelsRead and visualize machine perception output from Project Aria sequences (6DoF Trajectory, Local Eye Gaze) 3) Usability &amp; Bug Fixes:Various usability improvements across Desktop, Mobile &amp; Aria Data ToolsImproved documentation, including a “Troubleshooting &amp; Known Issues” sectionWe will now be logging high-level anonymous usage data to better understand how we can improve your experience ","version":"Next","tagName":"h3"},{"title":"ACCESSING NEW FEATURES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#accessing-new-features","content":"To gain access to new features, you will need to: Download the most recent version of the Mobile (v90) and Desktop (v34) Apps from the Portal (projectaria.com). Additionally, we will prompt Mobile App users to update their app.Access Aria Data Tools from Github Thank you, and again please feel free to provide feedback. We want to hear the good, the bad and the ugly! ","version":"Next","tagName":"h3"},{"title":"Detailed Release Notes​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#detailed-release-notes","content":"","version":"Next","tagName":"h2"},{"title":"ARIA MOBILE APP V90​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#aria-mobile-app-v90","content":"New features and bug fixes include: “Task ID” and “Client Tag” fields have been renamed “Name” and “Notes”.Access Denied screen no longer shows when external users start the app with no internet connection.Wi-Fi can be configured while glasses are not connected to power. It does not allow uploading without plugging in the device.Profile selection screen now shows sensor details for each profile. Note: If your current version is under v85 it will not update automatically. Please delete and install v90 by signing into the Portal (projectaria.com) with your Android device. ","version":"Next","tagName":"h3"},{"title":"ARIA DESKTOP APP V34​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#aria-desktop-app-v34","content":"New features and bug fixes include: New improved trajectory with open/close loop poses and online calibration.Eye Gaze vectors with uncertainty.Various usability improvements and bug fixes ","version":"Next","tagName":"h3"},{"title":"DEVICE SW (BUILD 4958601.360.70)​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#device-sw-build-495860136070","content":"Please make sure your device is charged and connected to wifi via Aria Mobile App to receive this build. New features and bug fixes include: Enabled USB streaming support on Mac OS (not available on AriaHub yet).Added telemetry logging for a subset of device events with an anonymized location.Added new recording profile21.Companion App shows details about a streaming session started from AriaHub.Updated security patch level to November 2022.Added the ability to run RGB in RAW mode at high frame rate (not exposed in a profile yet).Removed profile17 from the list of recording profiles.   ","version":"Next","tagName":"h3"},{"title":"November 11, 2022, Android Aria App v85 is now available](https://my.workplace.com/groups/1137227200340269/permalink/1215336389196016/)​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#november-11-2022-android-aria-app-v85-is-now-availablehttpsmyworkplacecomgroups1137227200340269permalink1215336389196016","content":"Dear Academic Partners,The Android Aria App v85 has now been released. Here is what you need to know.### NEW FEATURES AND IMPROVEMENTS Thumbnails showing what was recorded appear in each Recording Details page under the Recordings tab immediately after that recording is completed and saved (this feature should become available after the next Project Aria OS update around November 15th)If the Bluetooth and/or Location services need to be enabled, the app will now show a prompt to do so upon launching the app.During an ongoing recording, the Profile and Sensors used are now mentioned on the active recording page.Through the app, users can now connect a Project Aria device to an EAP-PWD Wi-Fi network (using username and password for authentication - does not support certificates).The Android app will now show a banner on top of the screen indicating that the user has not selected a default Profile for the paired Project Aria device, which can only be done via the Desktop app. ","version":"Next","tagName":"h2"},{"title":"IF YOUR CURRENT VERSION IS V80​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#if-your-current-version-is-v80","content":"As long as you're currently using v80 (check the app version in the app Settings page), launching the app should trigger a notification prompting you to download the latest version from the Partner Portal. ","version":"Next","tagName":"h3"},{"title":"IF YOUR CURRENT VERSION IS OLDER THAN V80​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#if-your-current-version-is-older-than-v80","content":"App versions older than v80 will not receive any notifications, as this is a new feature (as announced in this post). In this case, you will need to uninstall the app and install v85 from the Partner Portal. ","version":"Next","tagName":"h3"},{"title":"WHAT IF YOU DISMISSED THE PROMPT?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#what-if-you-dismissed-the-prompt","content":"If you dismissed the notification prompting you to update, it should appear again when you launch the Android Aria App the following day. If you don't want to wait that long, you can directly install the latest version from the Partner Portal. ","version":"Next","tagName":"h3"},{"title":"VERIFY V85 WAS INSTALLED​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#verify-v85-was-installed","content":"Once the Aria app is installed, login and tap the Settings tab at the bottom right corner of the Dashboard page. On that Settings page, you'll find the app version. ","version":"Next","tagName":"h3"},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback-12","content":"Contact Aria User Support by posting here in Project Aria Academic Partner Feedback &amp; Support or emailing AriaOps@meta.com.   ","version":"Next","tagName":"h3"},{"title":"October 10, 2022 ARIA DESKTOP APP V32​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#october-10-2022-aria-desktop-app-v32","content":"With v32 a single universal Mac application is now available, supporting both Intel and Apple Silicon architecture ","version":"Next","tagName":"h2"},{"title":"New Features​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new-features-3","content":"Single universal Mac application supporting both Intel and Apple Silicon architectureLocal Notifications for both Mac &amp; WindowsMerging both Uploads &amp; Local recordings in the same viewAbility to select a default profile when using the HW recording buttonResizable columns for the Local &amp; Uploads tablesOverall app speed and performance improved ","version":"Next","tagName":"h3"},{"title":"Deprecated Features​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#deprecated-features","content":"OS update &amp; Wireless connection (you may now use the latest Companion App for that) ","version":"Next","tagName":"h3"},{"title":"Bug Fixes​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes-12","content":"Frequent inability to select a recording profileInability to visualize the Aria Stream windowVarious issues when using the extracting features via the &quot;More&quot; toolbar buttonWindows app freezing randomlyOn Mac, when in fullscreen, the top toolbar covering the top of the window As always, please make sure to update your glasses to the latest version using the Companion App before starting to use Aria For Mac and Aria for Windows V32   ","version":"Next","tagName":"h3"},{"title":"ARIA Companion App APP V80​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#aria-companion-app-app-v80","content":"With v80, updating the companion app is now easier. ","version":"Next","tagName":"h2"},{"title":"New Features​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new-features-4","content":"Starting with v80, the Android app will show a notification prompting you to update it when a newer version becomes available. The on-screen prompt will take you to the location (Aria Web Portal) of the new version, allowing you to update the app faster. ","version":"Next","tagName":"h3"},{"title":"How to Update Your Aria Glasses' OS","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/troubleshooting/update_glasses_os","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"How to Update Your Aria Glasses' OS","url":"/projectaria_tools/docs/ARK/troubleshooting/update_glasses_os#overview","content":"This page is for Project Aria glasses users that wish to manually update their device's Operating System. Normally, your glasses' OS will automatically update when it is plugged into power and connected to Wi-Fi. ","version":"Next","tagName":"h2"},{"title":"To Identify Your OS Build​","type":1,"pageTitle":"How to Update Your Aria Glasses' OS","url":"/projectaria_tools/docs/ARK/troubleshooting/update_glasses_os#to-identify-your-os-build","content":" In the Mobile Companion App, select Device SettingsScroll down to view your Aria Glasses OS Version ","version":"Next","tagName":"h3"},{"title":"To Update the OS​","type":1,"pageTitle":"How to Update Your Aria Glasses' OS","url":"/projectaria_tools/docs/ARK/troubleshooting/update_glasses_os#to-update-the-os","content":"Plug in your glasses into power and ensure they are connected to Wi-Fi.In the Mobile Companion App, select Device SettingsScroll down to view the OS VersionSelect Check for UpdatesOnce your glasses have finished updating, it will reboot your glasses, and the update will be complete ","version":"Next","tagName":"h3"},{"title":"Fix USB Driver Issues in Linux","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/troubleshooting/linux_usb_driver","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Fix USB Driver Issues in Linux","url":"/projectaria_tools/docs/ARK/troubleshooting/linux_usb_driver#overview","content":"If the Aria Desktop app or computer can't detect a Project Aria device, it may be that your Aria device's battery is drained, or in Linux it may be because of your USB driver. Use the following instructions to resolve USB driver issues in Linux. ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Fix USB Driver Issues in Linux","url":"/projectaria_tools/docs/ARK/troubleshooting/linux_usb_driver#prerequisites","content":"Android Device Bridge (ADB) To install ADB use sudo apt-get android-tools ","version":"Next","tagName":"h2"},{"title":"Instructions​","type":1,"pageTitle":"Fix USB Driver Issues in Linux","url":"/projectaria_tools/docs/ARK/troubleshooting/linux_usb_driver#instructions","content":"","version":"Next","tagName":"h2"},{"title":"Look for Aria device​","type":1,"pageTitle":"Fix USB Driver Issues in Linux","url":"/projectaria_tools/docs/ARK/troubleshooting/linux_usb_driver#look-for-aria-device","content":"With your Aria device plugged into your computer, use the command adb devices. If your device can be found, you'll get an output like: List of devices attached 1820dc10 device  If you see no permissions: List of devices attached 1820dc10 no permissions  you likely need to change your udev. ","version":"Next","tagName":"h3"},{"title":"Change udev​","type":1,"pageTitle":"Fix USB Driver Issues in Linux","url":"/projectaria_tools/docs/ARK/troubleshooting/linux_usb_driver#change-udev","content":"The following instructions were taken from Arch Linux's Android Debug Bridge instructions and Janos Gyerik's Adding udev rules: Step 1: Get VENDOR_ID and PRODUCT_ID​ Use list devices to find the [VENDOR_ID] and [PRODUCT_ID] of your Aria device. The command lsusb  should show something like: Bus 002 Device 002: ID 2833:0086 Facebook, Inc. Aria  In the example above, [VENDOR_ID] = 2833 and [PRODUCT_ID]=0086 Step 2: Modify 51-android.rules​ Using lsusb will create a new file /etc/udev/rules.d/51-android.rules Modify 51-android.rules using the following commands or script. Make sure you create a group called adbusers and $USER, so that you have the correct permissions. Commands $ cat /etc/udev/rules.d/51-android.rules SUBSYSTEM==&quot;usb&quot;, ATTR{idVendor}==&quot;2833&quot;, MODE=&quot;0660&quot;, GROUP=&quot;adbusers&quot;, TAG+=&quot;uaccess&quot; SUBSYSTEM==&quot;usb&quot;, ATTR{idVendor}==&quot;2833&quot;, ATTR{idProduct}==&quot;0086&quot;, MODE=&quot;0660&quot;, GROUP=&quot;adbusers&quot;, SYMLINK+=&quot;android_adb&quot; SUBSYSTEM==&quot;usb&quot;, ATTR{idVendor}==&quot;2833&quot;, ATTR{idProduct}==&quot;0086&quot;, MODE=&quot;0660&quot;, GROUP=&quot;adbusers&quot;, SYMLINK+=&quot;android_fastboot&quot;  Reboot your workstation to ensure the changes are applied. Script This script will will apply the previous commands and reboot your workstation. IDs=$(lsusb | grep Facebook) if [[ &quot;$?&quot; -ne 0 ]]; then echo &quot;Make sure you have your VROS device connected to your workstation&quot; exit fi IDs=$(echo $IDs | cut -d &quot; &quot; -f 6) VID=$(echo $IDs | cut -d &quot;:&quot; -f 1) PID=$(echo $IDs | cut -d &quot;:&quot; -f 2) conf_f=/etc/udev/rules.d/51-android.rules sudo touch ${conf_f} echo &quot;SUBSYSTEM==\\&quot;usb\\&quot;, ATTR{idVendor}==\\&quot;$VID\\&quot;, MODE=\\&quot;0660\\&quot;, GROUP=\\&quot;adbusers\\&quot;, TAG+=\\&quot;uaccess\\&quot;&quot; &gt;&gt; $conf_f echo &quot;SUBSYSTEM==\\&quot;usb\\&quot;, ATTR{idVendor}==\\&quot;$VID\\&quot;, ATTR{idProduct}==\\&quot;$PID\\&quot;, MODE=\\&quot;0660\\&quot;, GROUP=\\&quot;adbusers\\&quot;, SYMLINK+=\\&quot;android_adb\\&quot;&quot; &gt;&gt; $conf_f echo &quot;SUBSYSTEM==&quot;usb&quot;, ATTR{idVendor}==\\&quot;$VID\\&quot;, ATTR{idProduct}==\\&quot;$PID\\&quot;, MODE=\\&quot;0660\\&quot;, GROUP=\\&quot;adbusers\\&quot;, SYMLINK+=\\&quot;android_fastboot\\&quot;&quot; &gt;&gt; $conf_f sudo groupadd adbusers sudo usermod -aG adbusers $USER  ","version":"Next","tagName":"h3"},{"title":"ARK Troubleshooting & Known Issues","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"ARK Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#overview","content":"This page provides troubleshooting information for issues you may encounter while using the Aria Research Kit (ARK). It covers: Device and Recording IssuesDesktop App IssuesDesktop App Machine Perception Services Issues and Error Messages Go to the Glasses Manual for information about LED States, button configuration, how to factory reset and power cycle your device. If you need further support, have feedback or feature requests, go to our Support page. ","version":"Next","tagName":"h2"},{"title":"Device and Recording Issues​","type":1,"pageTitle":"ARK Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#device-and-recording-issues","content":"","version":"Next","tagName":"h2"},{"title":"I haven't received my Project Aria glasses​","type":1,"pageTitle":"ARK Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#i-havent-received-my-project-aria-glasses","content":"Once your Aria glasses request has been approved, please contact AriaOps@meta.com if you project hasn't received the requested devices in: 7 days when shipping to continental USA14 days when shipping internationally ","version":"Next","tagName":"h3"},{"title":"I can't pair/detect my glasses with the Mobile Companion App!​","type":1,"pageTitle":"ARK Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#i-cant-pairdetect-my-glasses-with-the-mobile-companion-app","content":"Make sure your Aria glasses are plugged into power and that the privacy switch is not turned on (the switch should be pushed forwards (towards the lenses of your device) to be able to pair. ","version":"Next","tagName":"h3"},{"title":"I can't start recording​","type":1,"pageTitle":"ARK Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#i-cant-start-recording","content":"Check that the privacy switch is not engaged. The switch should be pushed forward (towards the lenses of your device) for recording to be possible.  ","version":"Next","tagName":"h3"},{"title":"Where's my recording?​","type":1,"pageTitle":"ARK Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#wheres-my-recording","content":"It is possible you may have accidentally discarded your recording. Do not use the privacy switch to stop recording, as it prevents recording and deletes any current recording. ","version":"Next","tagName":"h3"},{"title":"Stop recording does not work/ has a long delay​","type":1,"pageTitle":"ARK Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#stop-recording-does-not-work-has-a-long-delay","content":"The longer a device records for, the longer it takes for a recording to stop. This is because the larger the VRS file, the longer it takes a recording to finish indexing. This can be particularly apparent in the Desktop App, as you can press the Stop button and it looks like nothing has happened. note Recording has not fully stopped until the Recording LEDs have turned off ","version":"Next","tagName":"h3"},{"title":"Desktop App Issues​","type":1,"pageTitle":"ARK Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#desktop-app-issues","content":"","version":"Next","tagName":"h2"},{"title":"Aria device flickers on and off in the Desktop app​","type":1,"pageTitle":"ARK Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#aria-device-flickers-on-and-off-in-the-desktop-app","content":"You most likely need to update your Aria device's OS. Pair your device with the mobile companion app. Your device should update automatically once paired. If the issue continues to occur: In the Aria Mobile Companion App, select the settings icon next to your Paired GlassesScroll down to Device ModeIf you have the correct device, the device mode should say “Partner” If your device mode says anything else, please contact Aria User Support to get a replacement device If the device mode correctly shows “Partner”, scroll up to Glasses OS and select Check for Updates ","version":"Next","tagName":"h3"},{"title":"Copying files locally takes a long time!​","type":1,"pageTitle":"ARK Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#copying-files-locally-takes-a-long-time","content":"We recommend copying data from your Aria device using ADB or MTP (Windows automatic files transfer) rather than the Desktop App, for a faster experience. On a Windows machine, your Aria device will be automatically detected as a USB drive when plugged in and the Windows File Explorer will automatically open a new window showing Aria and its internal device storage. Go to the Quickstart Guide for ADB commands and MTP instructions. ","version":"Next","tagName":"h3"},{"title":"Desktop app/computer can't detect my Aria device​","type":1,"pageTitle":"ARK Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#desktop-appcomputer-cant-detect-my-aria-device","content":"It may be that your Aria device's battery is drained, make sure it is correctly charging (there should be a blue LED on the right arm) and wait ten minutes. On Linux, this also may be due to USB driver issues. Fix USB Driver Issues in Linux ","version":"Next","tagName":"h3"},{"title":"Machine Perception Services (MPS)​","type":1,"pageTitle":"ARK Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#machine-perception-services-mps","content":"","version":"Next","tagName":"h2"},{"title":"How do I find out what recording profile was used?​","type":1,"pageTitle":"ARK Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#how-do-i-find-out-what-recording-profile-was-used","content":"There are several ways to check what recording profile was used. Method 1: Check the vrs.json file​ Every VRS recording comes with a .vrs.json file that contains metadata about that recording. Open the file with any text editor and check the recording_profile value. Method 2: Use the Desktop app to get metadata from the VRS file​ Go to Recordings in the Desktop AppSelect ToolsSelect More &gt; VRS Data Layout &gt; JSON MetadataSelect a VRS recording from your directorySearch the opened .vrs-description.txt file for “profile” ","version":"Next","tagName":"h3"},{"title":"Can’t upload data to MPS/Upload keeps failing​","type":1,"pageTitle":"ARK Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#cant-upload-data-to-mpsupload-keeps-failing","content":"If you cannot upload your data to MPS it could be that your VRS file is timing out during upload. Finding a faster internet connection may help or you can reduce your VRS file’s size. You may also wish to try the MPS CLI How to reduce VRS file size ","version":"Next","tagName":"h3"},{"title":"Desktop App MPS Error Messages​","type":1,"pageTitle":"ARK Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#desktop-app-mps-error-messages","content":"","version":"Next","tagName":"h2"},{"title":"Unsupported Format​","type":1,"pageTitle":"ARK Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#unsupported-format","content":"You may encounter this error message when trying to unzip a downloaded MPS output file, especially large trajectory files. “Unsupported Format” occurs when the zip file has not fully downloaded. Please wait and try again later. ","version":"Next","tagName":"h3"},{"title":"Unsupported MPS Profile​","type":1,"pageTitle":"ARK Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#unsupported-mps-profile","content":"This recording profile does not support trajectory generation. This recording profile does not support eye gaze generation. Trajectory and Eye Gaze derived data can only be generated if they have the necessary sensor data. Go to the Recording Profiles page for information about supported profiles. ","version":"Next","tagName":"h3"},{"title":"Recording duration not supported​","type":1,"pageTitle":"ARK Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#recording-duration-not-supported","content":"This recording duration is not supported by trajectory generation. If you experience this error message, it is because the recording is either too long or too short. Recordings need to be longer than 5 seconds and shorter than one hour. ","version":"Next","tagName":"h3"},{"title":"Health checks failed (trajectory)​","type":1,"pageTitle":"ARK Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#health-checks-failed-trajectory","content":"The health checks for this recording failed. When trajectory data is derived, it goes through a series of health checks that are recorded in the summary.json file that's included with in every trajectory output. If your trajectory data fails, you can download a zip file that contains the summary.json file with further information. Go to Recordings &gt; Uploads to download the summary.json file. If you need to reach out to Support, please include the Transaction ID (found in the Tools tab, MPS Uploads) as well as the summary.json file. ","version":"Next","tagName":"h3"},{"title":"How to Join the Academic Partners Workplace Group","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/workplacegroup","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"How to Join the Academic Partners Workplace Group","url":"/projectaria_tools/docs/ARK/workplacegroup#overview","content":"As part of the Aria Research Kit (ARK), you'll get access to theProject Aria Academic Partner Announcements Feedback and Support Workplace Group. This group is Aria's research community space where researchers and engineers at Meta and all our Academic partners can connect, ask questions, share ideas and provide support. In addition to being the place where we make announcements, we want to support researchers to: Provide feedback about their Aria experiencesParticipate in general discussions about AriaAsk our community of researchers and engineers questions (support related or exploring an idea)Engage with other academic researchers and perhaps even set challenges for each other Once you've joined this group you can post support queries in the group or directly message our user support team. This group is specifically for people who are working with Aria Devices. ","version":"Next","tagName":"h2"},{"title":"How to Join​","type":1,"pageTitle":"How to Join the Academic Partners Workplace Group","url":"/projectaria_tools/docs/ARK/workplacegroup#how-to-join","content":"Please do not add extra people to this group If someone who is approved for the ARK and has received Aria glasses is missing, please let your Meta point of contact know. When you first onboard with Project Aria you will receive two emails. One will contain account credentials that you'll use with the Partner Portal and the Companion App. The second email will invite you to join the Project Aria Workplace Group. The email will be from notification@fbworkmail.com and have the subject “Join [person] in Project Aria Academic Partner Announcements, Feedback and Support”. ","version":"Next","tagName":"h2"},{"title":"To join the Workplace group:​","type":1,"pageTitle":"How to Join the Academic Partners Workplace Group","url":"/projectaria_tools/docs/ARK/workplacegroup#to-join-the-workplace-group","content":"Find the email invitation with the subject Join [Person] in Project Aria Academic Partner Announcements, Feedback &amp; Support and select “Join [person]” If you don't have this email invitation, please email AriaOps@meta.com In your browser, follow the prompts to create a new Workplace To set up a workplace group you just need to answer a few questions: your name, type of role, organization and organization size. Once you've set up your workplace, you should see a prompt saying that you've been invited into a closed multi-company group.Select Accept InviteYou should then see “Welcome [Your Name]!”Select Skip or Next If you can't see Skip or Next, scroll down the page. The next button might not be immediately visible on some mobile devices Follow the prompts until you get to the workplace group You may need to scroll further down the page if you do not see the Next button ","version":"Next","tagName":"h3"},{"title":"Accessing the Workplace Group​","type":1,"pageTitle":"How to Join the Academic Partners Workplace Group","url":"/projectaria_tools/docs/ARK/workplacegroup#accessing-the-workplace-group","content":"You should be able to continue from your workplace group creation. In addition, once you've created your workplace group you should get an email fromnotification@fbworkmail.com with the subject “[Your Workplace Group name] via Workplace”. Select Log into Workplace and enter your credentialsYou should see your groups displayed in the Home section On Mobile, select the menu icon on the top right of your screen to view Home ","version":"Next","tagName":"h3"},{"title":"Attribution and Contributing","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/attribution_citation","content":"","keywords":"","version":"Next"},{"title":"Citation​","type":1,"pageTitle":"Attribution and Contributing","url":"/projectaria_tools/docs/attribution_citation#citation","content":"","version":"Next","tagName":"h2"},{"title":"Project Aria Tools​","type":1,"pageTitle":"Attribution and Contributing","url":"/projectaria_tools/docs/attribution_citation#project-aria-tools","content":"If you use Project Aria tools or data in your research, please consider starring ⭐ our github repository, and citing the Project Aria Whitepaper. @misc{engel2023project, title={Project Aria: A New Tool for Egocentric Multi-Modal AI Research}, author={Jakob Engel and Kiran Somasundaram and Michael Goesele and Albert Sun and Alexander Gamino and Andrew Turner and Arjang Talattof and Arnie Yuan and Bilal Souti and Brighid Meredith and Cheng Peng and Chris Sweeney and Cole Wilson and Dan Barnes and Daniel DeTone and David Caruso and Derek Valleroy and Dinesh Ginjupalli and Duncan Frost and Edward Miller and Elias Mueggler and Evgeniy Oleinik and Fan Zhang and Guruprasad Somasundaram and Gustavo Solaira and Harry Lanaras and Henry Howard-Jenkins and Huixuan Tang and Hyo Jin Kim and Jaime Rivera and Ji Luo and Jing Dong and Julian Straub and Kevin Bailey and Kevin Eckenhoff and Lingni Ma and Luis Pesqueira and Mark Schwesinger and Maurizio Monge and Nan Yang and Nick Charron and Nikhil Raina and Omkar Parkhi and Peter Borschowa and Pierre Moulon and Prince Gupta and Raul Mur-Artal and Robbie Pennington and Sachin Kulkarni and Sagar Miglani and Santosh Gondi and Saransh Solanki and Sean Diener and Shangyi Cheng and Simon Green and Steve Saarinen and Suvam Patra and Tassos Mourikis and Thomas Whelan and Tripti Singh and Vasileios Balntas and Vijay Baiyya and Wilson Dreewes and Xiaqing Pan and Yang Lou and Yipu Zhao and Yusuf Mansour and Yuyang Zou and Zhaoyang Lv and Zijian Wang and Mingfei Yan and Carl Ren and Renzo De Nardi and Richard Newcombe}, year={2023}, eprint={2308.13561}, archivePrefix={arXiv}, primaryClass={cs.HC} }  ","version":"Next","tagName":"h3"},{"title":"Aria Digital Twin Dataset​","type":1,"pageTitle":"Attribution and Contributing","url":"/projectaria_tools/docs/attribution_citation#aria-digital-twin-dataset","content":"If you use Aria Digital Twin dataset and its tools, please cite the Aria Digital Twin dataset paper: @inproceedings{pan2023aria, title={Aria Digital Twin: A New Benchmark Dataset for Egocentric 3D Machine Perception}, author={Pan, Xiaqing and Charron, Nicholas and Yang, Yongqian and Peters, Scott and Whelan, Thomas and Kong, Chen and Parkhi, Omkar and Newcombe, Richard and Ren, Yuheng Carl}, booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages={20133--20143}, year={2023} }  ","version":"Next","tagName":"h3"},{"title":"Aria Synthetic Environments Dataset​","type":1,"pageTitle":"Attribution and Contributing","url":"/projectaria_tools/docs/attribution_citation#aria-synthetic-environments-dataset","content":"If you use Aria Synthetic Environments Dataset and its tools, please cite the Aria Synthetic Environments dataset paper (coming soon). ","version":"Next","tagName":"h3"},{"title":"Aria Pilot Dataset​","type":1,"pageTitle":"Attribution and Contributing","url":"/projectaria_tools/docs/attribution_citation#aria-pilot-dataset","content":"If you use the Aria Pilot Dataset in GitHub, please cite @misc{aria_pilot_dataset, title = {Aria Pilot Dataset}, author = {Zhaoyang Lv and Edward Miller and Jeff Meissner and Luis Pesqueira and Chris Sweeney and Jing Dong and Lingni Ma and Pratik Patel and Pierre Moulon and Kiran Somasundaram and Omkar Parkhi and Yuyang Zou and Nikhil Raina and Steve Saarinen and Yusuf M Mansour and Po-Kang Huang and Zijian Wang and Anton Troynikov and Raul Mur Artal and Daniel DeTone and Daniel Barnes and Elizabeth Argall and Andrey Lobanovskiy and David Jaeyun Kim and Philippe Bouttefroy and Julian Straub and Jakob Julian Engel and Prince Gupta and Mingfei Yan and Renzo De Nardi and Richard Newcombe}, howpublished = {\\url{https://about.facebook.com/realitylabs/projectaria/datasets}}, year = {2022} }  ","version":"Next","tagName":"h3"},{"title":"Contributing​","type":1,"pageTitle":"Attribution and Contributing","url":"/projectaria_tools/docs/attribution_citation#contributing","content":"We welcome contributions! See CONTRIBUTING for details about how to get started, and our code of conduct. ","version":"Next","tagName":"h2"},{"title":"Project Aria VRS","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_formats/aria_vrs","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Project Aria VRS","url":"/projectaria_tools/docs/data_formats/aria_vrs#overview","content":"This page provides an overview of how Project Aria uses VRS. The other pages in this section are: Project Aria VRS FormatVRS Files Timestamps ","version":"Next","tagName":"h2"},{"title":"How Project Aria uses VRS​","type":1,"pageTitle":"Project Aria VRS","url":"/projectaria_tools/docs/data_formats/aria_vrs#how-project-aria-uses-vrs","content":"Project Aria chose VRS as its data container because it is a file format designed to record and playback streams of XR (extended reality) sensor data and supports huge file sizes. These VRS files contain streams of time-sorted records generated for each sensor, with one set of sensors per stream. Project Aria data uses VRS for features such as: Records are structured as a succession of typed content blocks.Project Aria Tools recordings are structured following this VRS DataLayout. These definitions provide an overview of what information can be extracted for each stream from a Project Aria sequence.Streams contain Configuration, State and Data records, each with a timestamp in a common time domain for the whole file.Playback is optimized for timestamp order, which is key for network streaming.Random-access playback is supported via VRS. ","version":"Next","tagName":"h2"},{"title":"Further resources​","type":1,"pageTitle":"Project Aria VRS","url":"/projectaria_tools/docs/data_formats/aria_vrs#further-resources","content":"VRS ReadmeVRS Core Functionality ","version":"Next","tagName":"h3"},{"title":"Project Aria Data Formats","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_formats","content":"Project Aria Data Formats In this section, we describe: How Project Aria uses VRS to store raw dataHow Machine Perception Services (MPS) data is formatted MPS produces derived data that is useful for machine perception algorithms 2D and 3D Coordinate System Conventions","keywords":"","version":"Next"},{"title":"Timestamps in Aria VRS Files","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_formats/aria_vrs/timestamps_in_aria_vrs","content":"","keywords":"","version":"Next"},{"title":"VRS Timestamps (Single Device)​","type":1,"pageTitle":"Timestamps in Aria VRS Files","url":"/projectaria_tools/docs/data_formats/aria_vrs/timestamps_in_aria_vrs#vrs-timestamps-single-device","content":"","version":"Next","tagName":"h2"},{"title":"Device timestamps​","type":1,"pageTitle":"Timestamps in Aria VRS Files","url":"/projectaria_tools/docs/data_formats/aria_vrs/timestamps_in_aria_vrs#device-timestamps","content":"We strongly recommend always working with device timestamp when working with single-device Aria data. TimeDomain.DEVICE_TIME Each piece of data captured by Project Aria glasses is associated with a device timestamp. Also called capture timestamp in the VRS file format All sensors on the same pair of Aria glasses share the same device time domain issued from a single clock. ","version":"Next","tagName":"h3"},{"title":"Record and Host(Arrival) timestamps​","type":1,"pageTitle":"Timestamps in Aria VRS Files","url":"/projectaria_tools/docs/data_formats/aria_vrs/timestamps_in_aria_vrs#record-and-hostarrival-timestamps","content":"When working with Aria data you might encounter timestamps for different time events: TimeDomain.RECORD_TIME Record timestampsTimestamps stored in the index of VRS files.For Project Aria glasses, these are equal to the device timestamp converted to a double-precision floating point representation. TimeDomain.HOST_TIME Host or arrival timestampsTimestamps when the sensor data is saved to the device Note: this timestamp does not represent the timestamp when the sensor data is captured. Please use TimeDomain.DEVICE_TIME to access the capture timestamp Should not be needed for any purpose ","version":"Next","tagName":"h3"},{"title":"VRS Timestamps (Multiple Devices)​","type":1,"pageTitle":"Timestamps in Aria VRS Files","url":"/projectaria_tools/docs/data_formats/aria_vrs/timestamps_in_aria_vrs#vrs-timestamps-multiple-devices","content":"Accurate time synchronization is essential when co-ordinating data collection or analyzing data between multiple devices (real world or synthetic). Without synchronization, any device’s built-in recording of time will naturally drift and go out of sync (like when your microwave slowly loses time over a year). Multiple devices (either multiple Aria glasses or Aria glasses plus other devices) that are temporally aligned using a shared clock will include the TimeDomain.TIME_CODE datastream in the VRS file. ","version":"Next","tagName":"h2"},{"title":"TimeDomain.TIME_CODE​","type":1,"pageTitle":"Timestamps in Aria VRS Files","url":"/projectaria_tools/docs/data_formats/aria_vrs/timestamps_in_aria_vrs#timedomaintime_code","content":"We use time sync servers to record pairs of timestamps between the server’s local timestamp and the Aria glasses’ device timestamp. This generates a mapping between the Aria’s device time and the server’s local time. The server’s local time serves as a unified time domain shared by the multiple devices. Timecode time refers to the same “capture” event as device time, but differs by the clock assigning the timestamps. Thus we can convert between timecode time and device time by looking up values in the time mapping table. ","version":"Next","tagName":"h3"},{"title":"Project Aria Glasses 2D Image Coordinate System Conventions","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_formats/coordinate_convention/2d_image_coordinate_system_convention","content":"Project Aria Glasses 2D Image Coordinate System Conventions For any provided camera intrinsic calibration value we use the convention that the color value of a pixel with integer coordinates (u,v)(u,v)(u,v) is the average color of the square spanning from (u−0.5,v−0.5)(u-0.5,v-0.5)(u−0.5,v−0.5) to (u+0.5,v+0.5)(u+0.5,v+0.5)(u+0.5,v+0.5) in continuous coordinates. This is visualized in the Figure 1, and has the following important consequences: Checking in bound: A pixel (u,v)(u,v)(u,v) is considered to be in bound if −0.5≤u&lt;W−0.5-0.5\\leq u&lt;W-0.5−0.5≤u&lt;W−0.5 and −0.5≤v&lt;H−0.5-0.5\\leq v&lt;H-0.5−0.5≤v&lt;H−0.5.Interpolation: In bilinear interpolation, a point (u,v) can be interpolated of all four neighboring integer-valued pixel coordinates are in-bound. That requires 0≤u≤W−10 \\leq u \\leq W-10≤u≤W−1 and 0≤v≤H−10 \\leq v \\leq H-10≤v≤H−1.Image down-sampling: When downsampling images by a factor of sss, every s×ss \\times ss×s pixel are squeezed into a single pixel. For example, the intensity at pixel s×ss \\times ss×s in the scaled image accounts for all the photons collected in the area [−0.5,s−0.5]×[−0.5,s−0.5][-0.5,s-0.5]\\times[-0.5,s-0.5][−0.5,s−0.5]×[−0.5,s−0.5] (i.e. column 000 to s−1s - 1s−1, and row 000 to s−1s-1s−1 in the discrete coordinate) in the original image. In order to keep this assumption valid, the re-scaled point pscaledp_\\text{scaled}pscaled​ not only needs to scale from the corresponding point in the original image poriginalp_\\text{original}poriginal​ but also accounts for the (0.5,0.5)(0.5,0.5)(0.5,0.5) translation accordingly by pscaled=s(poriginal+0.5)−0.5p_\\text{scaled} =s (p_\\text{original}+0.5)-0.5pscaled​=s(poriginal​+0.5)−0.5 Figure 1: 2D Image Coordinate System Conventions","keywords":"","version":"Next"},{"title":"Project Aria VRS Format","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_format","content":"","keywords":"","version":"Next"},{"title":"Aria data streams​","type":1,"pageTitle":"Project Aria VRS Format","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_format#aria-data-streams","content":"In VRS, data is organized by streams. Each stream stores the data measured by a specific sensor, except for Eye Tracking (both camers share a single data stream) and microphones (up to 7 microphones share a single stream). The streams are identified by their stream ID. Each stream ID is composed of two parts, a recordable type ID to categorize the type of the sensor and a stream ID for identify the specific sensor instance. E.g. the first SLAM (aka Mono Scene) camera is identified as 1201-1 where 1201 is the numerical ID for SLAM camera data type, and 1 identifies the cameras as the first instance. Streams can also be identified by a short label. Labels are used to identify sensors in calibration. The following table lists the Stream ID and Recordable Type ID, as well as their label. GPS, Wi-Fi and Bluetooth have labels, but are not calibrated. If you use projectaria tools loaders, you do not have to memorize this mapping as there is an API that converts between Stream ID and labels. Table 1: IDs Used for Sensors  Sensor Stream ID Recordable Type ID label ET camera 211-1 EyeCameraRecordableClass camera-et RGB camera 214-1 RgbCameraRecordableClass camera-rgb Microphone 231-1 StereoAudioRecordableClass mic Barometer 247-1 BarometerRecordableClass baro GPS 281-1 GpsRecordableClass gps Wi-Fi 282-1 WifiBeaconRecordableClass\twps Bluetooth 283-1 BluetoothBeaconRecordableClass\tbluetooth SLAM/Mono Scene camera left 1201-1 SlamCameraData camera-slam-left SLAM/Mono Scene camera right\t1201-2 SlamCameraData camera-slam-right IMU (1kHz) 1202-1 SlamImuData imu-right IMU (800Hz) 1202-2 SlamImuData imu-left Magnetometer 1203-1 SlamMagnetometerData mag  Each stream also contains a configuration blob that stores sensor-specific information such as image resolution and nominal frame rate. All data in VRS is timestamped. Go to Timestamps in Aria VRS for more details. ","version":"Next","tagName":"h2"},{"title":"Aria sensor data and configuration​","type":1,"pageTitle":"Project Aria VRS Format","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_format#aria-sensor-data-and-configuration","content":"Sensor data includes: Sensor readoutTimestampsAcquisition parameters (exposure and gain settings)Conditions (e.g. temperature) during data collection Most sensor data of a single stream and at a specific timestamp is stored as a single piece, except for image and audio. ","version":"Next","tagName":"h2"},{"title":"How data is stored for image recordings​","type":1,"pageTitle":"Project Aria VRS Format","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_format#how-data-is-stored-for-image-recordings","content":"Each camera stores a single image frame at a time, with the exception of the ET camera. ET cameras pair share a single image frame by concatenating horizontally. The image frame contains two parts, the image itself and the image record. The image record stores timestamps, frame id, and acquisition parameters, such as exposure and gain. This avoids having to read image data to get the information in the record. ","version":"Next","tagName":"h3"},{"title":"How data is stored for audio recordings​","type":1,"pageTitle":"Project Aria VRS Format","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_format#how-data-is-stored-for-audio-recordings","content":"The audio data is grouped into data chunks of 4096 audio samples from all 7 microphones.Each chunk contains two parts, the data part for the audio signal, and the report part for the timestamps of each audio signal. ","version":"Next","tagName":"h3"},{"title":"Sensor configuration blob​","type":1,"pageTitle":"Project Aria VRS Format","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_format#sensor-configuration-blob","content":"The sensor configuration blob stores the static information of a stream. Common sensor configuration stores information, such as sensor model, sensor serial (if available) as well as frame rate. Stream-specific information, such as image resolution, is also stored in configurations. Go to the source code for the detailed implementation of sensor data and configurations. Go to the Advanced Code Snippets for example sensor data and how to access sensor data using Python data utilities. ","version":"Next","tagName":"h3"},{"title":"Useful VRS tools​","type":1,"pageTitle":"Project Aria VRS Format","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_format#useful-vrs-tools","content":"The most intuitive way to access Aria data is via our loaders and visualizers. We provide Python and C++ interface to easily access VRS data. You may also want to use VRS tools to extract or inspect VRS data. Here are a few common use cases: ","version":"Next","tagName":"h2"},{"title":"Check the VRS file’s validity and integrity​","type":1,"pageTitle":"Project Aria VRS Format","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_format#check-the-vrs-files-validity-and-integrity","content":"The check command decodes every record in the VRS file and prints how many records were decoded successfully. It proves that the VRS file is correct at the VRS level. You can also compute a checksum to ensure you have valid VRS files. For more information go to VRS File Validation. $ vrs check &lt;file.vrs&gt; $ vrs checksum &lt;file.vrs&gt;  If the file is not valid, it is normally because there is missing data that could lead to invalid behavior with the tooling. All files in our open datasets are valid, so if you encounter issues with these, re-downloading the files should resolve the issue. ","version":"Next","tagName":"h3"},{"title":"Extract image or audio content to folders​","type":1,"pageTitle":"Project Aria VRS Format","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_format#extract-image-or-audio-content-to-folders","content":"Use the following commands to extract JPEG or WAV files. Use the --to &lt;folder_path&gt; to specify a destination folder where the data will be extracted, or it will be added to the current working directory. $ vrs extract-images &lt;file.vrs&gt; --to &lt;image_folder&gt; $ vrs extract-audio &lt;file.vrs&gt; --to &lt;audio_folder&gt;  To extract RAW image files, use: vrs extract-images &lt;file.vrs&gt; --raw-images --to &lt;image_folder&gt;  ","version":"Next","tagName":"h3"},{"title":"Extract all content to folders and JSONs​","type":1,"pageTitle":"Project Aria VRS Format","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_format#extract-all-content-to-folders-and-jsons","content":"This command lets you extract all images, audio, and metadata into files: vrs extract-all &lt;file.vrs&gt; --to &lt;folder&gt;  The metadata is extracted into a single JSONS file that contains a succession of json messages, one per line. Each line corresponds to a single record, in timestamp order, so it is possible to parse it even if the number of records is huge. Saving all the data in a single file prevents saturating your disk with possibly millions of small files. Once extracted, your file will look like this:  ├── file.vrs ├── all_data * `NNNN-MM` folders: image folders, one folder per stream containing images. ├── 1201-1 # SLAM Left images ├── *.jpg ├── 1201-2 # SLAM Right images ├── *.jpg ├── 211-1 # Eye Tracking images ├── *.jpg ├── 214-1 # RGB (Color) Camera images ├── *.jpg ├── metadata.jsons └── ReadMe.md  For more information, go to VRS Data Extraction. ","version":"Next","tagName":"h3"},{"title":"Inspect how many data recordings there are by type​","type":1,"pageTitle":"Project Aria VRS Format","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_format#inspect-how-many-data-recordings-there-are-by-type","content":"vrs &lt;file.vrs&gt; | grep &quot;] records.&quot;  Will get you a return like this: 623 Eye Camera Class #1 - device/aria [211-1] records. 1244 RGB Camera Class #1 - device/aria [214-1] records. 729 Stereo Audio Class #1 - device/aria [231-1] records. 3101 Barometer Data Class #1 - device/aria [247-1] records. 65 Time Domain Mapping Class #1 - device/aria [285-1] records. 623 Camera Data (SLAM) #1 - device/aria [1201-1] records. 623 Camera Data (SLAM) #2 - device/aria [1201-2] records. 61965 IMU Data (SLAM) #1 - device/aria [1202-1] records. 50002 IMU Data (SLAM) #2 - device/aria [1202-2] records. 619 Magnetometer Data (SLAM) #1 - device/aria [1203-1] records.  Each line reports how many data records are stored in each data stream as well as the stream ID. For example, in this line: 623 Camera Data (SLAM) #2 - device/aria [1201-2] records.  We can see that: The stream name is Camera Data (SLAM) #2 (Mono Scene camera on the right) and identified by numerical ID [1201-2]SLAM camera #2 has recorded 623 frames ","version":"Next","tagName":"h3"},{"title":"3D Coordinate Frame Conventions for Project Aria Glasses","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_formats/coordinate_convention/3d_coordinate_frame_convention","content":"","keywords":"","version":"Next"},{"title":"SE(3) Lie groups​","type":1,"pageTitle":"3D Coordinate Frame Conventions for Project Aria Glasses","url":"/projectaria_tools/docs/data_formats/coordinate_convention/3d_coordinate_frame_convention#se3-lie-groups","content":"The 6-DoF poses are represented by SE(3) Lie group. The quaternion part of SE(3) uses Hamilton convention following the Eigen library, in which the exact formula to convert a quaternion to a rotation matrix of the SE(3) can be found in the Eigen code repository. We use the SE3d class in the Sophus Library to represent SE(3) Lie groups, and provide a minimal pybind for the class. ","version":"Next","tagName":"h2"},{"title":"A note on sensor naming and motivation​","type":1,"pageTitle":"3D Coordinate Frame Conventions for Project Aria Glasses","url":"/projectaria_tools/docs/data_formats/coordinate_convention/3d_coordinate_frame_convention#a-note-on-sensor-naming-and-motivation","content":"T_sensor1_sensor2 represents a relative SE(3) transformation from sensor2 frame to sensor1 frame. An easy mnemonic is the chaining principle is: T_sensor1_sensor2 * T_sensor2_sensor3 * p_sensor3 = p_sensor1 (where p_sensor is a 3D point measured from sensor) ","version":"Next","tagName":"h3"},{"title":"Code​","type":1,"pageTitle":"3D Coordinate Frame Conventions for Project Aria Glasses","url":"/projectaria_tools/docs/data_formats/coordinate_convention/3d_coordinate_frame_convention#code","content":"PythonC++ transform_a_b represents a SE(3) rigid transformation from b coordinate frame to a coordinate frame. p_a represents an R^3 point (or vector) in the coordinate system of a. Easy mnemonics of the chaining principle (a, b, c are coordinate frames): transform_a_c = transform_a_b @ transform_b_c; p_a = transform_a_b @ p_b If you want to get quaternion from the SE3d, please notice the order is consistent to numpy quaternion_a_b = transform_a_b.to_quat() # order is w, x, y, z  3D Coordinate frame conventions Every sensor on Aria glasses has their own local coordinate system. We represent the 6DoF pose of each sensor as the relative pose (rotation and translation) with regard to the “Device frame&quot;. The device frame is by-default the local frame of the left Mono Scene (SLAM) camera.  ","version":"Next","tagName":"h3"},{"title":"Camera coordinate system convention​","type":1,"pageTitle":"3D Coordinate Frame Conventions for Project Aria Glasses","url":"/projectaria_tools/docs/data_formats/coordinate_convention/3d_coordinate_frame_convention#camera-coordinate-system-convention","content":"A camera's local frame has its origin at the camera's optical center. Coarsely, when the camera is placed up-right, the camera coordinate frame's axes points to left, up and forward. More rigorously, we define a camera's local frame based on the optical axis and the entrance pupil of its lens. Both are uniquely defined for each camera according to the camera's lens prescription. The origin of a camera's local frame is at center of the camera's entrance pupil. The frame's Z axis is aligned with the optical axis. The camera's X axis are aligned with the projection of the image plane's X axis on the entrance pupil plane. The cross-product of the X and Z axis defines the system's Y axis.  ","version":"Next","tagName":"h2"},{"title":"Non-visual sensor coordinate system​","type":1,"pageTitle":"3D Coordinate Frame Conventions for Project Aria Glasses","url":"/projectaria_tools/docs/data_formats/coordinate_convention/3d_coordinate_frame_convention#non-visual-sensor-coordinate-system","content":"We choose the IMU coordinate systems to have their origins at the position of the accelerometer, oriented along the direction of the accelerometer sensitive axis, eventually orthogonalized to compensate for sensor orthogonalities error. We use a similar arrangement for the magnetometer.  ","version":"Next","tagName":"h2"},{"title":"The nominal Central Pupil Frame (CPF)​","type":1,"pageTitle":"3D Coordinate Frame Conventions for Project Aria Glasses","url":"/projectaria_tools/docs/data_formats/coordinate_convention/3d_coordinate_frame_convention#the-nominal-central-pupil-frame-cpf","content":"The CPF frame is placed at the midpoint between the eye boxes of the left and right eye. CPF's X-axis points left, Y-axis points up and the Z-axis points forward, from the person's perspective. Aria's ET gaze is defined as a vector in the CPF space originating at (0,0,0)(0, 0, 0)(0,0,0) of the CPF frame.  ","version":"Next","tagName":"h2"},{"title":"Overview","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_formats/mps/slam","content":"Overview This section covers MPS outputs relating to SLAM. The following outputs are generated for eligible recordings when requesting SLAM data using the MPS CLI (aka Location data in the Desktop app) 6DoF TrajectorySemi-Dense Point CloudOnline Sensor CalibrationMulti-SLAM Multiple VRS recordings required to generateThis service is only available via the MPS CLI Some datasets may also have static camera calibration data.","keywords":"","version":"Next"},{"title":"MPS output - Calibration","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_formats/mps/slam/mps_calibration","content":"","keywords":"","version":"Next"},{"title":"Online calibration​","type":1,"pageTitle":"MPS output - Calibration","url":"/projectaria_tools/docs/data_formats/mps/slam/mps_calibration#online-calibration","content":"The JSONL file contains one json online calibration record per line. Each record is a json dict object that contains timestamp metadata and the result of online calibration for the cameras and IMUs. The calibration parameters contain intrinsics and extrinsics parameters for each sensor as well as a time offsets which best temporally align their data. For how to load and read online calibrations in Python and C++, please check the code example ","version":"Next","tagName":"h2"},{"title":"Static camera calibration​","type":1,"pageTitle":"MPS output - Calibration","url":"/projectaria_tools/docs/data_formats/mps/slam/mps_calibration#static-camera-calibration","content":"Some Aria data may contain the poses and intrinsic calibration of set stationary cameras. For the utility function to load the static cameras in Python and C++, please check the code example Column\tType\tDescriptioncam_uid\tstring\tUnique identifier of camera graph_uid\tstring\tUnique identifier of the world coordinate frame {tx,ty,tz,qx,qy,qz,qw}_world_cam\tfloat\tPose of the camera coordinate frame in world frame T_world_cam, translation (tx, ty, tz) in meters and rotation quaternion (qx, qy, qz, qw) image_width\tint\tImage size in pixels image_height\tint\tImage size in pixels intrinsics_type\tstring\tCamera intrinsics calibration type. Currently support types: KANNALABRANDTK3: KB3 model intrinsics_{0-7}\tfloat\tCamera intrinsics parameters start_frame_idx\tint\tUsed to determine if start frame number of the video is stationary, and if stationary camera pose and intrinsic calibration results can be applied. start_frame_idx and end_frame_idx will both be -1 if the stationary pose and intrinsic calibration can be applied to the whole video end_frame_idx\tint\tUsed to determine if the end frame number of the video is stationary and if stationary camera pose and intrinsic calibration results can be applied. start_frame_idx and end_frame_idx will both be -1 if the stationary pose and intrinsic calibration can be applied to the whole video ","version":"Next","tagName":"h2"},{"title":"Basics","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_formats/mps/mps_summary","content":"","keywords":"","version":"Next"},{"title":"MPS File Structure​","type":1,"pageTitle":"Basics","url":"/projectaria_tools/docs/data_formats/mps/mps_summary#mps-file-structure","content":"MPS outputs use the following structure, in this example recording1.vrs was used to generate MPS.  └── Example folder ├── mps_recording1_vrs │ ├── eye_gaze │ │ ├── general_eye_gaze.csv │ │ └── summary.json │ ├── slam │ │ ├── closed_loop_trajectory.csv │ │ ├── online_calibration.jsonl │ │ ├── open_loop_trajectory.csv │ │ ├── semidense_observations.csv.gz │ │ ├── semidense_points.csv.gz │ │ └── summary.json │ ├── vrs_health_check.json │ └── vrs_health_check_slam.json └── recording1.vrs  mps_[name of VRS file]_vrs Sibling directory where all the intermediate data and MPS output is saved vrs_health_check.json Output of the health check performed on your computer before data is uploadedContains information about data drops in all the sensor streams vrs_health_check_slam.json Summary of SLAM specific checks on the VRS filesIf the health check fails it will contain details about which health checks failed slam folder Contains outputs after running SLAM (Trajectory and Semi-Dense Point Cloud data) eye_gaze folder Contains eye gaze outputs ","version":"Next","tagName":"h2"},{"title":"Common terminologies​","type":1,"pageTitle":"Basics","url":"/projectaria_tools/docs/data_formats/mps/mps_summary#common-terminologies","content":"","version":"Next","tagName":"h2"},{"title":"graph_uid​","type":1,"pageTitle":"Basics","url":"/projectaria_tools/docs/data_formats/mps/mps_summary#graph_uid","content":"graph_uid is a unique identifier for the world coordinate frame. For all the 3D geometric instances like pose and points in the world frames (having _world in the suffix), when they have the same graph_uid, they are in the same coordinate frame. For simulation (such as Aria Synthetic Environments) and Aria Digital Twin(ADT) datasets we use the same random value for one space, e.g. the same graph_uid for one ADT/simulation space. ","version":"Next","tagName":"h3"},{"title":"tracking_timestamp_us​","type":1,"pageTitle":"Basics","url":"/projectaria_tools/docs/data_formats/mps/mps_summary#tracking_timestamp_us","content":"tracking_timestamp_us's values are shaped by whether it is real world or synthetic data. For real world data, tracking_timestamp_us provides the Device timestamps from your Aria glasses. Go to Timestamps in Aria VRS for a definition of the device timestamps. In simulation datasets, this will be the timestamp in the simulator. In tracking_timestamp_us This clock has arbitrary starting points, which are not synchronized between recording sessions or devices.This clock is strictly monotonic, has stable clock speed, and is accurate in duration If you want to compute the time duration between two timestamps (especially when touching dynamics, e.g. integrating acceleration to velocity over time), you should use this timestamp. ","version":"Next","tagName":"h3"},{"title":"utc_timestamp_ns​","type":1,"pageTitle":"Basics","url":"/projectaria_tools/docs/data_formats/mps/mps_summary#utc_timestamp_ns","content":"utc_timestamp_ns is the timestamp from Aria real-time clock (RTC). This time is synchronized to the cell phone time via the Aria Mobile Companion app to get UTC time at the beginning of the recording which is a rough estimate of the external standard clock. This clock is not available in the simulation datasets.This clock provides rough synchronization between sessions and devices.This clock is not guaranteed to be monotonic, or have stable clock speed, due to synchronization with NTP. So do not compute duration between two UTC timestamps. ","version":"Next","tagName":"h3"},{"title":"Operator summary​","type":1,"pageTitle":"Basics","url":"/projectaria_tools/docs/data_formats/mps/mps_summary#operator-summary","content":"The operator summary includes individual operator’s status (except for eye tracking that does not have a summary today), whether the operation is successful. There is three possible status flag: SUCCESS: the operator is successfully finished without known issues.WARN: the operator is finished, but internally it detects problem(s) which may affect results quality. The operator still outputs the results, but we don’t have enough confidence in the quality of the results, so consume the results with caution.ERROR: the operator is not finished, or finished with major error, or the quality of the results are too bad to be consumed. Results may or may not be generated, and should not be consumed even if there are results. Other than the status flag, extra information (or warning/error reasons if known) messages are included as part of the summary. Here’s an example summary JSON output:  &quot;SLAM&quot;: { &quot;status&quot;: &quot;SUCCESS&quot;, &quot;info&quot;: [ &quot;Recording total time: 1104.00s; Trajectory total length: 155.42m&quot;, &quot;Total Vision Translational Correction (mm): p50: 0.048; p99: 0.451&quot;, &quot;Rotational Correction (deg): p50: 0.001; p99: 0.007&quot; ], &quot;warnings&quot;: [], &quot;errors&quot;: [] }, ...  ","version":"Next","tagName":"h2"},{"title":"Multi-SLAM","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_formats/mps/slam/mps_multi_slam","content":"Multi-SLAM Multi-SLAM is a Project Aria Machine Perception Service (MPS) that can be requested on two or more recordings. It creates SLAM MPS outputs in a shared co-ordinate frame. Multi-SLAM data can be visualized in the Python version of MPS Viewer. Open datasets that contain Multi-SLAM outputs where there are recordings with 2 or more Project Aria glasses: Aria Everyday Activities (AEA) datasetAria Digital Twin (ADT) dataset The Multi-SLAM outputs are mostly the same as the standard SLAM MPS outputs. The differences are: Multi-SLAM can only be requested via MPS CLIAll the recordings that were aligned together will have the same graph_uid in the output.The output may contain multiple aligned islands and multiple consecutive graph_uids. Outputs are saved to a user defined directory. Each numbered folder contains the outputs for a specific VRS file: └── multi_slam_output # user defined directory for outputs ├── 0 │ ├── slam │ │ ├── closed_loop_trajectory.csv │ │ ├── online_calibration.jsonl │ │ ├── open_loop_trajectory.csv │ │ ├── semidense_observations.csv.gz │ │ ├── semidense_points.csv.gz │ │ └── summary.json │ ├── vrs_health_check.json │ └── vrs_health_check_slam.json ├── 1 │ ├── slam │ │ ├── closed_loop_trajectory.csv │ │ ├── online_calibration.jsonl │ │ ├── open_loop_trajectory.csv │ │ ├── semidense_observations.csv.gz │ │ ├── semidense_points.csv.gz │ │ └── summary.json │ ├── vrs_health_check.json │ └── vrs_health_check_slam.json └── vrs_to_multi_slam.json vrs_to_multi_slam.json associates the VRS file name with a numbered folder, for example: { &quot;/example/recording1.vrs&quot;: &quot;0&quot;, &quot;/example/recording2.vrs&quot;: &quot;1&quot;, &quot;/example2/recording1.vrs&quot;: &quot;2&quot;, } ","keywords":"","version":"Next"},{"title":"MPS output - Eye gaze","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_formats/mps/mps_eye_gaze","content":"","keywords":"","version":"Next"},{"title":"Eye Gaze Data Format​","type":1,"pageTitle":"MPS output - Eye gaze","url":"/projectaria_tools/docs/data_formats/mps/mps_eye_gaze#eye-gaze-data-format","content":"Project Aria's Machine Perception Services (MPS) uses Aria's Eye Tracking (ET) camera images to estimate the direction the user is looking. This eye gaze estimation is in Central Pupil Frame. Eye Gaze MPS file outputs are: summary.json - high level report on MPS eye gaze generationgeneral_eye_gaze.csv - based on the standard eye gaze configurationpersonalized_eye_gaze.csv - only if you’ve made the recording with in-session Eye Gaze Calibration Eye gaze data can be visualized using the MPS Viewer in Python or C++. ","version":"Next","tagName":"h2"},{"title":"general_eye_gaze.csv​","type":1,"pageTitle":"MPS output - Eye gaze","url":"/projectaria_tools/docs/data_formats/mps/mps_eye_gaze#general_eye_gazecsv","content":"general_eye_gaze.csv outputs are available for all recordings made with Eye Tracking cameras, and contain the following fields: Column\tType\tDescriptiontracking_timestamp_us\tint\tThis is the timestamp, in microseconds, of the eye tracking camera frame in device time domain. The MPS location output also contains pose estimations in the same time domain and these timestamps can be directly used to infer the device pose from the MPS location output yaw_rads_cpf\tfloat\tThis is the eye gaze yaw angle in radians in CPF frame. The yaw angle is the angle between the projection of the eye gaze vector (originating at CPF) on XZ plane and the Z axis in the CPF frame pitch_rads_cpf\tfloat\tThis is the eye gaze pitch angle in radians in CPF frame. The pitch angle is the angle between the projection of the eye gaze vector (originating at CPF) on YZ plane and the Z axis in the CPF frame depth_m\tfloat\tThis is the absolute depth in meters of the 3D gaze point in CPF frame. This value is currently not available as part of MPS output yaw_low_rads_cpf\tfloat\tThis value represents the lower bound of the confidence interval for the yaw estimation pitch_low_rads_cpf\tfloat\tThis value represents the lower bound of the confidence interval for the pitch estimation yaw_high_rads_cpf\tfloat\tThis value represents the upper bound of the confidence interval for the yaw estimation pitch_high_rads_cpf\tfloat\tThis value represents the upper bound of the confidence interval for the pitch estimation session_uid\tstring\tUnique identifier for a session within the VRS file ","version":"Next","tagName":"h2"},{"title":"personalized_eye_gaze.csv​","type":1,"pageTitle":"MPS output - Eye gaze","url":"/projectaria_tools/docs/data_formats/mps/mps_eye_gaze#personalized_eye_gazecsv","content":"personalized_eye_gaze.csv outputs are only generated if the recording has in-session Eye Gaze Calibration data. The schema is exactly the same as general_eye_gaze.csv. The session_uids between generalized_gaze output and calibrated gaze output will be the same. The in-session calibration is used to compute user specific calibration (gaze correction parameters). The yaw, pitch, yaw_low, yaw_high, pitch_low, pitch_high will be adjusted based on this calibration. If the instructions for in-session calibration are followed correctly, the calibrated eye gaze is expected to be more accurate in comparison to generalized eye gaze. ","version":"Next","tagName":"h2"},{"title":"General Principles​","type":1,"pageTitle":"MPS output - Eye gaze","url":"/projectaria_tools/docs/data_formats/mps/mps_eye_gaze#general-principles","content":"The following principles apply to general_eye_gaze.csv and personalized_eye_gaze.csv ","version":"Next","tagName":"h2"},{"title":"Confidence Intervals​","type":1,"pageTitle":"MPS output - Eye gaze","url":"/projectaria_tools/docs/data_formats/mps/mps_eye_gaze#confidence-intervals","content":"The confidence intervals represent the models uncertainty estimation. A smaller interval represents higher confidence and a wider interval represents lower confidence. The confidence interval angles are in radians and in CPF frame. Some common factors that impact uncertainty include: BlinkingHair occluding the eye tracking camerasRe-adjusting glasses or taking them off to clean them For utility function to load the eye gaze in Python and C++, please check the code examples ","version":"Next","tagName":"h3"},{"title":"Yaw/Pitch to 3D vector conversion​","type":1,"pageTitle":"MPS output - Eye gaze","url":"/projectaria_tools/docs/data_formats/mps/mps_eye_gaze#yawpitch-to-3d-vector-conversion","content":"A common use case is to convert the gaze angles into 3D vectors. To convert a gaze measurement (yaw/pitch) into a 3D gaze vector originating at the origin of CPF use the operation here. ","version":"Next","tagName":"h3"},{"title":"Session_uid​","type":1,"pageTitle":"MPS output - Eye gaze","url":"/projectaria_tools/docs/data_formats/mps/mps_eye_gaze#session_uid","content":"When there are multiple users in the same vrs file (users handing off glasses to a different user without stopping the recording), session_uid identifies intervals corresponding to different calibration sessions if in-app calibration is performed during the hand-offs. All the rows with the same session_uid belong to the same session within the VRS fileIf there are multiple calibration sessions, the session_uid would be unique for each session general_eye_gaze.csv There will be a single value when there is no in-session eye calibration or only one in-session calibrationThe session_uid column values will always match those in personalized_eye_gaze.csv Examples​ No calibrated eye gaze - general_eye_gaze will have one session_uid across all rowsOne in-session calibration - general_eye_gaze will have one session_uid across all rows and this value will be identical in personalized_eye_gazek &gt; 1 in-session calibrations - both generalized and calibrated eye gaze will have k unique session_uid that start when in-session calibration begins and this value will be identical in personalized_eye_gaze ","version":"Next","tagName":"h3"},{"title":"summary.json​","type":1,"pageTitle":"MPS output - Eye gaze","url":"/projectaria_tools/docs/data_formats/mps/mps_eye_gaze#summaryjson","content":"The summary.json file provides a high level overview of the output for each of the major stages. This is similar to the operator summary output from the MPS location pipeline. For each stage of the ET pipeline, there will be one section in this file. If the section is missing, that means that the stage is not applicable or was not run. ","version":"Next","tagName":"h2"},{"title":"Stage 1: GazeInference (all recordings)​","type":1,"pageTitle":"MPS output - Eye gaze","url":"/projectaria_tools/docs/data_formats/mps/mps_eye_gaze#stage-1-gazeinference-all-recordings","content":"Uncalibrated Eye Gaze derived data has been generated. If you’re able to download the data to view the .json file it will say SUCCESS. Name\tType Description status\tstring\tSUCCESS (if you are able to download the data and view this file) message\tstring\tAny further details, if available ","version":"Next","tagName":"h3"},{"title":"Stage 2: InSessionCalibration (if in-session calibration available)​","type":1,"pageTitle":"MPS output - Eye gaze","url":"/projectaria_tools/docs/data_formats/mps/mps_eye_gaze#stage-2-insessioncalibration-if-in-session-calibration-available","content":"If the recording contains one or more valid in-session calibration intervals, the ET pipeline will compute the calibration parameters. Each calibration session found in the VRS file will generate the following information: Name\tType\tDescription status\tstring\tSUCCESS / FAIL message\tstring\tAny further details, if available session_uid\tstring\tUnique ID representing the session start_time_us\tint\tWhen the first wearer starts using the Aria glasses, or when subsequent wearer begins in-session calibration (2nd eye calibration onwards) end_time_us\tint\tWhen a wearer session or recording ends params\tArray[float]\tThe calibration parameters (4 floats) note The status should be SUCCESS, unless there was an issue where the wearer began the in-session calibration, but did not generate the necessary data. In this case it would FAIL. ","version":"Next","tagName":"h3"},{"title":"Stage 3: CalibrationCorrection​","type":1,"pageTitle":"MPS output - Eye gaze","url":"/projectaria_tools/docs/data_formats/mps/mps_eye_gaze#stage-3-calibrationcorrection","content":"If Stage 2 has been successful, CalibrationCorrection will contain details about calibrated eye gaze. For each calibration session, we will output the following information: Name\tType\tDescription status\tstring\tSUCCESS / FAIL message\tstring\tAny further details, if available session_uid\tstring\tUnique id representing the session generalized_gaze_error_rads\tdict\tGeneralized gaze error in radians calibrated_gaze_error_rads\tdict\tCalibrated gaze error in radians If the previous stages completed successfully, the status for this stage should always be SUCCESS. ","version":"Next","tagName":"h3"},{"title":"Example summary.json files​","type":1,"pageTitle":"MPS output - Eye gaze","url":"/projectaria_tools/docs/data_formats/mps/mps_eye_gaze#example-summaryjson-files","content":"Scenario 1: No calibration available​ This report is quite short, as no in-session calibration data is available. Eye Gaze MPS was successfully created: { &quot;GazeInference&quot;: { &quot;status&quot;: &quot;SUCCESS&quot; } }  Scenario 2: In-session calibration available​ In this example, there were multiple calibration sessions: In session one calibration was completed successfullyIn session two, the user began the in-session calibration, but did not generate the necessary data. { &quot;GazeInference&quot;: { &quot;status&quot;: &quot;SUCCESS&quot; }, &quot;InSessionCalibration&quot;: [ { &quot;Status&quot;: &quot;SUCCESS&quot;, &quot;session_uid&quot;: &quot;01ac9bf2-334a-49c6-9dc6-fdc07ab08a2a&quot;, &quot;message&quot;: &quot;&quot;, &quot;start_time_us&quot;: 147588973, &quot;end_time_us&quot;: 208304973, &quot;num_calibu_frames&quot;: 1000, &quot;parameters&quot;:[1.02361481, 1.05426864, 0.01158671, 0.01403982] }, { &quot;Status&quot;: &quot;FAIL&quot;, &quot;message&quot;: &quot;Couldn't compute GT gaze vectors for the interval [487241235, 508304973]&quot;, &quot;session_uid&quot;: &quot;6063bf11-84ef-4ed5-a785-ac44b4328fdc&quot;, &quot;start_time_us&quot;: 487241235, &quot;end_time_us&quot;: 508304973, &quot;num_calibu_frames&quot;: 10, } ], &quot;CalibrationCorrection&quot;: [ { &quot;status&quot;: &quot;SUCCESS&quot;, &quot;message&quot;: &quot;&quot;, &quot;session_uid&quot;: &quot;01ac9bf2-334a-49c6-9dc6-fdc07ab08a2a&quot;, &quot;generalized_gaze_error_rads&quot;: { &quot;mean&quot;: 0.047444001119500284, &quot;std&quot;: 0.015775822542178554, &quot;min&quot;: 0.009264740570696107, &quot;max&quot;: 0.16895371875829926, &quot;p25&quot;: 0.036160872560797655, &quot;p50&quot;: 0.04529629090291307, &quot;p75&quot;: 0.05761677117669144, &quot;p95&quot;: 0.0675233675673802 }, &quot;calibrated_gaze_error_rads&quot;: { &quot;mean&quot;: 0.037444001119500284, &quot;std&quot;: 0.005775822542178554, &quot;min&quot;: 0.006364740570696107, &quot;max&quot;: 0.06835371875829926, &quot;p25&quot;: 0.026060872560797655, &quot;p50&quot;: 0.02519329090291307, &quot;p75&quot;: 0.03760677117669144, &quot;p95&quot;: 0.0474232675673802 } }, { &quot;status&quot;: &quot;FAILURE&quot;, &quot;message&quot;: &quot;No calibration available for this session&quot;, &quot;session_uid&quot;: &quot;6063bf11-84ef-4ed5-a785-ac44b4328fdc&quot; } ] }  ","version":"Next","tagName":"h3"},{"title":"MPS Output - Semi-Dense Point Cloud","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_formats/mps/slam/mps_pointcloud","content":"","keywords":"","version":"Next"},{"title":"What are semi-dense points?​","type":1,"pageTitle":"MPS Output - Semi-Dense Point Cloud","url":"/projectaria_tools/docs/data_formats/mps/slam/mps_pointcloud#what-are-semi-dense-points","content":"Semi-dense points are the 3D points associated with tracks from our semi-dense tracking pipeline. Semi-dense tracks are continually created in pixel locations of input frames that lie in regions of high image gradient, and are then successively tracked in the following frames. Each track is associated with a 3D point, parameterized as an inverse distance along a ray originating from the track's first initial observation, as well as its uncertainty in inverse distance and distance. These points are transformed from their original camera coordinate spaces to the same coordinate frame associated with the closed loop trajectory of the sequence. ","version":"Next","tagName":"h2"},{"title":"User needs to define how to enforce quality​","type":1,"pageTitle":"MPS Output - Semi-Dense Point Cloud","url":"/projectaria_tools/docs/data_formats/mps/slam/mps_pointcloud#user-needs-to-define-how-to-enforce-quality","content":"To support user flexibility the tool outputs the associated points of all tracks regardless of quality. This means the data will contain a number of points whose positions have high uncertainty and are geometrically less accurate. Users will either need to threshold the point cloud by setting a maximum allowed inverse distance / distance certainty or correctly weight points by their certainty when using them in downstream tasks. Nominal threshold values are a maximum inv_dist_std of 0.005 and a maximum dist_std of 0.01. ","version":"Next","tagName":"h3"},{"title":"Points in the world coordinate frame​","type":1,"pageTitle":"MPS Output - Semi-Dense Point Cloud","url":"/projectaria_tools/docs/data_formats/mps/slam/mps_pointcloud#points-in-the-world-coordinate-frame","content":"This file is the gzip compressed semi-dense points in the world coordinate system. The world coordinate frame is the same frame of the closed loop trajectory. For utility function to load the points in Python and C++, please check the code examples Column\tType\tDescriptionuid\tint\tA unique identifier of this point within this map graph_uid\tstring\tUnique identifier of the world coordinate frame. Associated with an equivalent graph_uid found in close_loop_trajectory.csv, depending on the frame this point was first observed in p{x,y,z}_world\tfloat\tPoint location in the world coordinate frame p_world inv_dist_std\tfloat\tStandard deviation of the inverse distance estimate, in meter^-1. Could be used for determining the quality of the 3D point position estimate dist_std\tfloat\tStandard deviation of the distance estimate, in meters. Could be used for determining the quality of the 3D point position estimate ","version":"Next","tagName":"h2"},{"title":"Point observations​","type":1,"pageTitle":"MPS Output - Semi-Dense Point Cloud","url":"/projectaria_tools/docs/data_formats/mps/slam/mps_pointcloud#point-observations","content":"The observation file is the gzip compressed semi-dense 2D observations, described in image pixel 2D coordinate frame. For utility function to load the observations in Python and C++, please check the code examples Column\tType\tDescriptionuid\tint\tA unique identifier integer of this point within this map frame_tracking_timestamp_us\tint\tAria device timestamp of the host frame’s center of exposure, in microsecond camera_serial\tstring\tThe serial number of the camera which observes this point u\tfloat\tThe sub-pixel-accuracy observed measurement of the point in pixels, in the observing frame’s camera v\tfloat\tThe sub-pixel-accuracy observed measurement of the point in pixels, in the observing frame’s camera ","version":"Next","tagName":"h2"},{"title":"MPS output - Trajectory","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_formats/mps/slam/mps_trajectory","content":"","keywords":"","version":"Next"},{"title":"Open loop trajectory​","type":1,"pageTitle":"MPS output - Trajectory","url":"/projectaria_tools/docs/data_formats/mps/slam/mps_trajectory#open-loop-trajectory","content":"Open loop trajectory is the high frequency (IMU rate, which is 1kHz) odometry estimation output by the visual-inertial odometry (VIO), in an arbitrary odometry coordinate frame. The estimation includes pose and dynamics (translational and angular velocities). The open loop trajectory has good “relative” and “local” accuracy: the relative transformation between two poses is accurate when the time span between two frames is short (within a few minutes). However, the open loop trajectory has increased drift error accumulated over time spent and travel distance. Consider using closed loop trajectory if you are looking for trajectory without drift error. For the utility function to load the open loop trajectory in Python and C++, please check the code examples Column\tType\tDescriptiontracking_timestamp_us\tint\tAria device timestamp in microseconds utc_timestamp_ns\tint\tWall clock UTC time in nanoseconds. If not available, the value will be -1 session_uid\tstring\tUnique identifier of the odometry coordinate frame. When the session_uid is the same, poses and velocities are defined in the same coordinate frame {tx,ty,tz,qx,qy,qz,qw}_odometry_device\tfloat\tPose of the device coordinate frame in odometry frame T_odometry_device, include translation (tx, ty, tz) in meters and rotation quaternion (qx, qy, qz, qw) device_linear_velocity_{x,y,z}_odometry\tfloat\tVelocity of device coordinate frame in odometry frame, (x, y, z) in meter/s angular_velocity_{x,y,z}_device\tfloat\tAngular velocity of device coordinate frame in device frame, (x, y, z) in rad/s gravity_{x,y,z}_odometry\tfloat\tEarth gravity vector in odometry frame, (x, y, z) in meter/s^2. This vector is pointing toward the ground, and includes gravitation and centrifugal forces from earth rotation quality_score\tfloat\tA quality score between 0.0 to 1.0. The larger the score is, the higher confidence the estimation has higher quality ","version":"Next","tagName":"h2"},{"title":"Closed loop trajectory​","type":1,"pageTitle":"MPS output - Trajectory","url":"/projectaria_tools/docs/data_formats/mps/slam/mps_trajectory#closed-loop-trajectory","content":"Closed loop trajectory is the high frequency (IMU rate, which is 1kHz) pose estimation output by our mapping process, in an arbitrary gravity aligned world coordinate frame. The estimation includes pose and dynamics (translational and angular velocities). Closed loop trajectories are fully bundle adjusted with detected loop closures, reducing the VIO drift which is present in the open loop trajectories. However, due to the loop closure correction, the “relative” and “local” trajectory accuracy within a short time span (i.e. seconds) might be worse compared to open loop trajectories. In some open datasets we also share and use this format for trajectory pose ground truth from simulation or Optitrack, and the files will be called in a different file name aria_gt_trajectory.csv. For the utility function to load the closed loop trajectory in Python and C++, please check the code examples Column\tType\tDescriptiongraph_uid\tstring\tUnique identifier of the world coordinate frame tracking_timestamp_us\tint\tAria device timestamp in microsecond utc_timestamp_ns\tint\tWall clock UTC time in nanosecond. If not available, the value will be -1 {tx,ty,tz,qx,qy,qz,qw}_world_device\tfloat\tPose of the device coordinate frame in world frame T_world_device, translation (tx, ty, tz) in meters and rotation quaternion (qx, qy, qz, qw) device_linear_velocity_{x,y,z}_device\tfloat\tVelocity of device coordinate frame in device frame, (x, y, z) in meter/s angular_velocity_{x,y,z}_device\tfloat\tAngular velocity of device coordinate frame in device frame, (x, y, z) in rad/s gravity_{x,y,z}_world\tfloat\tGravity vector (x, y, z) in the world frame, in meter/s^2. MPS output will all have fixed value` [0, 0, -9.81]’, while other source (e.g. simulation or Optitrack ground truth) may give different values quality_score\tfloat\tA quality score between 0.0 to 1.0. The larger the score is, the higher confidence the estimation has higher quality` ","version":"Next","tagName":"h2"},{"title":"Project Aria Tools, Data Utilities","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Project Aria Tools, Data Utilities","url":"/projectaria_tools/docs/data_utilities#overview","content":"Project Aria Tools provides Python and C++ APIs. This section provides a detailed walk through of installation, visualization, code tutorials and snippets. Getting started A quickstart guide showing how to install Project Aria Tools using a Python package, followed by tutorials using Jupyter notebooks. It contains: Dataprovider quickstart tutorial: a walk-through of accessing sensor data from VRS file, obtaining sensor calibrations and accessing project/unproject functionalities, undistorting an image, etc.Sophus Pybind Tutorial: access Sophus Library SO3, SE3, interpolate and iterativeMean featuresMachine Perception Services (MPS) tutorial: How to visualize MPS derived data (gaze, trajectory, and point cloud) Installation guide Various installation processes for Project Aria Tools API in Python and C++How to download the MPS Sample datasetHow to add CMake to your projectsInstall and Build Troubleshooting Visualizers Tutorials showing how to visualize raw Aria data and MPS data Python VisualizationC++ Visualization Core Code Snippets Python and C++ code snippets for Project Aria Tools core functionality Data Provider: open and load Aria raw data (VRS files)Image: Access and manage Aria imagesCalibration: Access device, 6DoF and sensor calibrationMPS: How to work with Aria derived data generated by Project Aria's Machine Perception Services Advanced Code Snippets Plotting Sensor Data (Python) Save images as PNGPlot the raw sensor data of a VRS file and store the plots in PDF files Image Utilities (Python and C++)Export VRS to MP4 (Python) ","version":"Next","tagName":"h2"},{"title":"Export a VRS file to MP4","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/vrs_to_mp4","content":"","keywords":"","version":"Next"},{"title":"Install dependencies​","type":1,"pageTitle":"Export a VRS file to MP4","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/vrs_to_mp4#install-dependencies","content":"pip install projectaria_tools moviepy  ","version":"Next","tagName":"h2"},{"title":"Usage​","type":1,"pageTitle":"Export a VRS file to MP4","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/vrs_to_mp4#usage","content":"vrs_to_mp4 --vrs &quot;projectaria_tools/data/mps_sample/sample.vrs&quot; --output_video &quot;sample.mp4&quot;  ","version":"Next","tagName":"h2"},{"title":"Advanced Image Utilities","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/image_utilities","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Advanced Image Utilities","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/image_utilities#overview","content":"This page provides advanced image utilities code snippets for Project Aria Tools, see also Image Code Snippets. ","version":"Next","tagName":"h2"},{"title":"Image debayer​","type":1,"pageTitle":"Advanced Image Utilities","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/image_utilities#image-debayer","content":"Some recording profiles outputs raw RGB images (Profile 7 in Recording Profile). We provide functionalities to debayer them and perform white-balancing to get RGB images. PythonC++ from projectaria_tools.core import data_provider, image stream_id = provider.get_stream_id_from_label(&quot;camera-rgb&quot;) image_data = provider.get_image_data_by_index(stream_id, 0) image_data_array = image_data[0].to_numpy_array() debayered_array = image.debayer(image_data_array)   See projectaria_tools/core/image/utility/Debayer.cpp for implementation ","version":"Next","tagName":"h2"},{"title":"Image undistortion​","type":1,"pageTitle":"Advanced Image Utilities","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/image_utilities#image-undistortion","content":"In this example, we remove distortions in raw sensor data so that straight 3D lines appear straight in the undistorted images. There is existing C++ implementation and python wrapper of this helper function in the data utilities. PythonC++ from projectaria_tools.core import data_provider, calibration camera_label = &quot;camera-slam-left&quot; stream_id = provider.get_stream_id_from_label(camera_label) calib = provider.get_device_calibration().get_camera_calib(camera_label) pinhole = calibration.get_linear_camera_calibration(512, 512, 150) raw_image = provider.get_image_data_by_index(stream_id, 0)[0].to_numpy_array() undistorted_image = calibration.distort_by_calibration(raw_image, pinhole, calib)   See projectaria_tools/core/calibration/utility/Distort.cpp for implementation. ","version":"Next","tagName":"h2"},{"title":"Rotated image clockwise 90 degrees​","type":1,"pageTitle":"Advanced Image Utilities","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/image_utilities#rotated-image-clockwise-90-degrees","content":"In this example, we rotated the RGB image 90 degrees and provide the new calibration object. Calibration rotation only applies to pinhole camera modelPinhole camera calibration object needs to be initialized as pinhole = calibration.get_linear_camera_calibration(512, 512, 150, camera_label, calib.get_transform_device_camera())with camera_label and the pose calib.get_transform_device_camera() so that pinhole_cw90 can have the correct transformation matrix when unprojecting a pixel to get ray_in_device_frame. PythonC++ camera_label = &quot;camera-rgb&quot; stream_id = provider.get_stream_id_from_label(camera_label) calib = provider.get_device_calibration().get_camera_calib(camera_label) pinhole = calibration.get_linear_camera_calibration(512, 512, 150, camera_label, calib.get_transform_device_camera()) raw_image = provider.get_image_data_by_index(stream_id, 0)[0].to_numpy_array() undistorted_image = calibration.distort_by_calibration(raw_image, pinhole, calib) # Rotated image by CW90 degrees rotated_image = np.rot90(undistorted_image, k=3) # Get rotated image calibration pinhole_cw90 = calibration.rotate_camera_calib_cw90deg(pinhole) # Unproject a pixel and get a ray from device coordinate frame test_pixel_in_rotated_image = [10,0] ray_in_device_frame = pinhole_cw90.get_transform_device_camera() @ pinhole_cw90.unproject_no_checks(test_pixel_in_rotated_image)  ","version":"Next","tagName":"h2"},{"title":"Calibration Code Snippets","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/calibration","content":"","keywords":"","version":"Next"},{"title":"Accessing device calibration​","type":1,"pageTitle":"Calibration Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/calibration#accessing-device-calibration","content":"Device calibration stores: The device's CAD model, which contains the 6DoF poses of sensors of the device as designed.The calibration of all sensors on a single Aria device. See the Accessing sensor calibration section for details.The device's sub-type (DVT-S or DVT-L to indicate small or large) PythonC++ from projectaria_tools.core import data_provider, calibration from projectaria_tools.core.stream_id import StreamId vrsfile = &quot;example.vrs&quot; provider = data_provider.create_vrs_data_provider(vrsfile) # returns None if vrs does not have a calibration device_calib = provider.get_device_calibration() print(device_calib.get_device_subtype())  ","version":"Next","tagName":"h3"},{"title":"Accessing 6DoF poses of sensors with Sophus Python binding​","type":1,"pageTitle":"Calibration Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/calibration#accessing-6dof-poses-of-sensors-with-sophus-python-binding","content":"All 6DoF poses (a.k.a. extrinsic parameters) are represented as relative to the device frame. The device frame is a specific sensor frame, identified by the sensor's label. Aria device frame is by default camera-slam-left. We also provide the pose of the central-pupil-frame in the device frame or as relative to a sensor frame. PythonC++ label = &quot;camera-slam-right&quot; transform_device_sensor = device_calib.get_transform_device_sensor(label) transform_device_cpf = device_calib.get_transform_device_cpf() transform_cpf_sensor = device_calib.get_transform_cpf_sensor(label)  ","version":"Next","tagName":"h3"},{"title":"Accessing sensor calibration​","type":1,"pageTitle":"Calibration Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/calibration#accessing-sensor-calibration","content":"Each sensor on the device may have a corresponding stream in the vrs and may have a corresponding calibration. However, some types of sensors may not have calibration defined for them (e.g. GPS, WPS, bluetooth), and some sensors may not record stream in a specific vrs. For sensor streams where calibration is available, they can be accessed by labels: PythonC++ # returns None if vrs does not have a calibration device_calib = provider.get_device_calibration() sensor_calib = device_calib.get_sensor_calib(label) More conveniently, you can just do stream_id = StreamId(&quot;1201-1&quot;) calib = provider.get_sensor_calibration(stream_id) If you know the calibration type, you can also do # returns None if the calibration label does not exist cam_calib = device_calib.get_camera_calib(&quot;camera-rgb&quot;); imu_calib = device_calib.get_imu_calib(&quot;imu-left&quot;);  ","version":"Next","tagName":"h3"},{"title":"Accessing ET and Microphone calibration​","type":1,"pageTitle":"Calibration Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/calibration#accessing-et-and-microphone-calibration","content":"Note Aria's ET camera stream and audio are special types: Aria's ET stream switches the stream for left and right ET together, thus its calibration is a pair of CameraCalibration.Aria's Audio stream has 7 channels, thus its calibration is an array of seven microphone calib. PythonC++ # returns None if the calibration label does not exist et_calib = device_calib.get_aria_et_camera_calib() print(et_calib[0].get_label()) mic_calib = device_calib.get_aria_microphone_calib() print(mic_calib[0].get_label())   ","version":"Next","tagName":"h3"},{"title":"Python binding for Sophus library​","type":1,"pageTitle":"Calibration Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/calibration#python-binding-for-sophus-library","content":"Sophus Python PyBind implements Python binding for Sophus that provides access to SO3, SE3, interpolate and iterativeMean features. This Python binding has been submitted to Sophus and will be officially supported by the Sophus Library GitHub repo soon. Once it is available through the Sophus Library, this section will point to Sophus documentation and code, to avoid duplication. The user interface is inspired by scipy.spatial.transform.Rotation. ","version":"Next","tagName":"h2"},{"title":"Feature list​","type":1,"pageTitle":"Calibration Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/calibration#feature-list","content":"SO3 Initialize with from_quat(), from_matrix(), exp()Convert to functions: to_quat(), to_matrix(), log()Multiplication with SO3 or 3D pointsOperator [] for setting/getting items with index or slicesInverse, copy, print, and lenFunction vectorization SE3 Initialize with from_quat_and_translation(), from_matrix(), from_matrix3x4(), exp()Convert to functions to_quat_and_translation(), to_matrix(), to_matrix3x4(), log()Multiplication with SE3 or 3D pointsGet rotation and translation component with rotation() and translation()Operator [] for setting/getting items with index or slicesFunction vectorizationInverse, copy, print, and lenInterpolate between two SE3Iterative mean of a group of SE3 ","version":"Next","tagName":"h3"},{"title":"Import Sophus Python binding​","type":1,"pageTitle":"Calibration Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/calibration#import-sophus-python-binding","content":"from projectaria_tools.core.sophus import SO3, SE3, interpolate, iterativeMean  ","version":"Next","tagName":"h3"},{"title":"Example code​","type":1,"pageTitle":"Calibration Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/calibration#example-code","content":"Example code is provided in projectaria_tools/core/examples/sophus_quickstart_tutorial.ipynb python3 -m jupyter ensures that the Jupyter comes from the virtual environment that contains the projectaria_tools module. ","version":"Next","tagName":"h3"},{"title":"Vectorization detail​","type":1,"pageTitle":"Calibration Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/calibration#vectorization-detail","content":"In Python, we chose to export our Sophus::SO3 as a vector of SO3 objects by binding the cpp object SO3Group defined below. This is because numerical code in Python tends to work with array of values so that the program can be efficient. This approach is inspired by scipy.spatial.transform.Rotation. ","version":"Next","tagName":"h3"},{"title":"Passing a single SO3/SE3 object to C++ code in Python binding​","type":1,"pageTitle":"Calibration Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/calibration#passing-a-single-so3se3-object-to-c-code-in-python-binding","content":"To allow other Python binding C++ code to take in a single SO3/SE3 object, we built a caster so that, even if we wrap SO3Group/SE3Group in Python, those can be implicitly converted to the C++ Sophus::SO3/SE3 object at the boundaries between languages. This enables us to pass the Python SO3/SE3 object to a C++ function as if they were a regular 1-element Sophus::SO3/SE3 object. This simplifies binding the rest of the C++ code. The implicit cast fails if the Python object is not a 1-element object. ","version":"Next","tagName":"h3"},{"title":"Image Code Snippets","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/image","content":"","keywords":"","version":"Next"},{"title":"Raw sensor data​","type":1,"pageTitle":"Image Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/image#raw-sensor-data","content":"Raw image data is stored in ImageData. ImageData is a type alias of an std::pair. The two components of that pair are: The image frame stored in vrs::PixelFrame class (potentially compressed) We recommend that users do not directly use PixelFrame Image data records Image acquisition information such as timestamps, exposure and gain PythonC++ from projectaria_tools.core import data_provider, image from projectaria_tools.core.stream_id import StreamId vrsfile = &quot;example.vrs&quot; provider = data_provider.create_vrs_data_provider(vrsfile) stream_id = provider.get_stream_id_from_label(&quot;camera-slam-left&quot;) image_data = provider.get_image_data_by_index(stream_id, 0) pixel_frame = image_data[0].pixel_frame  ","version":"Next","tagName":"h2"},{"title":"Manipulating images​","type":1,"pageTitle":"Image Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/image#manipulating-images","content":"PythonC++ In Python, we provide an interface for converting from ImageData into numpy arrays. image_array = image_data[0].to_numpy_array()  ","version":"Next","tagName":"h2"},{"title":"Aria Data Provider Code Snippets","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/data_provider","content":"","keywords":"","version":"Next"},{"title":"Open a VRS file​","type":1,"pageTitle":"Aria Data Provider Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/data_provider#open-a-vrs-file","content":"PythonC++ from projectaria_tools.core import data_provider from projectaria_tools.core.sensor_data import TimeDomain, TimeQueryOptions from projectaria_tools.core.stream_id import RecordableTypeId, StreamId vrsfile = &quot;example.vrs&quot; provider = data_provider.create_vrs_data_provider(vrsfile) assert provider is not None, &quot;Cannot open file&quot;  ","version":"Next","tagName":"h3"},{"title":"Mapping between labels and stream ids​","type":1,"pageTitle":"Aria Data Provider Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/data_provider#mapping-between-labels-and-stream-ids","content":"PythonC++ Stream IDs can be mapped from labels by using get_stream_id_from_label: stream_id = provider.get_stream_id_from_label(&quot;camera-slam-left&quot;) Inversely, you can retrieve a label from a stream ID by using get_stream_id_from_label: label = provider.get_label_from_stream_id(StreamId(&quot;1201-1&quot;))  ","version":"Next","tagName":"h3"},{"title":"Random access data by index​","type":1,"pageTitle":"Aria Data Provider Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/data_provider#random-access-data-by-index","content":"PythonC++ for stream_id in provider.get_all_streams(): for i in range(0, provider.get_num_data(stream_id)): sensor_data = provider.get_sensor_data_by_index(stream_id, i)  ","version":"Next","tagName":"h3"},{"title":"Random access data by timestamp​","type":1,"pageTitle":"Aria Data Provider Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/data_provider#random-access-data-by-timestamp","content":"Project Aria data has four kinds of TimeDomain entries. We strongly recommend always working with DEVICE_TIME when using single-device Aria data. The TIME_CODE TimeDomain is used when synchronizing time across multiple devices. Go to Timestamps in Aria VRS Files for more information. PythonC++ TimeDomain.RECORD_TIMETimeDomain.DEVICE_TIME - recommendedTimeDomain.HOST_TIMETimeDomain.TIME_CODE - for multiple devices You can also search using three different time query options: TimeQueryOptions.BEFORE (default): last data with t &lt;= t_queryTimeQueryOptions.AFTER : first data with t &gt;= t_queryTimeQueryOptions.CLOSEST : the data where |t - t_query| is smallest for stream_id in provider.get_all_streams(): t_first = provider.get_first_time_ns(stream_id, TimeDomain.DEVICE_TIME) t_last = provider.get_last_time_ns(stream_id, TimeDomain.DEVICE_TIME) query_timestamp = (t_first + t_last) // 2 # example query timestamp sensor_data = provider.get_sensor_data_by_time_ns(stream_id, query_timestamp, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST)  ","version":"Next","tagName":"h3"},{"title":"Deliver all sensor data in VRS​","type":1,"pageTitle":"Aria Data Provider Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/data_provider#deliver-all-sensor-data-in-vrs","content":"PythonC++ Async iterator to deliver sensor data for all streams in device time order: for data in provider.deliver_queued_sensor_data(): print(data.get_time_ns(TimeDomain.DEVICE_TIME)) Alternatively, you can use iterator-type syntax: seq = provider.deliver_queued_sensor_data() obj = next(seq) while True: print(obj.get_time_ns(TimeDomain.DEVICE_TIME)) try: obj = next(seq) except StopIteration: break Deliver with sub-stream selection, time truncation, and frame rate sub-sampling: # Starts by default options which activates all sensors deliver_option = provider.get_default_deliver_queued_options() # Only play data from two cameras, also reduce framerate to half of vrs deliver_option.deactivate_stream_all() for label in [&quot;camera-slam-left&quot;, &quot;camera-slam-right&quot;]: streamId = provider.get_stream_id_from_label(label) deliver_option.activate_stream(streamId) deliver_option.set_subsample_rate(streamId, 2) # skip first 100ns deliver_option.set_truncate_first_device_time_ns(100) for data in provider.deliver_queued_sensor_data() : print(data.get_time_ns(TimeDomain.DEVICE_TIME))  ","version":"Next","tagName":"h3"},{"title":"How to Use CMake with Project Aria Tools","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/installation/build_with_cmake","content":"","keywords":"","version":"Next"},{"title":"Example code​","type":1,"pageTitle":"How to Use CMake with Project Aria Tools","url":"/projectaria_tools/docs/data_utilities/installation/build_with_cmake#example-code","content":"Please refer to the sample project for a full example. Install targets are coming soon! So that you can install projectaria_tools and access it using find_package() in your CMakeLists.txt ","version":"Next","tagName":"h3"},{"title":"MPS Code Snippets","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/mps","content":"","keywords":"","version":"Next"},{"title":"Load MPS output​","type":1,"pageTitle":"MPS Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/mps#load-mps-output","content":"The loaders for MPS output (projectaria_tools/main/core/mps) provide a convenient way to quickly load the MPS output in a few lines of code into data structures that can then be used downstream. Please refer to the MPS data schema wiki page to learn more about the specifics of what each MPS output consists of. Here, we will focus only on the loading APIs in Python and C++. ","version":"Next","tagName":"h2"},{"title":"Open loop/Closed loop trajectory​","type":1,"pageTitle":"MPS Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/mps#open-loopclosed-loop-trajectory","content":"PythonC++ import projectaria_tools.core.mps as mps open_loop_path = &quot;/path/to/mps/output/trajectory/open_loop_trajectory.csv&quot; open_loop_traj = mps.read_open_loop_trajectory(open_loop_path) closed_loop_path = &quot;/path/to/mps/output/trajectory/closed_loop_trajectory.csv&quot; closed_loop_traj = mps.read_closed_loop_trajectory(closed_loop_path) # example: get transformation from this device to world coordinate frame for closed_loop_pose in closed_loop_traj: transform_world_device = closed_loop_pose.transform_world_device # example: query to find the closest Timestamp device pose and move it to the Aria RGB camera pose from projectaria_tools.core import data_provider from projectaria_tools.core.mps.utils import get_nearest_pose from projectaria_tools.core.stream_id import StreamId query_timestamp_ns = int(closed_loop_traj[1].tracking_timestamp.total_seconds() * 1e9) # to be updated with your VRS timestamps pose_info = get_nearest_pose(closed_loop_traj, query_timestamp_ns) if pose_info: T_world_device = pose_info.transform_world_device # Move this pose to the Project Aria RGB camera vrs_file = &quot;example.vrs&quot; vrs_data_provider = data_provider.create_vrs_data_provider(vrs_file) rgb_stream_id = StreamId(&quot;214-1&quot;) rgb_stream_label = vrs_data_provider.get_label_from_stream_id(rgb_stream_id) device_calibration = vrs_data_provider.get_device_calibration() rgb_camera_calibration = device_calibration.get_camera_calib(rgb_stream_label) T_device_rgb_camera = rgb_camera_calibration.get_transform_device_camera() T_world_rgb_camera = T_world_device @ T_device_rgb_camera print(T_world_rgb_camera)  ","version":"Next","tagName":"h3"},{"title":"Point cloud​","type":1,"pageTitle":"MPS Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/mps#point-cloud","content":"Always filter global point clouds in 3D Post-filtering the point cloud using inverse distance and distance certainty is required to get point cloud accurate in 3D space. There are points cannot be accurately estimated in 3D space due to low parallax, but those points are well tracked in 2D images, and produce valid 2D observations. We choose to output all the points, include those have poor 3D estimations, in case researchers need them. Go to the Semi-Dense Point Cloud page for more information. Loading observations could be slow When the Aria recording is long, loading point observations could be memory and time consuming (&gt; 1 minute). A typical 20 minutes long Aria recording will have roughly total 10+ millions of 3D points with total 100+ millions of 2D observations. PythonC++ import projectaria_tools.core.mps as mps from projectaria_tools.core.mps.utils import filter_points_from_confidence global_points_path = &quot;/path/to/mps/output/trajectory/semidense_points.csv.gz&quot; points = mps.read_global_point_cloud(global_points_path) # filter the point cloud by inverse depth and depth const float inverse_distance_std_threshold = 0.001; const float distance_std_threshold = 0.15; filtered_points = filter_points_from_confidence(points, inverse_distance_std_threshold, distance_std_threshold) # example: get position of this point in the world coordinate frame for point in filtered_points: position_world = point.position_world observations_path = &quot;/path/to/mps/output/trajectory/semidense_observations.csv.gz&quot; observations = mps.read_point_observations(observations_path)  ","version":"Next","tagName":"h3"},{"title":"Online calibration​","type":1,"pageTitle":"MPS Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/mps#online-calibration","content":"PythonC++ import projectaria_tools.core.mps as mps online_calib_path = &quot;/path/to/mps/output/trajectory/online_calibration.jsonl&quot; online_calibs = mps.read_online_calibration(online_calib_path) for calib in online_calibs: # example: get left IMU's online calibration for imuCalib in calib.imu_calibs: if imuCalib.get_label() == &quot;imu-left&quot;: leftImuCalib = imuCalib # example: get left SLAM camera's online calibration for camCalib in calib.camera_calibs: if camCalib.get_label() == &quot;camera-slam-left&quot;: leftCamCalib = camCalib  ","version":"Next","tagName":"h3"},{"title":"Eye gaze​","type":1,"pageTitle":"MPS Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/mps#eye-gaze","content":"PythonC++ import projectaria_tools.core.mps as mps gaze_path = &quot;/path/to/mps/output/eye_gaze/general_eye_gaze.csv&quot; gaze_cpf = mps.read_eyegaze(eye_gaze_path) # project the 3D gaze point assume depth is 1.0 meter depth_m = 1.0 gaze_point_cpf = mps.get_eyegaze_point_at_depth(gaze_cpf[1].yaw, gaze_cpf[1].pitch, depth_m) # example: query to find the closest Timestamp eye gaze data from projectaria_tools.core import data_provider from projectaria_tools.core.stream_id import StreamId from projectaria_tools.core.mps.utils import ( get_gaze_vector_reprojection, get_nearest_eye_gaze ) query_timestamp_ns = int(gaze_cpf[1].tracking_timestamp.total_seconds() * 1e9) # to be updated with your VRS timestamps eye_gaze_info = get_nearest_eye_gaze(gaze_cpf, query_timestamp_ns) if eye_gaze_info: # Re project the eye gaze point in the RGB camera vrs_file = &quot;example.vrs&quot; vrs_data_provider = data_provider.create_vrs_data_provider(vrs_file) rgb_stream_id = StreamId(&quot;214-1&quot;) rgb_stream_label = vrs_data_provider.get_label_from_stream_id(rgb_stream_id) device_calibration = vrs_data_provider.get_device_calibration() rgb_camera_calibration = device_calibration.get_camera_calib(rgb_stream_label) gaze_projection = get_gaze_vector_reprojection( eye_gaze_info, rgb_stream_label, device_calibration, rgb_camera_calibration, depth_m, ) print(gaze_projection)  ","version":"Next","tagName":"h3"},{"title":"Static camera calibration​","type":1,"pageTitle":"MPS Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/mps#static-camera-calibration","content":"PythonC++ import projectaria_tools.core.mps as mps static_cameras_path = &quot;/path/to/mps/output/trajectory/static_cam_calibs.csv&quot; static_cameras = mps.read_static_camera_calibrations(static_cameras_path)  ","version":"Next","tagName":"h3"},{"title":"Getting Started with Project Aria Data Utilities","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/getting_started","content":"","keywords":"","version":"Next"},{"title":"Running Jupyter Notebooks on Google Colab​","type":1,"pageTitle":"Getting Started with Project Aria Data Utilities","url":"/projectaria_tools/docs/data_utilities/getting_started#running--jupyter-notebooks-on-google-colab","content":"Use the following links to run the Python notebooks in an installation free playground: Dataprovider Quickstart Tutorial - core tutorialMachine Perception Services Tutorial - core tutorialAria Digital Twin (ADT) - open data tutorial ","version":"Next","tagName":"h2"},{"title":"Running the Jupyter Notebooks locally​","type":1,"pageTitle":"Getting Started with Project Aria Data Utilities","url":"/projectaria_tools/docs/data_utilities/getting_started#running-the-jupyter-notebooks-locally","content":"","version":"Next","tagName":"h2"},{"title":"Step 0 : Check system requirements and download codebase​","type":1,"pageTitle":"Getting Started with Project Aria Data Utilities","url":"/projectaria_tools/docs/data_utilities/getting_started#step-0--check-system-requirements-and-download-codebase","content":"Go to the Download Codebase page to: Check your system is supportedDownload projectaria_tools codebase from the github ","version":"Next","tagName":"h3"},{"title":"Step 1 : Install/Update Python 3​","type":1,"pageTitle":"Getting Started with Project Aria Data Utilities","url":"/projectaria_tools/docs/data_utilities/getting_started#step-1--installupdate-python-3","content":"To use the Jupyter notebooks in this tutorial you'll need Python 3.9 or above. To ensure all utilities work effectively, we recommend keeping Python 3 up to date. Python 3 download pageTo check what what version of Python 3 you have use python3 --version ","version":"Next","tagName":"h3"},{"title":"Step 2 : Create a virtual environment​","type":1,"pageTitle":"Getting Started with Project Aria Data Utilities","url":"/projectaria_tools/docs/data_utilities/getting_started#step-2--create-a-virtual-environment","content":"rm -rf $HOME/projectaria_tools_python_env python3 -m venv $HOME/projectaria_tools_python_env source $HOME/projectaria_tools_python_env/bin/activate  ","version":"Next","tagName":"h3"},{"title":"Step 3 : Install projectaria_tools from pypi​","type":1,"pageTitle":"Getting Started with Project Aria Data Utilities","url":"/projectaria_tools/docs/data_utilities/getting_started#step-3--install-projectaria_tools-from-pypi","content":"pip3 install --upgrade pip pip3 install projectaria-tools'[all]'  ","version":"Next","tagName":"h3"},{"title":"Step 4: Run Dataprovider quickstart tutorial​","type":1,"pageTitle":"Getting Started with Project Aria Data Utilities","url":"/projectaria_tools/docs/data_utilities/getting_started#step-4-run-dataprovider-quickstart-tutorial","content":"cd $HOME/Documents/projectaria_sandbox jupyter notebook projectaria_tools/core/examples/dataprovider_quickstart_tutorial.ipynb  Jupyter notebook error If you get a Jupyter notebook error please upgrade Python 3 to the latest version and recreate your virtual environment. ","version":"Next","tagName":"h3"},{"title":"Step 5: Run Sophus Pybind Tutorial​","type":1,"pageTitle":"Getting Started with Project Aria Data Utilities","url":"/projectaria_tools/docs/data_utilities/getting_started#step-5-run-sophus-pybind-tutorial","content":"cd $HOME/Documents/projectaria_sandbox jupyter notebook projectaria_tools/core/examples/sophus_quickstart_tutorial.ipynb  ","version":"Next","tagName":"h3"},{"title":"Step 6: Run Machine Perception Services (MPS) quickstart tutorial​","type":1,"pageTitle":"Getting Started with Project Aria Data Utilities","url":"/projectaria_tools/docs/data_utilities/getting_started#step-6-run-machine-perception-services-mps-quickstart-tutorial","content":"In the MPS tutorial, the notebook walks through how to visualize gaze, trajectory, and point cloud from MPS data. cd $HOME/Documents/projectaria_sandbox jupyter notebook projectaria_tools/core/examples/mps_quickstart_tutorial.ipynb  ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"Getting Started with Project Aria Data Utilities","url":"/projectaria_tools/docs/data_utilities/getting_started#troubleshooting","content":"Check the Troubleshooting Guide if you encounter issues using this tutorial. ","version":"Next","tagName":"h2"},{"title":"Other Useful Links​","type":1,"pageTitle":"Getting Started with Project Aria Data Utilities","url":"/projectaria_tools/docs/data_utilities/getting_started#other-useful-links","content":"TroubleshootingInstallation guideVisualizers PythonC++ ","version":"Next","tagName":"h2"},{"title":"How to Download Project Aria Tools","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/installation/download_codebase","content":"","keywords":"","version":"Next"},{"title":"Supported Platforms​","type":1,"pageTitle":"How to Download Project Aria Tools","url":"/projectaria_tools/docs/data_utilities/installation/download_codebase#supported-platforms","content":"The codebase is supported on: x64 Linux distributions of: Fedora 36, 37, 38Ubuntu focal (20.04 LTS) and jammy (22.04 LTS) Mac Intel or Mac ARM-based (M1) with MacOS 11 (Big Sur) or newer ","version":"Next","tagName":"h2"},{"title":"Stable versus Develop​","type":1,"pageTitle":"How to Download Project Aria Tools","url":"/projectaria_tools/docs/data_utilities/installation/download_codebase#stable-versus-develop","content":"Access the latest stable version of Project Aria Tools with git TAGS. The develop version is pushed continuously to the main branch. ","version":"Next","tagName":"h2"},{"title":"Download codebase​","type":1,"pageTitle":"How to Download Project Aria Tools","url":"/projectaria_tools/docs/data_utilities/installation/download_codebase#download-codebase","content":"StableDevelop mkdir -p $HOME/Documents/projectaria_sandbox cd $HOME/Documents/projectaria_sandbox git clone https://github.com/facebookresearch/projectaria_tools.git -b 1.4.0  ","version":"Next","tagName":"h2"},{"title":"How to Download MPS sample dataset","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/installation/download_mps_sample_data","content":"How to Download MPS sample dataset This sample (hosted at projectaria.com) contains a raw VRS file and all the corresponding MPS outputs. info If you are in zsh (default on Mac OS), you may need to run setopt shwordsplit for the following instructions to work export MPS_SAMPLE_PATH=/tmp/mps_sample export BASE_URL=&quot;https://www.projectaria.com/async/sample/download/?bucket=mps&amp;filename=&quot; mkdir -p $MPS_SAMPLE_PATH export OPTIONS=&quot;-C - -O -L&quot; curl -o $MPS_SAMPLE_PATH/sample.vrs $OPTIONS &quot;${BASE_URL}sample.vrs&quot; curl -o $MPS_SAMPLE_PATH/trajectory.zip $OPTIONS &quot;${BASE_URL}trajectory.zip&quot; curl -o $MPS_SAMPLE_PATH/eye_gaze.zip $OPTIONS &quot;${BASE_URL}eye_gaze.zip&quot; unzip -o $MPS_SAMPLE_PATH/eye_gaze.zip -d $MPS_SAMPLE_PATH unzip -o $MPS_SAMPLE_PATH/trajectory.zip -d $MPS_SAMPLE_PATH The above commands will download the most recent sample dataset from Project Aria. You can also download a more limited sample dataset from our GitHub repository.","keywords":"","version":"Next"},{"title":"How to Install the Python Package for Project Aria Tools","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/installation/installation_python","content":"","keywords":"","version":"Next"},{"title":"Install via virtual environment​","type":1,"pageTitle":"How to Install the Python Package for Project Aria Tools","url":"/projectaria_tools/docs/data_utilities/installation/installation_python#install-via-virtual-environment","content":"","version":"Next","tagName":"h2"},{"title":"Step 1 : Install Python​","type":1,"pageTitle":"How to Install the Python Package for Project Aria Tools","url":"/projectaria_tools/docs/data_utilities/installation/installation_python#step-1--install-python","content":"To use the Jupyter notebooks featured in our documentation you'll need Python 3.9 or above. To ensure all utilities work effectively, we recommend keeping Python 3 up to date. Python 3 download pageTo check what what version of Python 3 you have use python3 --version ","version":"Next","tagName":"h3"},{"title":"Step 2 : Create a virtual environment​","type":1,"pageTitle":"How to Install the Python Package for Project Aria Tools","url":"/projectaria_tools/docs/data_utilities/installation/installation_python#step-2--create-a-virtual-environment","content":"rm -rf $HOME/projectaria_tools_python_env python3 -m venv $HOME/projectaria_tools_python_env source $HOME/projectaria_tools_python_env/bin/activate  ","version":"Next","tagName":"h3"},{"title":"Step 3 : Install the required Python packages​","type":1,"pageTitle":"How to Install the Python Package for Project Aria Tools","url":"/projectaria_tools/docs/data_utilities/installation/installation_python#step-3--install-the-required-python-packages","content":"pip3 install --upgrade pip pip3 install projectaria-tools'[all]'  ","version":"Next","tagName":"h3"},{"title":"Step 4 : Use Project Aria Tools​","type":1,"pageTitle":"How to Install the Python Package for Project Aria Tools","url":"/projectaria_tools/docs/data_utilities/installation/installation_python#step-4--use-project-aria-tools","content":"A few resources are: Getting Started with Project Aria Data Utilities - Python TutorialRequest Machine Perception Services - for Research Partners Python Module Error? Check the Python module import error section of Data Utilities Troubleshooting Guide if you encounter this issue ","version":"Next","tagName":"h3"},{"title":"Building Python bindings from source (advanced user)​","type":1,"pageTitle":"How to Install the Python Package for Project Aria Tools","url":"/projectaria_tools/docs/data_utilities/installation/installation_python#building-python-bindings-from-source-advanced-user","content":"You'll need to install C++ dependencies to build Python bindings from source. Go to the C++ Installation page and follow the instructions to install dependenciesGo to the projectaria_tools code folderEnter the following commands cd $HOME/Documents/projectaria_sandbox/projectaria_tools pip3 install --upgrade pip pip3 install .  info For multithread build use the following: CMAKE_BUILD_PARALLEL_LEVEL=4 pip3 install . If you encounter error during the build process like ERROR: No matching distribution found for library X: Double check that you are using the right pip, or python version and then use python -m pip install . or python3 -m pip install . ","version":"Next","tagName":"h2"},{"title":"Generate and install type hinting from source (advanced user)​","type":1,"pageTitle":"How to Install the Python Package for Project Aria Tools","url":"/projectaria_tools/docs/data_utilities/installation/installation_python#generate-and-install-type-hinting-from-source-advanced-user","content":"Install pybind11-stubgen to generate the stub filesGenerate python type hinting with generate_stubs.py script once projectaria_tools package is installedInstall type hinting package for projectaria_tools pip3 install pybind11-stubgen==1.1 cd $HOME/Documents/projectaria_sandbox/projectaria_tools python3 generate_stubs.py cp -r projectaria_tools-stubs/projectaria_tools . pip3 install .  ","version":"Next","tagName":"h2"},{"title":"How to Install Project Aria Tools for C++","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/installation/installation_cpp","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"How to Install Project Aria Tools for C++","url":"/projectaria_tools/docs/data_utilities/installation/installation_cpp#overview","content":"This page provides instructions on how to: Build projectaria_tools for C++, without visualizationBuild projectaria_tools for C++, with visualization To install dependencies, follow the instructions for build without visualization first ","version":"Next","tagName":"h2"},{"title":"Build from source without visualization​","type":1,"pageTitle":"How to Install Project Aria Tools for C++","url":"/projectaria_tools/docs/data_utilities/installation/installation_cpp#build-from-source-without-visualization","content":"","version":"Next","tagName":"h2"},{"title":"Step 1 : Install dependencies​","type":1,"pageTitle":"How to Install Project Aria Tools for C++","url":"/projectaria_tools/docs/data_utilities/installation/installation_cpp#step-1--install-dependencies","content":"UbuntuFedoraMacOS # Install build essentials sudo apt install build-essential git cmake # Install VRS/Pangolin dependencies sudo apt install libgtest-dev libgmock-dev libgoogle-glog-dev libfmt-dev \\ liblz4-dev libzstd-dev libxxhash-dev libboost-all-dev libpng-dev \\ libjpeg-turbo8-dev libturbojpeg0-dev libglew-dev libgl1-mesa-dev libeigen3-dev  ","version":"Next","tagName":"h3"},{"title":"Step 2 : Compile C++ source code​","type":1,"pageTitle":"How to Install Project Aria Tools for C++","url":"/projectaria_tools/docs/data_utilities/installation/installation_cpp#step-2--compile-c-source-code","content":"cd $HOME/Documents/projectaria_sandbox mkdir -p build &amp;&amp; cd build # compile the C++ API cmake ../projectaria_tools/ make -j2   ","version":"Next","tagName":"h3"},{"title":"Build from source with visualization​","type":1,"pageTitle":"How to Install Project Aria Tools for C++","url":"/projectaria_tools/docs/data_utilities/installation/installation_cpp#build-from-source-with-visualization","content":"","version":"Next","tagName":"h2"},{"title":"Step 1 : Install dependencies​","type":1,"pageTitle":"How to Install Project Aria Tools for C++","url":"/projectaria_tools/docs/data_utilities/installation/installation_cpp#step-1--install-dependencies-1","content":"Follow the above steps to install dependencies build from source ","version":"Next","tagName":"h3"},{"title":"Step 2 : Compile Pangolin​","type":1,"pageTitle":"How to Install Project Aria Tools for C++","url":"/projectaria_tools/docs/data_utilities/installation/installation_cpp#step-2---compile-pangolin","content":"The viewers are built using Pangolin. # compile &amp; install Pangolin cd /tmp git clone --recursive https://github.com/stevenlovegrove/Pangolin.git mkdir -p Pangolin_Build &amp;&amp; cd Pangolin_Build cmake -DCMAKE_BUILD_TYPE=Release -DBUILD_TOOLS=OFF -DBUILD_PANGOLIN_PYTHON=OFF \\ -DBUILD_EXAMPLES=OFF ../Pangolin/ make -j2 sudo make install  ","version":"Next","tagName":"h3"},{"title":"Step 3 : Build projectaria_tools with visualization​","type":1,"pageTitle":"How to Install Project Aria Tools for C++","url":"/projectaria_tools/docs/data_utilities/installation/installation_cpp#step-3--build-projectaria_tools-with-visualization","content":"cd $HOME/Documents/projectaria_sandbox mkdir -p build &amp;&amp; cd build # Build C++ Aria Viewer cmake ../projectaria_tools -DPROJECTARIA_TOOLS_BUILD_TOOLS=ON make -j2  ","version":"Next","tagName":"h3"},{"title":"Step 4 : Verify installation by running the viewer​","type":1,"pageTitle":"How to Install Project Aria Tools for C++","url":"/projectaria_tools/docs/data_utilities/installation/installation_cpp#step-4--verify-installation-by-running-the-viewer","content":"cd $HOME/Documents/projectaria_sandbox/build # Running the Aria Viewer with default example data ./tools/visualization/aria_viewer \\ --vrs ../projectaria_tools/data/mps_sample/sample.vrs  ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"How to Install Project Aria Tools for C++","url":"/projectaria_tools/docs/data_utilities/installation/installation_cpp#troubleshooting","content":"Check the Troubleshooting Guide if you encounter any issues. ","version":"Next","tagName":"h2"},{"title":"Tutorial: How to Plot Sensor Data Using Python","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/plotting_sensor_data","content":"","keywords":"","version":"Next"},{"title":"Save Images as PNGs​","type":1,"pageTitle":"Tutorial: How to Plot Sensor Data Using Python","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/plotting_sensor_data#save-images-as-pngs","content":"Because we support converting image data to numpy arrays, images can be converted to PIL images and saved as PNG files. from PIL import Image stream_mappings = { &quot;camera-slam-left&quot;: StreamId(&quot;1201-1&quot;), &quot;camera-slam-right&quot;: StreamId(&quot;1201-2&quot;), &quot;camera-rgb&quot;: StreamId(&quot;214-1&quot;), &quot;camera-eyetracking&quot;: StreamId(&quot;211-1&quot;), } for [stream_name, stream_id] in stream_mappings.items(): image = provider.get_image_data_by_index(stream_id, index) Image.fromarray(image[0].to_numpy_array()).save(f'{stream_name}.png')  The above snippets will save the following images to the local folder: SLAM images Eye Tracking images RGB images\t ","version":"Next","tagName":"h3"},{"title":"Plotting IMU​","type":1,"pageTitle":"Tutorial: How to Plot Sensor Data Using Python","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/plotting_sensor_data#plotting-imu","content":"Organize the data into 6 lists. Each list stores one axis of a specific IMU. stream_id = provider.get_stream_id_from_label(&quot;imu-left&quot;) accel_x = [] accel_y = [] accel_z = [] gyro_x = [] gyro_y = [] gyro_z = [] timestamps = [] for index in range(0, provider.get_num_data(stream_id)): imu_data = provider.get_imu_data_by_index(stream_id, index) accel_x.append(imu_data.accel_msec2[0]) accel_y.append(imu_data.accel_msec2[1]) accel_z.append(imu_data.accel_msec2[2]) gyro_x.append(imu_data.gyro_radsec[0]) gyro_y.append(imu_data.gyro_radsec[1]) gyro_z.append(imu_data.gyro_radsec[2]) timestamps.append(imu_data.capture_timestamp_ns * 1e-9)  Plot the data with matplotlib plt.figure() fig, axes = plt.subplots(1, 2, figsize=(12, 5)) fig.suptitle(f&quot;{stream_id.get_name()}&quot;) axes[0].plot(timestamps, accel_x, 'r-', label=&quot;x&quot;) axes[0].plot(timestamps, accel_y, 'g-', label=&quot;y&quot;) axes[0].plot(timestamps, accel_z, 'b-', label=&quot;z&quot;) axes[0].legend(loc='upper left') axes[0].grid('on') axes[0].set_xlabel('timestamps (s)') axes[0].set_ylabel('accelerometer readout (m/sec2)') axes[1].plot(timestamps, gyro_x, 'r-', label=&quot;x&quot;) axes[1].plot(timestamps, gyro_y, 'g-', label=&quot;y&quot;) axes[1].plot(timestamps, gyro_z, 'b-', label=&quot;z&quot;) axes[1].legend(loc='upper left') axes[1].grid('on') axes[1].set_xlabel('timestamps (s)') axes[1].set_ylabel('gyroscope readout (rad/sec)')  The plotted image looks like this: Save the plot to PDF plt.savefig(&quot;imu.pdf&quot;, format=&quot;pdf&quot;, bbox_inches=&quot;tight&quot;)  ","version":"Next","tagName":"h3"},{"title":"Magnetometer​","type":1,"pageTitle":"Tutorial: How to Plot Sensor Data Using Python","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/plotting_sensor_data#magnetometer","content":"Plotting magnetometer is similar to plotting IMU. Organize the data into 3 lists. Each list stores one axis of magnetometer data. stream_id = provider.get_stream_id_from_label(&quot;mag0&quot;) mag_x = [] mag_y = [] mag_z = [] timestamps = [] for index in range(0, provider.get_num_data(stream_id)): mag_data = provider.get_magnetometer_data_by_index(stream_id, index) mag_x.append(mag_data.mag_tesla[0] * 1e6) mag_y.append(mag_data.mag_tesla[1] * 1e6) mag_z.append(mag_data.mag_tesla[2] * 1e6) timestamps.append(mag_data.capture_timestamp_ns * 1e-9)  Plot the data with matplotlib plt.figure() fig, axes = plt.subplots(1, 1, figsize=(12, 5)) fig.suptitle(f&quot;Magnetometer signal&quot;) axes.plot(timestamps, mag_x, 'r-', label=&quot;x&quot;) axes.plot(timestamps, mag_y, 'g-', label=&quot;y&quot;) axes.plot(timestamps, mag_z, 'b-', label=&quot;z&quot;) axes.legend(loc='upper left') axes.grid('on') axes.set_xlabel('timestamps (s)') axes.set_ylabel('magnetometer readout (uT)') plt.savefig(&quot;mag.pdf&quot;, format=&quot;pdf&quot;, bbox_inches=&quot;tight&quot;)   ","version":"Next","tagName":"h3"},{"title":"Audio​","type":1,"pageTitle":"Tutorial: How to Plot Sensor Data Using Python","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/plotting_sensor_data#audio","content":"Audio data is interesting since each data is in fact a 7x4096 chunk Load the audio data stream_id = provider.get_stream_id_from_label(&quot;mic&quot;) timestamps = [] audio = [[] for c in range(0, 7)] for index in range(0, 2): audio_data_i = provider.get_audio_data_by_index(stream_id, index) audio_signal_block = audio_data_i[0].data timestamps_block = [t * 1e-9 for t in audio_data_i[1].capture_timestamps_ns]; timestamps += timestamps_block for c in range(0, 7): audio[c] += audio_signal_block[c::7]  Plot the data with matplotlib plt.figure() fig, axes = plt.subplots(1, 1, figsize=(12, 5)) fig.suptitle(f&quot;Microphone signal&quot;) for c in range(0, 7): plt.plot(timestamps, audio[c], '-', label = f&quot;channel {c}&quot;) axes.legend(loc='upper left') axes.grid('on') axes.set_xlabel('timestamps (s)') axes.set_ylabel('audio readout') plt.savefig(&quot;audio.pdf&quot;, format=&quot;pdf&quot;, bbox_inches=&quot;tight&quot;)   ","version":"Next","tagName":"h3"},{"title":"Barometer​","type":1,"pageTitle":"Tutorial: How to Plot Sensor Data Using Python","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/plotting_sensor_data#barometer","content":"Load and plot the data using the following commands plt.figure() fig, axes = plt.subplots(1, 2, figsize=(12, 5)) fig.suptitle(f&quot;Barometer signal&quot;) stream_id = provider.get_stream_id_from_label(&quot;baro0&quot;) pressure = [] temperature = [] timestamps = [] for index in range(0, provider.get_num_data(stream_id)): baro_data = provider.get_barometer_data_by_index(stream_id, index) pressure.append(baro_data.pressure * 1e-3) temperature.append(baro_data.temperature) timestamps.append(baro_data.capture_timestamp_ns * 1e-9) axes[0].plot(timestamps, pressure, 'r-') axes[0].grid('on') axes[0].set_xlabel('timestamps (s)') axes[0].set_ylabel('pressure readout (kPascal)') axes[1].plot(timestamps, temperature, 'r-') axes[1].grid('on') axes[1].set_xlabel('timestamps (s)') axes[1].set_ylabel('temperature readout (C)') plt.savefig(&quot;baro.pdf&quot;, format=&quot;pdf&quot;, bbox_inches=&quot;tight&quot;)   ","version":"Next","tagName":"h3"},{"title":"GPS​","type":1,"pageTitle":"Tutorial: How to Plot Sensor Data Using Python","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/plotting_sensor_data#gps","content":"GPS data can be visualized with 2D or 3D plots. 2D plots​ plt.figure() fig, axes = plt.subplots(1, 3, figsize=(12, 3)) fig.suptitle(f&quot;GPS signal&quot;) stream_id = provider.get_stream_id_from_label(&quot;gnss&quot;) latitude = [] longitude = [] altitude = [] timestamps = [] for index in range(100, 300): gps_data = provider.get_gps_data_by_index(stream_id, index) latitude.append(gps_data.latitude) longitude.append(gps_data.longitude) altitude.append(gps_data.altitude) timestamps.append(gps_data.capture_timestamp_ns * 1e-9) ax = axes[0] ax.plot(timestamps, latitude, 'r-') ax.grid('on') ax.set_xlabel('timestamps (s)') ax.set_ylabel('latitude') ax.yaxis.set_major_formatter(ticker.ScalarFormatter(useMathText=True, useOffset=False)) ax = axes[1] ax.plot(timestamps, longitude, 'r-') ax.grid('on') ax.set_xlabel('timestamps (s)') ax.set_ylabel('longitude') ax.yaxis.set_major_formatter(ticker.ScalarFormatter(useMathText=True, useOffset=False)) ax = axes[2] ax.plot(timestamps, altitude, 'r-') ax.grid('on') ax.set_xlabel('timestamps (s)') ax.set_ylabel('altitude') ax.yaxis.set_major_formatter(ticker.ScalarFormatter(useMathText=True, useOffset=False)) fig.tight_layout() plt.savefig(&quot;gps.pdf&quot;, format=&quot;pdf&quot;, bbox_inches=&quot;tight&quot;)   3D plots​ plt.figure() fig = plt.figure() axes = fig.add_subplot(projection='3d') axes.plot(latitude, longitude, altitude) axes.view_init(elev=20., azim=-35, roll=0) plt.savefig(&quot;gps3d.pdf&quot;, format=&quot;pdf&quot;, bbox_inches=&quot;tight&quot;)   ","version":"Next","tagName":"h3"},{"title":"Wi-Fi beacon​","type":1,"pageTitle":"Tutorial: How to Plot Sensor Data Using Python","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/plotting_sensor_data#wi-fi-beacon","content":"Group the Wi-Fi beacon data by mac bssid stream_id = provider.get_stream_id_from_label(&quot;wps&quot;) rssi = {} timestamps = {} print(provider.get_num_data(stream_id)) for index in range(0, provider.get_num_data(stream_id)): wps_data = provider.get_wps_data_by_index(stream_id, index) if wps_data.bssid_mac not in rssi: rssi[wps_data.bssid_mac] = [] timestamps[wps_data.bssid_mac] = [] rssi[wps_data.bssid_mac].append(wps_data.rssi) timestamps[wps_data.bssid_mac].append(wps_data.board_timestamp_ns * 1e-9)  Plot the mac address This example has &gt; 15 samples plt.figure() fig, ax = plt.subplots(1, 1, figsize=(6, 5)) fig.suptitle(f&quot;Wi-Fi beacon signal&quot;) for ssid in list(timestamps.keys()): if len(timestamps[ssid]) &lt; 15: continue ax.scatter(timestamps[ssid], rssi[ssid], label=ssid) ax.grid('on') ax.set_xlabel('timestamps (s)') ax.set_ylabel('Wi-Fi RSSI(dBm)') plt.legend(loc='upper left') plt.savefig(&quot;wifi.pdf&quot;, format=&quot;pdf&quot;, bbox_inches=&quot;tight&quot;)   ","version":"Next","tagName":"h3"},{"title":"Bluetooth beacon​","type":1,"pageTitle":"Tutorial: How to Plot Sensor Data Using Python","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/plotting_sensor_data#bluetooth-beacon","content":"Group data by unique_id (similar to Wi-Fi grouping) stream_id = provider.get_stream_id_from_label(&quot;bluetooth&quot;) rssi = {} timestamps = {} for index in range(0, provider.get_num_data(stream_id)): bluetooth_data = provider.get_bluetooth_data_by_index(stream_id, index) if bluetooth_data.unique_id not in rssi: rssi[bluetooth_data.unique_id] = [] timestamps[bluetooth_data.unique_id] = [] rssi[bluetooth_data.unique_id].append(bluetooth_data.rssi) timestamps[bluetooth_data.unique_id].append(bluetooth_data.board_timestamp_ns * 1e-9)  Plot the data per unique_id plt.figure() fig, ax = plt.subplots(1, 1, figsize=(6, 5)) fig.suptitle(f&quot;Bluetooth beacon signal&quot;) for ssid in list(timestamps.keys()): ax.plot(timestamps[ssid], rssi[ssid], '.') ax.grid('on') ax.set_xlabel('timestamps (s)') ax.set_ylabel('bluetooth RSSI(dBm') fig.tight_layout() plt.savefig(&quot;ble.pdf&quot;, format=&quot;pdf&quot;, bbox_inches=&quot;tight&quot;)   ","version":"Next","tagName":"h3"},{"title":"How to use projectaria_tools type annotation","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/installation/type_hinting","content":"","keywords":"","version":"Next"},{"title":"How to use type hinting​","type":1,"pageTitle":"How to use projectaria_tools type annotation","url":"/projectaria_tools/docs/data_utilities/installation/type_hinting#how-to-use-type-hinting","content":"Please follow the link to install projectaria_tools package. ","version":"Next","tagName":"h2"},{"title":"Type hinting setup for vscode​","type":1,"pageTitle":"How to use projectaria_tools type annotation","url":"/projectaria_tools/docs/data_utilities/installation/type_hinting#type-hinting-setup-for-vscode","content":"Install python extensionSelect your own virtual environment on the bottom right cornerEnter the virtual environment path (i.e $HOME/projectaria_tools_python_env/bin/python)Type hinting appears when hover the mouse over the imported functions in your python code ","version":"Next","tagName":"h3"},{"title":"Type hinting setup for pycharm​","type":1,"pageTitle":"How to use projectaria_tools type annotation","url":"/projectaria_tools/docs/data_utilities/installation/type_hinting#type-hinting-setup-for-pycharm","content":"Add new interpreterEnter the virtual environment path (i.e $HOME/projectaria_tools_python_env/bin/python)Type hinting appears when hover the mouse over the imported functions in your python code ","version":"Next","tagName":"h3"},{"title":"Project Aria Tools Troubleshooting","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/installation/troubleshooting","content":"","keywords":"","version":"Next"},{"title":"Jupyter notebook issues​","type":1,"pageTitle":"Project Aria Tools Troubleshooting","url":"/projectaria_tools/docs/data_utilities/installation/troubleshooting#jupyter-notebook-issues","content":"","version":"Next","tagName":"h2"},{"title":"Jupyter notebook error​","type":1,"pageTitle":"Project Aria Tools Troubleshooting","url":"/projectaria_tools/docs/data_utilities/installation/troubleshooting#jupyter-notebook-error","content":"Jupyter notebook works with Python 3.9 or above. If you have problems using Jupyter examples, please upgrade Python 3 to the latest version. If you are using a virtual environment you'll need to recreate it to bring in the update. ","version":"Next","tagName":"h3"},{"title":"Python module import error​","type":1,"pageTitle":"Project Aria Tools Troubleshooting","url":"/projectaria_tools/docs/data_utilities/installation/troubleshooting#python-module-import-error","content":"There are several things that could cause this error message. Python version mismatch​ When running Jupyter notebook, it might use a Python 3 version that's not in the virtual environment. There are two ways you can resolve this issue. Remove the Jupyter notebook from outside of the virtual environmentDirectly start the Jupyter notebook from the virtual environment bin folder. If the virtual environment was created using python3 -m venv $HOME/projectaria_tools_python_env, you can directly call Jupyter from the virtual env as $HOME/projectaria_tools_python_env/bin/jupyter notebook notebook_example.ipynb  Old version of projectaria_tools​ You may also encounter a Python module import error if you are running an old version of projectaria_tools. Make sure you've installed the latest version of projectaria_tools. ","version":"Next","tagName":"h3"},{"title":"Visualizer issues​","type":1,"pageTitle":"Project Aria Tools Troubleshooting","url":"/projectaria_tools/docs/data_utilities/installation/troubleshooting#visualizer-issues","content":"","version":"Next","tagName":"h2"},{"title":"Visualizer does not build​","type":1,"pageTitle":"Project Aria Tools Troubleshooting","url":"/projectaria_tools/docs/data_utilities/installation/troubleshooting#visualizer-does-not-build","content":"If the visualizer does not build it may be because of missing Pangolin functions. Aria Digital Twin (ADT) dataset depends on very recent changes to Pangolin's master branch. If ADT depends on Pangolin functions that are not available on your installed version of Pangolin, please reinstall using the most recent master. ","version":"Next","tagName":"h3"},{"title":"Runtime errors/missing libraries​","type":1,"pageTitle":"Project Aria Tools Troubleshooting","url":"/projectaria_tools/docs/data_utilities/installation/troubleshooting#runtime-errorsmissing-libraries","content":"Runtime errors can be caused by missing libraries. The following commands may resolve the issue. # Missing libpango_geometry.so, libpango_windowing.so, etc sudo ldconfig LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib/ export LD_LIBRARY_PATH  ","version":"Next","tagName":"h3"},{"title":"Visualizer Window freezes - X11 known issue​","type":1,"pageTitle":"Project Aria Tools Troubleshooting","url":"/projectaria_tools/docs/data_utilities/installation/troubleshooting#visualizer-window-freezes---x11-known-issue","content":"If you are running a platform that uses X11 the Visualizer window may freeze. This is most likely because of a graphics driver bug in X11.Pangolin has a discussion on the issue. If the issue is triggered by Pangolin Plotter, the fix is swap from X11 to EGL. Step 1: Check the cause​ Test to see if it's a display driver issue triggered by Pangolin Plotter. Build the latest version of Pangolin cd /tmp/Pangolin_Build cmake -DCMAKE_BUILD_TYPE=Release -DBUILD_TOOLS=OFF -DBUILD_PANGOLIN_PYTHON=OFF \\ -DBUILD_EXAMPLES=ON ../Pangolin/ make -j2  Run the following example, it should work without issues ./examples/BasicOpenGl/tutorial_3_gl_intro_classic_triangle_vbo_shader  Run the following example, if it shows a black window and the machine freezes, this may be the graphics driver issue. Move on to Step 2. ./examples/SimplePlot/SimplePlot  Step 2: Checkout the fix rebase onto master​ Use the following commands to checkout the fix on github and rebase onto master cd /tmp/Pangolin git fetch origin pull/389/head:x11_to_egl git checkout x11_to_egl git rebase master # rebuild pangolin cd /tmp/Pangolin_Build cmake -DCMAKE_BUILD_TYPE=Release -DBUILD_TOOLS=OFF -DBUILD_PANGOLIN_PYTHON=OFF \\ -DBUILD_EXAMPLES=ON ../Pangolin/ make -j2 sudo make install  Confirm this is the correct fix by rebuilding and retesting SimplePlot from Step 1. Step 3: Patch CMake for Visualizers​ Update the AriaViewer CMakeLists in$HOME/Documents/projectaria_sandbox/projectaria_tools/tools/CMakeLists.txtUpdate AriaDigitalTwinViewer CMakeLists in$HOME/Documents/projectaria_sandbox/projectaria_tools/projects/AriaDigitalTwinDatasetTools/visualization/CMakeLists.txt By adding the following line: find_package(OpenGL QUIET COMPONENTS EGL)  Step 4. Rebuild Aria Viewer and validate that it works​ cd $HOME/Documents/projectaria_sandbox/build cmake ../projectaria_tools -DPROJECTARIA_TOOLS_BUILD_TOOLS=ON make -j2 ./tools/visualization/aria_viewer \\ --vrs ../projectaria_tools/data/mps_sample/sample.vrs  ","version":"Next","tagName":"h3"},{"title":"Project Aria Tools C++ Visualization","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/visualization/visualization_cpp","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Project Aria Tools C++ Visualization","url":"/projectaria_tools/docs/data_utilities/visualization/visualization_cpp#overview","content":"This page introduces our C++ visualization tools available in Project Aria Tools. We've provided example datasets to test these tools. Aria Viewer: visualize raw Aria dataMPS 3D Scene Viewer: renders a static scene using Aria data with trajectories, global point cloud, and static camera posesMPS 3D Replay Viewer: renders static scene and dynamic elements: 2D/3D observations rays + eye gaze dataMPS Eye Gaze Viewer: visualize Aria data with eye gaze data  ","version":"Next","tagName":"h2"},{"title":"Requirements​","type":1,"pageTitle":"Project Aria Tools C++ Visualization","url":"/projectaria_tools/docs/data_utilities/visualization/visualization_cpp#requirements","content":"","version":"Next","tagName":"h2"},{"title":"Step 0 : Check system requirements and download codebase​","type":1,"pageTitle":"Project Aria Tools C++ Visualization","url":"/projectaria_tools/docs/data_utilities/visualization/visualization_cpp#step-0--check-system-requirements-and-download-codebase","content":"Go to the Download Codebase page to: Check your system is supportedDownload projectaria_tools codebase from the GitHub ","version":"Next","tagName":"h3"},{"title":"Step 1 : Download the sample dataset​","type":1,"pageTitle":"Project Aria Tools C++ Visualization","url":"/projectaria_tools/docs/data_utilities/visualization/visualization_cpp#step-1--download-the-sample-dataset","content":"Go to the Download MPS Sample dataset to retrieve a raw VRS file and all the corresponding MPS outputs. ","version":"Next","tagName":"h3"},{"title":"Step 2 : Build and install visualizers​","type":1,"pageTitle":"Project Aria Tools C++ Visualization","url":"/projectaria_tools/docs/data_utilities/visualization/visualization_cpp#step-2--build-and-install-visualizers","content":"The visualizers need the C++ version of Project Aria Tools to run. In the C++ Installation Guide, follow the instructions to build from source with visualization  ","version":"Next","tagName":"h3"},{"title":"Run Aria Viewer​","type":1,"pageTitle":"Project Aria Tools C++ Visualization","url":"/projectaria_tools/docs/data_utilities/visualization/visualization_cpp#run-aria-viewer","content":"Aria Viewer enable you to to visualize Aria device recorded VRS files. It shows all sensor data including: Camera imagesIMUAudio (visualization of waveform, sound is not available) cd $HOME/Documents/projectaria_sandbox/build ./tools/visualization/aria_viewer --vrs $MPS_SAMPLE_PATH/sample.vrs    ","version":"Next","tagName":"h2"},{"title":"Run MPS 3D Scene Viewer​","type":1,"pageTitle":"Project Aria Tools C++ Visualization","url":"/projectaria_tools/docs/data_utilities/visualization/visualization_cpp#run-mps-3d-scene-viewer","content":"The MPS 3D Scene Viewer renders a static scene using location MPS output. Through this tool you can create visualizations using: Closed loop trajectoriesGlobal point cloudStatic camera posesOpen loop trajectories Because open loop is in odometry frame of reference, it shouldn’t be visualized with closed loop trajectories, global points or static camera poses This tutorial generates a visualization containing: Closed loop trajectoriesGlobal point cloud cd $HOME/Documents/projectaria_sandbox/build ./tools/mps_visualization/mps_3d_scene_viewer \\ --closed-loop-traj $MPS_SAMPLE_PATH/trajectory/closed_loop_trajectory.csv \\ --global-point-cloud $MPS_SAMPLE_PATH/trajectory/global_points.csv.gz   info Because the sample dataset doesn't have static cameras you won't be able to interact with the static camera settings  ","version":"Next","tagName":"h2"},{"title":"Run MPS 3D Replay Viewer​","type":1,"pageTitle":"Project Aria Tools C++ Visualization","url":"/projectaria_tools/docs/data_utilities/visualization/visualization_cpp#run-mps-3d-replay-viewer","content":"The MPS 3D Replay Viewer renders static scene and dynamic elements at each frame: Aria's pose + 2D/3D observations rays + eye gaze data. Through this tool you can create visualizations using: Closed loop trajectoriesSemi-Dense Point Cloud Global point cloudPoint observations Static camera poses This tutorial generates a visualization containing: Static elements Closed loop trajectoriesGlobal point cloud Dynamic elements One closed loop trajectory for replay2D/3D point observations raysGeneralized and Personalized Eye Gaze vectors cd $HOME/Documents/projectaria_sandbox/build ./tools/mps_visualization/mps_3d_replay_viewer \\ --vrs $MPS_SAMPLE_PATH/sample.vrs \\ --replay-trajectory $MPS_SAMPLE_PATH/trajectory/closed_loop_trajectory.csv \\ --closed-loop-traj $MPS_SAMPLE_PATH/trajectory/closed_loop_trajectory.csv \\ --global-point-cloud $MPS_SAMPLE_PATH/trajectory/global_points.csv.gz \\ --point-obs $MPS_SAMPLE_PATH/trajectory/semidense_observations.csv.gz \\ --generalized-eye-gaze $MPS_SAMPLE_PATH/eye_gaze/generalized_eye_gaze.csv \\ --calibrated-eye-gaze $MPS_SAMPLE_PATH/eye_gaze/calibrated_eye_gaze.csv    ","version":"Next","tagName":"h2"},{"title":"MPS Eye Gaze visualizer​","type":1,"pageTitle":"Project Aria Tools C++ Visualization","url":"/projectaria_tools/docs/data_utilities/visualization/visualization_cpp#mps-eye-gaze-visualizer","content":"The MPS Eye Gaze visualizer renders the computed eye gaze and vrs data side by side. The visualizer contains: Eye Tracking camera streamRGB, Mono Scene (SLAM) left and right camera streams A red dot shows the projection of the eye gaze onto the imageThe projection is computed using a fixed depth of 1m 2D graph plot of the gaze yaw and pitch angles in radians2D radar plot of the eye gaze yaw and pitch angles ","version":"Next","tagName":"h2"},{"title":"Run visualizer and visualize both generalized and optional calibrated eye gaze​","type":1,"pageTitle":"Project Aria Tools C++ Visualization","url":"/projectaria_tools/docs/data_utilities/visualization/visualization_cpp#run-visualizer-and-visualize-both-generalized-and-optional-calibrated-eye-gaze","content":"cd $HOME/Documents/projectaria_sandbox/build ./tools/mps_visualization/mps_eyegaze_viewer --vrs $MPS_SAMPLE_PATH/sample.vrs \\ --generalized-eye-gaze $MPS_SAMPLE_PATH/eye_gaze/general_eye_gaze.csv \\ --calibrated-eye-gaze $MPS_SAMPLE_PATH/eye_gaze/personalized_eye_gaze.csv   ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"Project Aria Tools C++ Visualization","url":"/projectaria_tools/docs/data_utilities/visualization/visualization_cpp#troubleshooting","content":"Check the Troubleshooting Guide if you encounter issues using this tutorial. ","version":"Next","tagName":"h2"},{"title":"Project Aria Tools Python Visualization","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/visualization/visualization_python","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Project Aria Tools Python Visualization","url":"/projectaria_tools/docs/data_utilities/visualization/visualization_python#overview","content":"This page introduces our core Python visualization tools, developed with Rerun, that are part of Project Aria Tools. Aria Sensor Viewer: 3D visualization of Aria sensorsMPS Viewer: renders MPS metadata (point cloud, device trajectory and wearer eye gaze) ","version":"Next","tagName":"h2"},{"title":"Requirements​","type":1,"pageTitle":"Project Aria Tools Python Visualization","url":"/projectaria_tools/docs/data_utilities/visualization/visualization_python#requirements","content":"Python Project Aria Tools is installedMPS Sample data Official Sample Data - most up to date sample dataGitHub Repo Sample dataset - useful for unit testing etc. It will work, but is not maintained and updated the way the official sample data is  ","version":"Next","tagName":"h2"},{"title":"Run Aria Sensor Viewer​","type":1,"pageTitle":"Project Aria Tools Python Visualization","url":"/projectaria_tools/docs/data_utilities/visualization/visualization_python#run-aria-sensor-viewer","content":"viewer_aria_sensors displays the relative position and orientation of all most of Project Aria glasses sensors (cameras, IMUs, microphones, magnetometer &amp; barometer) in a common reference. viewer_aria_sensors --vrs $MPS_SAMPLE_PATH/sample.vrs   tip Selecting the different sensors in the Blueprint left column will help you quickly identify where a given sensor is located  ","version":"Next","tagName":"h2"},{"title":"Run MPS Viewer​","type":1,"pageTitle":"Project Aria Tools Python Visualization","url":"/projectaria_tools/docs/data_utilities/visualization/visualization_python#run-mps-viewer","content":"viewer_mps displays an interactive visualization of the Aria VRS RGB frames along with MPS data (Closed loop trajectory, Global point cloud, Wearer eye gaze). As you are playing or moving along the timeline, you can see the position of the camera and the wearer eye gaze direction at the timestamp of your choice.  viewer_mps --vrs $MPS_SAMPLE_PATH/sample.vrs or to specify each MPS file viewer_mps --vrs $MPS_SAMPLE_PATH/sample.vrs \\ --trajectory $MPS_SAMPLE_PATH/closed_loop_trajectory.csv \\ --points $MPS_SAMPLE_PATH/global_points.csv.gz \\ --eyegaze $MPS_SAMPLE_PATH/generalized_eye_gaze.csv    tip Switching between device_time and timestamp timeline allows you to retrieve a specific timestamp for the VRS sequence or MPS annotation ","version":"Next","tagName":"h2"},{"title":"Visualization of Multi-SLAM data​","type":1,"pageTitle":"Project Aria Tools Python Visualization","url":"/projectaria_tools/docs/data_utilities/visualization/visualization_python#visualization-of-multi-slam-data","content":"The MPS Viewer can also be used to visualize 3D data from multiple Project Aria devices, if the MPS data has been generated using Multi-SLAM or is part of datasets that contain this type of data, such as Aria Everyday Activities (AEA). AEA Example​ The example below uses Trajectory and Semi-Dense Point Cloud data from AEA. Go to AEA Dataset Download for how to download this data. viewer_mps --trajectory `find -P ~/Documents/projectaria_tools_aea_data/loc1*/*/*/closed_loop_trajectory.csv -print` --points `find -P Documents/projectaria_tools_aea_data/loc1*/*/*/semidense_points.csv.gz -print`   MPS CLI Example​ The following visualization uses Multi-Slam data generated using MPS CLI sample data. The MPS CLI is part of the Aria Research Kit. viewer_mps --trajectory `find -P ~/documents/multi_slam_output/*/*/closed_loop_trajectory.csv -print` --points `find -P ~/documents/multi_slam_output/*/*/semidense_points.csv.gz -print`    ","version":"Next","tagName":"h3"},{"title":"An introduction to Rerun​","type":1,"pageTitle":"Project Aria Tools Python Visualization","url":"/projectaria_tools/docs/data_utilities/visualization/visualization_python#an-introduction-to-rerun","content":"Rerun is an open source SDK and engine for visualizing and interacting with multi modal data streams. It's usable from Python, Rust and C++. Rerun consists in a log API and a visualizer. The main GUI sections of the Rerun visualizer are: BluePrint A: User defined Scene Graph (Entities and hierarchy you define)B: Visual view of the Scene Graph (User customizable) Timeline C: Interactive navigation and inspection of log events on multiple timeline (log, frame, or device time) Visibility and property control D: Fine grained control and inspection of Entities/Components  ","version":"Next","tagName":"h2"},{"title":"Project Aria Tools","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/intro","content":"","keywords":"","version":"Next"},{"title":"Further resources​","type":1,"pageTitle":"Project Aria Tools","url":"/projectaria_tools/docs/intro#further-resources","content":"projectaria.com - about the project, how to partner with Project Aria, learn about Grand Challenges and download datasetsProject Aria: A New Data Platform for Egocentric Multi-modal AI Research - Project Aria WhitepaperVRS (Vision Replay System) - A file format optimized to record and playback streams of sensor data, stored in per-device streams of timestamped recordsEgoBlur - an open source AI model from Meta to preserve privacy by detecting and blurring PII from images ","version":"Next","tagName":"h2"},{"title":"Overview of sections​","type":1,"pageTitle":"Project Aria Tools","url":"/projectaria_tools/docs/intro#overview-of-sections","content":"Technical Specifications: hardware specifications, the different configurations Aria glasses can use for recording and an overview of extrinsics and intrinsics calibration. Data Formats: information about data formatting conventions used with Aria raw sensor data (stored in VRS files) as well as Machine Perception Services (MPS) data. Data Utilities: our opensource C++/Python library provides the ability to work with Aria raw sensor data as well as MPS data. We also provide binaries implemented in C++ to visualize the data. If you want to immediately dive in with the code, go to the Python Getting Started guide for a quick tour of the library in a Jupyter notebook. Aria Research Kit: Project Aria glasses quickstart guide, the Companion apps, the Client SDK or request Machine Perception Services. Open Datasets, how to download the data and use our opensource tooling to visualize and access the data of: Aria Everyday Activities datasetAria Digital Twin datasetAria Synthetic Environments dataset Tech Insights: technical deeper dives on domain-specific topics. Attribution and Contributing: Citation information and how to contribute to Project Aria Tools. Support ","version":"Next","tagName":"h2"},{"title":"SW Release Notes​","type":1,"pageTitle":"Project Aria Tools","url":"/projectaria_tools/docs/intro#sw-release-notes","content":"Project Aria Tools SW Release notes are posted to Project Aria Tools GitHub repository, using Tags &gt; Releases. Release notes for the Aria Research Kit (requires Aria glasses) are posted to the ARK Release Notes section of this wiki. ","version":"Next","tagName":"h2"},{"title":"Project Aria Open Datasets","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets","content":"Project Aria Open Datasets This section provides information about how to use Project Aria's open data. To download the datasets go to https://www.projectaria.com/datasets/","keywords":"","version":"Next"},{"title":"Aria Digital Twin Dataset","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Aria Digital Twin Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset#overview","content":"Project Aria Tools provides Python and C++ APIs to access the Aria Digital Twin (ADT) dataset (paper). ","version":"Next","tagName":"h2"},{"title":"About the data​","type":1,"pageTitle":"Aria Digital Twin Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset#about-the-data","content":"ADT provides raw and synthesized sensor data from Project Aria glasses, combined with groundtruth data generated using a motion capture system including depth images, device trajectories, object trajectories and bounding boxes, and human tracking. We also provide processed sensor data from our Machine Perception Services. Go to ADT Data Format to see a full list of the data we provide. The ADT dataset contains 222 sequences recording single and dual-person activities. The data was recorded in two spaces: an apartment and a single room office. There are 74 single-instance dynamic objects shared between the two spaces. Go to the Getting Started Tutorial to explore the sample dataset (available on Google colab, no download necessary) or the Dataset Download page to get started. The sample dataset is a single-user dataset with body pose in the Apartment. It is a pretty representative example that should give you an idea of the dataset. ","version":"Next","tagName":"h2"},{"title":"Apartment scene​","type":1,"pageTitle":"Aria Digital Twin Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset#apartment-scene","content":"170 sequences were recorded in the apartment scene. The apartment comprised of a living room, kitchen, dining room and bedroom and contained 281 unique stationary objects. Given some objects have multiple instances that may differ slightly, the apartment has 324 stationary object instances in total. ","version":"Next","tagName":"h3"},{"title":"Office scene​","type":1,"pageTitle":"Aria Digital Twin Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset#office-scene","content":"52 sequences were recorded in the office scene, a single room with minimal office furniture. The office room contained 15 unique stationary objects and 20 stationary object instances. ","version":"Next","tagName":"h3"},{"title":"Activities​","type":1,"pageTitle":"Aria Digital Twin Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset#activities","content":"In the office scene, users examined objects. For the apartment scene we designed five single-person activities and three dual-person activities. The single-person activities were: Room decorationMeal preparationWorkObject examinationRoom cleaning The dual-person activities included: PartyingRoom cleaningDining table cleaning Every activity has 10 to 50 sequences and the activity names are embedded into the sequence_names. ","version":"Next","tagName":"h3"},{"title":"Other statistics:​","type":1,"pageTitle":"Aria Digital Twin Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset#other-statistics","content":"Number of multi-person sequences: 77Number of sequences with no skeletons: 110Number of sequences with 1 skeleton: 92Number of sequences with 2 skeleton: 20 info We provide a mix of datasets where users may or may not be wearing an Aria and/or a bodysuit. Please refer to the skeleton_aria_association.json to see the case for each specific sub-sequence. ","version":"Next","tagName":"h3"},{"title":"Documentation​","type":1,"pageTitle":"Aria Digital Twin Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset#documentation","content":"The ADT section of the wiki covers: Getting Started A quickstart tutorial available as a Google colab project or install Project Aria Tools python package to run locally, run the ADT notebook to access and visualize ground-truth data. Dataset Download A walkthrough of using adt_benchmark_dataset_downloader to download the published ADT dataset. Data Format How ADT data is organized and stored Data Loader APIs to load ADT data with handy code snippets Visualizers Compile and run our visualizer using an example that accesses ADT data in C++. Advanced tutorial A guide to learn how device synchronization works in ADT and run through a Jupyter notebook. ADT Challenges Learn more about the ADT Grand Challenge ","version":"Next","tagName":"h3"},{"title":"ADT Object Detection 2023-4 Challenges","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/adt_challenges","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"ADT Object Detection 2023-4 Challenges","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/adt_challenges#overview","content":"The 2023 Aria Digital Twin (ADT) challenges aim to accelerate research into 3D object detection and spatialization, serving as a catalyst for the broader research community. ","version":"Next","tagName":"h2"},{"title":"Tasks​","type":1,"pageTitle":"ADT Object Detection 2023-4 Challenges","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/adt_challenges#tasks","content":"","version":"Next","tagName":"h2"},{"title":"Challenge One - 3D Object detection & tracking with scene generalization​","type":1,"pageTitle":"ADT Object Detection 2023-4 Challenges","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/adt_challenges#challenge-one---3d-object-detection--tracking-with-scene-generalization","content":"The ADT benchmark dataset consists of a number of moving objects and recorded sequences ground-truthed in two distinct scenes. For this task, generate accurate 6DoF poses for the same set of objects within recordings from a completely new and unseen scene we provide in the challenge data. ","version":"Next","tagName":"h3"},{"title":"Challenge Two - Few-shot 3D Object detection & tracking​","type":1,"pageTitle":"ADT Object Detection 2023-4 Challenges","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/adt_challenges#challenge-two---few-shot-3d-object-detection--tracking","content":"We've provided a short period of ground-truth data for a specific target object within each sequence, your task is to leverage this data to accurately detect the same object and predict its 6DoF poses for the remaining duration of the sequence. ","version":"Next","tagName":"h3"},{"title":"Prize per challenge​","type":1,"pageTitle":"ADT Object Detection 2023-4 Challenges","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/adt_challenges#prize-per-challenge","content":"First place: $10,000 and a certificate. Second and Third places will receive certificates and recognition on the Challenge leaderboard. Additionally, teams with the best performing or noteworthy submissions may be invited to present their work at the CVPR conference in June 2024. ","version":"Next","tagName":"h2"},{"title":"Important dates​","type":1,"pageTitle":"ADT Object Detection 2023-4 Challenges","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/adt_challenges#important-dates","content":"Submission deadline for results: March 22, 2024 (11:59PM PST)Presentation of awards: CVPR in June 2024 (pending confirmation of our workshop) ","version":"Next","tagName":"h2"},{"title":"Key participation terms​","type":1,"pageTitle":"ADT Object Detection 2023-4 Challenges","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/adt_challenges#key-participation-terms","content":"Entrants can register as an Individual or TeamPredictions are submitted online at Eval.aiWinners agree to share a public report (e.g. whitepaper or publication) describing the method used for generating the PredictionsParticipants are encouraged (but not required) to make source code used to generate the Predictions available under an Approved OSS License Full terms can be found on ProjectAria.com/challenges/terms. ","version":"Next","tagName":"h2"},{"title":"How to participate​","type":1,"pageTitle":"ADT Object Detection 2023-4 Challenges","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/adt_challenges#how-to-participate","content":"Follow the detailed instructions at Eval.ai. ","version":"Next","tagName":"h2"},{"title":"Advanced Tutorials","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/advanced_tutorials","content":"","keywords":"","version":"Next"},{"title":"Multi-person Synchronization​","type":1,"pageTitle":"Advanced Tutorials","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/advanced_tutorials#multi-person-synchronization","content":"This tutorial will walk you through the steps to get synchronized ground truth data in a multi-person sequence in the Aria Digital Twin (ADT) dataset. ","version":"Next","tagName":"h2"},{"title":"Further resources​","type":1,"pageTitle":"Advanced Tutorials","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/advanced_tutorials#further-resources","content":"Timestamps in Aria VRS Files ","version":"Next","tagName":"h3"},{"title":"How does time synchronization work in ADT​","type":1,"pageTitle":"Advanced Tutorials","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/advanced_tutorials#how-does-time-synchronization-work-in-adt","content":"In a single-person ADT sequence, all ground truth data is stored in the device capture time of the Project Aria device used by the person. For a multi-person ADT sequence, we store ground truth files in two separate folders, each representing a person's Aria recording. Each subsequence is self-contained such that all ground truth data is also stored in the device capture time domain of the associated Aria device. To synchronize the two Aria devices, we store a mapping between timecode timestamps and device capture timestamps in each Aria data. ","version":"Next","tagName":"h3"},{"title":"Step 0 : Install project_aria_tools package and create venv if not already done​","type":1,"pageTitle":"Advanced Tutorials","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/advanced_tutorials#step-0--install-project_aria_tools-package-and-create-venv-if-not-already-done","content":"Follow Step 0 to Step 3 in Getting Started. ","version":"Next","tagName":"h3"},{"title":"Step 1 : Prepare download​","type":1,"pageTitle":"Advanced Tutorials","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/advanced_tutorials#step-1--prepare-download","content":"Follow Step 1 to Step 3 in Download the sample ADT sequence ","version":"Next","tagName":"h3"},{"title":"Step 2 : Download sample multi-person sequence​","type":1,"pageTitle":"Advanced Tutorials","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/advanced_tutorials#step-2--download-sample-multi-person-sequence","content":"From your python virtual environment, run: adt_benchmark_dataset_downloader -c $HOME/Documents/projectaria_tools_adt_data/aria_digital_twin_dataset_download_urls.json \\ -o $HOME/Documents/projectaria_tools_adt_data/ -l Apartment_release_multiskeleton_party_seq114 \\ -d 0  ","version":"Next","tagName":"h3"},{"title":"Step 3 : Run the tutorial notebook​","type":1,"pageTitle":"Advanced Tutorials","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/advanced_tutorials#step-3--run-the-tutorial-notebook","content":"From your python virtual environment, run: cd $HOME/Documents/projectaria_sandbox jupyter notebook projectaria_tools/projects/AriaDigitalTwinDatasetTools/examples/adt_multiperson_tutorial.ipynb  ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"Advanced Tutorials","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/advanced_tutorials#troubleshooting","content":"Go to Data Utilities Troubleshooting if you experience issues using this guide. ","version":"Next","tagName":"h2"},{"title":"Getting Started With ADT","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/getting_started","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Getting Started With ADT","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/getting_started#overview","content":"This section provides a step-by-step guide to run the Aria Digital Twin (ADT) quickstart tutorial in a Jupyter notebook. This notebook provides a walkthrough of: Loading an ADT sequenceAccessing and visualizing all ADT ground-truth data: 6DoF object poses2D object bounding boxSegmentation imagesDepth imagesSkeleton and synthetic renderingEye gaze An example of undistorting ADT ground-truth data We also have an Advanced Tutorial that will walk you through getting synchronized ground truth data in a multi-person sequence. ","version":"Next","tagName":"h2"},{"title":"Run Jupyter Notebook on Google Colab​","type":1,"pageTitle":"Getting Started With ADT","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/getting_started#run-jupyter-notebook-on-google-colab","content":"Use the following link to run the Python notebook in an installation free playground: Aria Digital Twin (ADT) ","version":"Next","tagName":"h2"},{"title":"Running the Jupyter Notebook locally​","type":1,"pageTitle":"Getting Started With ADT","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/getting_started#running-the-jupyter-notebook-locally","content":"","version":"Next","tagName":"h2"},{"title":"Step 0 : Check system requirements and download codebase​","type":1,"pageTitle":"Getting Started With ADT","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/getting_started#step-0--check-system-requirements-and-download-codebase","content":"Ensure your system is supported and then download projectaria_tools codebase from the github ","version":"Next","tagName":"h3"},{"title":"Step 1 : Install Python​","type":1,"pageTitle":"Getting Started With ADT","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/getting_started#step-1--install-python","content":"If you have already installed projectaria-tools using Python Package Installation, you can skip to Step 4. The ADT Python code is part of the main projectaria-tools package. Jupyter notebook error If you have problems using Jupyter examples, please upgrade python3 to the latest version. ","version":"Next","tagName":"h3"},{"title":"Step 2 : Create a virtual environment​","type":1,"pageTitle":"Getting Started With ADT","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/getting_started#step-2--create-a-virtual-environment","content":"rm -rf $HOME/projectaria_tools_python_env python3 -m venv $HOME/projectaria_tools_python_env source $HOME/projectaria_tools_python_env/bin/activate  ","version":"Next","tagName":"h3"},{"title":"Step 3 : Install projectaria_tools from pypi​","type":1,"pageTitle":"Getting Started With ADT","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/getting_started#step-3--install-projectaria_tools-from-pypi","content":"pip3 install --upgrade pip pip3 install projectaria-tools'[all]'  ","version":"Next","tagName":"h3"},{"title":"Step 4 : Download Sample Sequence:​","type":1,"pageTitle":"Getting Started With ADT","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/getting_started#step-4--download-sample-sequence","content":"Download the sample ADT sequence by following steps 0 to 4 inHow to Download the ADT Dataset. ","version":"Next","tagName":"h3"},{"title":"Step 5 : Run Tutorial​","type":1,"pageTitle":"Getting Started With ADT","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/getting_started#step-5--run-tutorial","content":"From your projectaria_tools python virtual environment, run: cd $HOME/Documents/projectaria_sandbox jupyter notebook projectaria_tools/projects/AriaDigitalTwinDatasetTools/examples/adt_quickstart_tutorial.ipynb  ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"Getting Started With ADT","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/getting_started#troubleshooting","content":"Go to Data Utilities Troubleshooting if you have issues implementing this guide. ","version":"Next","tagName":"h2"},{"title":"Other useful links​","type":1,"pageTitle":"Getting Started With ADT","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/getting_started#other-useful-links","content":"Use projectaria_tools with CMakeTimestamps in Aria VRS Files ","version":"Next","tagName":"h2"},{"title":"How to Download the ADT Dataset","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"How to Download the ADT Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download#overview","content":"This page covers how to download the sample Aria Digital Twin (ADT) sequence as well as how to download specific sequences and types of data. Follow the instructions to download the sample datasets and from there you'll be able to use the CLI to download more data. The sample dataset is a single-user dataset with body pose in the Apartment. This is a pretty representative example that should give you an idea of the dataset. By downloading the datasets you agree that you have read and accepted the terms of the Aria Digital Twin Dataset License Agreement. ","version":"Next","tagName":"h2"},{"title":"Download the sample Aria Digital Twin (ADT) sequence​","type":1,"pageTitle":"How to Download the ADT Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download#download-the-sample-aria-digital-twin-adt-sequence","content":"","version":"Next","tagName":"h2"},{"title":"Step 0: install project_aria_tools package and create venv if not done before​","type":1,"pageTitle":"How to Download the ADT Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download#step-0-install-project_aria_tools-package-and-create-venv-if-not-done-before","content":"Follow Step 0 to Step 3 in Getting Started. ","version":"Next","tagName":"h3"},{"title":"Step 1 : Visit ADT website sign up.​","type":1,"pageTitle":"How to Download the ADT Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download#step-1--visit-adt-website-sign-up","content":"Scroll down to the bottom of the page. Enter you email and select Access the Datasets.  ","version":"Next","tagName":"h3"},{"title":"Step 2 : Download the download-links file​","type":1,"pageTitle":"How to Download the ADT Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download#step-2--download-the-download-links-file","content":"Once you've selected Access the Datasets you'll be taken back to the top of the ADT page. Scroll down the page to select Aria Digital Twin Download Links and download the file to the folder $HOME/Downloads.  The download-links file will expire in 14 days You can redownload the download links whenever they expire ","version":"Next","tagName":"h3"},{"title":"Step 3 : Set up a folder for ADT data​","type":1,"pageTitle":"How to Download the ADT Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download#step-3--set-up-a-folder-for-adt-data","content":"mkdir -p $HOME/Documents/projectaria_tools_adt_data mv $HOME/Downloads/aria_digital_twin_dataset_download_urls.json $HOME/Documents/projectaria_tools_adt_data/  ","version":"Next","tagName":"h3"},{"title":"Step 4 : Download the sample sequence (~500MB) via CLI:​","type":1,"pageTitle":"How to Download the ADT Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download#step-4--download-the-sample-sequence-500mb-via-cli","content":"From your Python virtual environment, run: adt_benchmark_dataset_downloader -c $HOME/Documents/projectaria_tools_adt_data/aria_digital_twin_dataset_download_urls.json \\ -o $HOME/Documents/projectaria_tools_adt_data/ \\ -d 0 1 2 3 -e  The sample dataset is a single-user dataset with body pose in the Apartment. This is a pretty representative example to give an idea of the dataset. For more information on the content in the other sequences, see the Data Content section below ","version":"Next","tagName":"h3"},{"title":"Download the Aria Digital Twin (ADT) benchmark dataset​","type":1,"pageTitle":"How to Download the ADT Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download#download-the-aria-digital-twin-adt-benchmark-dataset","content":"","version":"Next","tagName":"h2"},{"title":"Data size​","type":1,"pageTitle":"How to Download the ADT Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download#data-size","content":"Aria Digital Twin dataset consists of 217 sequences in total. The total size of the dataset is about 3.5TB. The dataset is split into 4 data types that can be downloaded individually. The size of each data type is below. Data type\tWhat's included\tPer sequence size\tTotal size for all sequencesmain\tAria raw data, 2D bounding box, 3D object poses and bounding box, skeleton data, eye gaze data\t3 - 6 GB\t~700 GB segmentation\tInstance segmentation data\t2 - 4 GB\t~750 GB depth\tDepth map data\t4 - 8 GB\t~1.5 TB synthetic\tSynthetic rendering data\t2 - 4 GB\t500 GB ","version":"Next","tagName":"h3"},{"title":"Download via CLI​","type":1,"pageTitle":"How to Download the ADT Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download#download-via-cli","content":"Follow the ADT Getting Started Guide to download the example data. This section will introduce how to download the dataset using the adt_benchmark_dataset_downloader. Resumable download​ The adt_benchmark_dataset_downloader checks the previous download status of the sequences in the --output_folder. If the downloading breaks in the middle, relaunch the CLI and it will continue the downloading. Detailed arguments​ Arguments\tType\tDescription--cdn_file\tstr\tThe download-urls file you downloaded from the ADT website page after signing up --output_folder\tstr\tA local path where the downloaded files and metadata will be stored --metadata_only\tflag\tOnly download the metadata --data_types\tlist of int\t0→main, 1→segmentation, 2→depth, 3→synthetic --example_only\tflag\tOnly download example data --overwrite\tflag\tDisable resumable download. Force download and overwrite existing data --sequence_names\tlist of str\tlist of sequence names. If not specified, download all sequences ","version":"Next","tagName":"h3"},{"title":"Download Examples​","type":1,"pageTitle":"How to Download the ADT Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download#download-examples","content":"Note that all these commands must be run from your Python virtual environment that has the projectaria-tools package and dependencies installed. Download metadata for ADT datasets​ adt_benchmark_dataset_downloader --cdn_file ${PATH_TO_YOUR_CDN_FILE} --output_folder ${OUTPUT_FOLDER_PATH} --metadata_only  Download main data for all sequences​ adt_benchmark_dataset_downloader --cdn_file ${PATH_TO_YOUR_CDN_FILE} --output_folder ${OUTPUT_FOLDER_PATH} --data_types 0  Download all data for all sequences​ adt_benchmark_dataset_downloader --cdn_file ${PATH_TO_YOUR_CDN_FILE} --output_folder ${OUTPUT_FOLDER_PATH} --data_types 0 1 2 3  Download main data for 2 specific sequences​ adt_benchmark_dataset_downloader --cdn_file ${PATH_TO_YOUR_CDN_FILE} --output_folder ${OUTPUT_FOLDER_PATH} --data_types 0 --sequence_names Lite_release_recognition_BambooPlate_seq031 Lite_release_recognition_BirdHouseToy_seq030  Download main data for all sequences and overwrite​ adt_benchmark_dataset_downloader --cdn_file ${PATH_TO_YOUR_CDN_FILE} --output_folder ${OUTPUT_FOLDER_PATH} --data_types 0 --overwrite  ","version":"Next","tagName":"h3"},{"title":"Select specific sequences​","type":1,"pageTitle":"How to Download the ADT Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download#select-specific-sequences","content":"The dataset metadata JSON “aria_digital_twin_benchmark_metadata.json”, which can be downloaded using adt_benchmark_dataset_downloader, contains metadata for each ADT sequence. The metadata fields of each sequence are: Field Name\tDescriptionscenes\tThe scene that a sequence is captured at, Apartment or LiteOffice, in the current ADT release, there will only be one element in the list is_multi_person\tWhether the sequence is a single person activity or a multiperson activity num_skeleton\tnumber of persons whose body skeleton is tracked aria_digital_twin_dataset_searcher.py is an example Python script for filtering sequences via different criteria. ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"How to Download the ADT Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download#troubleshooting","content":"Check the troubleshooting if you are having issues in this guide. ","version":"Next","tagName":"h2"},{"title":"ADT Data Format","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format","content":"","keywords":"","version":"Next"},{"title":"Sequence and Subsequence​","type":1,"pageTitle":"ADT Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#sequence-and-subsequence","content":"A sequence in Aria Digital Twin (ADT) dataset represents a data recording in a scene. It can be either a multi-person activity, which may include multiple Aria devices recording at the same time, or a single-person activity, which includes only one Aria device. Inside a sequence, we use subsequences to represent the recording of each Aria device and its associated ground truth data. SLAM and Eyegaze data that has been generated by Project Aria Machine Perception Services (MPS) (rather than ADT ground truth) is stored in the MPS folder of each subsequence. So far, an ADT sequence contains at most 2 subsequences. Each sequence has the following folder structure: |SequenceName| ├──Subsequence1Name| ├──Subsequence2Name| [Optional] # Omitted if a sequence is a single person activity ├──metadata.json  The metadata.json file contains the high-level sequence information such as the included Aria's serial number, the scene name, etc, which can be loaded and queried by AriaDigitalTwinDataPathProvider. gt-metadata.json name change Prior to v1.1 of the dataset metadata.json was called gt-metadata.json. ","version":"Next","tagName":"h2"},{"title":"Subsequence structure​","type":1,"pageTitle":"ADT Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#subsequence-structure","content":"|Subsequence1Name| ├──video.vrs # Aria recording data ├──instances.json # metadata of all instances in a sequence. An instance can be an object or a skeleton ├──aria_trajectory.csv # 6DoF Aria trajectory ├──2d_bounding_box.csv # 2D bounding box data for instances in three Aria sensors: RGB camera, left SLAM camera, right SLAM camera ├──3d_bounding_box.csv # 3D AABB of each object ├──scene_objects.csv # 6 DoF poses of objects ├──eyegaze.csv # Eye gaze ├──synthetic_video.vrs # Synthetic rendering of video.vrs ├──depth_images.vrs # Depth images of video.vrs ├──segmentations.vrs # Instance segmentations of video.vrs ├──skeleton_aria_association.json [optional] # File showing association between Aria devices and skeletons, if they exist. Omitted if a sequence does not have skeleton ground truth. ├──Skeleton_*.json [optional] # Body skeleton data. * is the skeleton name. Omitted if a sequence does not have skeleton ground truth ├──2d_bounding_box_with_skeleton.csv [optional] # 2D bounding box data with body mesh occlusions. Omitted if a sequence does not have skeleton ground truth ├──depth_images_with_skeleton.vrs [optional] # Depth images with body mesh occlusions. Omitted if a sequence does not have skeleton ground truth ├──segmentations_with_skeleton.vrs [optional] # Segmentations with body mesh occlusions. Omitted if a sequence does not have skeleton ground truth ├──MPS # Go to Data Formats/MPS Output for more information about the data in this directory ├── eye_gaze ├── general_eye_gaze.csv ├── summary.json ├── slam ├── alignment_results.json # Alignment results between the MPS closed loop trajectory and the ADT GT trajectory ├── closed_loop_trajectory.csv ├── open_loop_trajectory.csv ├── online_calibration.csv ├── semidense_observations.csv.gz ├── semidense_points.csv.gz ├── summary.json  SkeletonMetaData.json name change Prior to v1.1 of the dataset, skeleton_aria_association.json was called SkeletonMetaData.json. ","version":"Next","tagName":"h2"},{"title":"Timestamps Mapping Data​","type":1,"pageTitle":"ADT Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#timestamps-mapping-data","content":"Project Aria glasses recording concurrently in the same location leverage SMPTE timecode to receive a synchronized time clock with sub-millisecond accuracy. The mapping between device time and timecode clock for each sequence is stored in the VRS file as a Time Domain Mapping Class. Go to Timestamps in Aria VRS Files for more information about how Aria sensor data is timestamped. Go to Advanced Tutorials for how to get synchronized ground truth data in a multi-person sequence. ","version":"Next","tagName":"h3"},{"title":"Ground Truth Data​","type":1,"pageTitle":"ADT Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#ground-truth-data","content":"You can use the AriaDigitalTwinDataPathProvider to load a sequence and select a subsequence.AriaDigitalTwinDataPathProvider will manage all the ground truth files in a subsequence folder (not the MPS files). ","version":"Next","tagName":"h2"},{"title":"Aligning Ground Truth and MPS Data​","type":1,"pageTitle":"ADT Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#aligning-ground-truth-and-mps-data","content":"The alignment_results.json file in mps/slam directory contains the alignment results between the MPS closed loop trajectory and the ADT GT trajectory. The alignment results have already been applied to the closed loop trajectory and the semidense pointcloud to convert from the SLAM frame to the ADT frame, ensuring all ADT data is expressed in the same coordinate frame for all sequences. ","version":"Next","tagName":"h3"},{"title":"Skeleton Data and Availability​","type":1,"pageTitle":"ADT Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#skeleton-data-and-availability","content":"Not all ADT sequences have skeleton tracking. For those sequences with skeleton tracking enabled, we use the marker measurements from the bodysuit to generate a 3D mesh estimate of the wearer which is then used in our ground truth generation pipeline to calculate 2D bounding boxes, segmentation images and depth images. In these cases, ADT provides two sets of ground truth data: one with skeleton occlusion, one without. segmentations.vrs vs. segmentations_with_skeleton.vrsdepth_images.vrs vs. depth_images_with_skeleton.vrs'2d_bounding_box.csv' vs. '2d_bounding_box_with_skeleton.csv' You can use AriaDigitalTwinDataPathsProvider to switch between these two sets. ","version":"Next","tagName":"h2"},{"title":"Ground Truth Data Format​","type":1,"pageTitle":"ADT Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#ground-truth-data-format","content":"Our data loader loads all this data into a single class with useful tools for accessing data. For more information on the data classes returned by the loader, go to the Data Loader page. ","version":"Next","tagName":"h2"},{"title":"2d_bounding_box.csv or 2d_bounding_box_with_skeleton.csv​","type":1,"pageTitle":"ADT Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#2d_bounding_boxcsv-or-2d_bounding_box_with_skeletoncsv","content":"Column\tType\tDescriptionstream_id\tstring\tcamera stream id associated with the bounding box image object_uid\tuint64_t\tid of the instance (object or skeleton) timestamp[ns]\tint64_t\ttimestamp of the image in nanoseconds x_min[pixel]\tint\tminimum dimension in the x axis x_max[pixel]\tint\tmaximum dimension in the x axis y_min[pixel]\tint\tminimum dimension in the y axis y_max[pixel]\tint\tmaximum dimension in the y axis visibility_ratio[%]\tdouble\tpercentage of the object that is visible (0: not visible, 1: fully visible) ","version":"Next","tagName":"h3"},{"title":"3d_bounding_box.csv​","type":1,"pageTitle":"ADT Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#3d_bounding_boxcsv","content":"Column\tType\tDescriptionobject_uid\tuint64_t\tid of the instance (object or skeleton) timestamp[ns]\tint64_t\ttimestamp of the image in nanoseconds. -1 means the instance is static p_local_obj_xmin[m]\tdouble\tminimum dimension in the x axis (in meters) of the bounding box p_local_obj_xmax[m]\tdouble\tmaximum dimension in the x axis (in meters) of the bounding box p_local_obj_ymin[m]\tdouble\tminimum dimension in the y axis (in meters) of the bounding box p_local_obj_ymax[m]\tdouble\tmaximum dimension in the y axis (in meters) of the bounding box p_local_obj_zmin[m]\tdouble\tminimum dimension in the z axis (in meters) of the bounding box p_local_obj_zmax[m]\tdouble\tmaximum dimension in the z axis (in meters) of the bounding box ","version":"Next","tagName":"h3"},{"title":"aria_trajectory.csv​","type":1,"pageTitle":"ADT Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#aria_trajectorycsv","content":"ADT uses the same trajectory format as closed loop trajectory in MPS. While the data structure is the same, the file is generated by the ADT ground truth system, not by MPS. ","version":"Next","tagName":"h3"},{"title":"eyegaze.csv​","type":1,"pageTitle":"ADT Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#eyegazecsv","content":"ADT uses the same eye gaze format as MPS. Unlike MPS outputs, the ground truth eyegaze.csv contains depth mapping estimated by the ADT ground truth system. ","version":"Next","tagName":"h3"},{"title":"scene_objects.csv​","type":1,"pageTitle":"ADT Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#scene_objectscsv","content":"Column\tType\tDescriptionobject_uid\tuint64_t\tid of the instance (object or skeleton) timestamp[ns]\tint64_t\ttimestamp of the image in nanoseconds. -1 means the instance is static t_wo_x[m]\tdouble\tx translation from object frame to world (scene) frame (in meters) t_wo_y[m]\tdouble\ty translation from object frame to world (scene) frame (in meters) t_wo_z[m]\tdouble\tz translation from object frame to world (scene) frame (in meters) q_wo_w\tdouble\tw component of quaternion from object frame to world (scene) frame q_wo_x\tdouble\tx component of quaternion from object frame to world (scene) frame q_wo_y\tdouble\ty component of quaternion from object frame to world (scene) frame q_wo_z\tdouble\tz component of quaternion from object frame to world (scene) frame ","version":"Next","tagName":"h3"},{"title":"instances.json​","type":1,"pageTitle":"ADT Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#instancesjson","content":"{ &quot;IID1&quot;: { &quot;instance_id&quot;: IID1, &quot;instance_name&quot;: &quot;XXXX&quot;, &quot;prototype_name&quot;: &quot;XXXX&quot;, &quot;category&quot;: &quot;XXXX&quot;, &quot;category_uid&quot;: XXXX, &quot;motion_type&quot;: &quot;static/dynamic&quot;, &quot;instance_type&quot;: &quot;object/human&quot;, &quot;rigidity&quot;: &quot;rigid/deformable&quot;, &quot;rotational_symmetry&quot;: { &quot;is_annotated&quot;: true/false }, &quot;canonical_pose&quot;: { &quot;up_vector&quot;: [ x, y, z ], &quot;front_vector&quot;: [ x, y, z ] } }, ... }  ","version":"Next","tagName":"h3"},{"title":"Skeleton_T.json or Skeleton_C.json​","type":1,"pageTitle":"ADT Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#skeleton_tjson-or-skeleton_cjson","content":"{ &quot;dt_optitrack_minus_device_ns&quot;: { &quot;1WM103600M1292&quot;: XXXXX }, &quot;frames&quot;: [ { &quot;markers&quot;: [ [ mx1 my1 mz1 ], ... ], &quot;joints&quot;: [ [ jx1 jy1 jz1 ], ... ], &quot;timestamp_ns&quot;: tsns1 }, ... ] }  ","version":"Next","tagName":"h3"},{"title":"skeleton_aria_association.json​","type":1,"pageTitle":"ADT Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#skeleton_aria_associationjson","content":"This file shows the skeleton info including name, Id, and associated Aria device for each human in the sequence. Because it's possible to have a person wearing a bodysuit that is not wearing an Aria device, it's possible to have a skeleton with no associated AriaDeviceSerial. It's also possible to have an Aria wearer with no bodysuit, which means there may be an empty skeleton Id and a name associated with an Aria device. { &quot;SkeletonMetadata&quot;: [ { &quot;AssociatedDeviceSerial&quot;: &quot;AriaSerial1/NONE&quot;, &quot;SkeletonId&quot;: ID1, &quot;SkeletonName&quot;: &quot;SkeletonName1/NONE&quot; }, ... ] }  ","version":"Next","tagName":"h3"},{"title":"video.vrs​","type":1,"pageTitle":"ADT Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#videovrs","content":"video.vrs contains the raw sensor recording from the Aria device. Aria Hardware Specifications shows the sensors used to make recordings Images were all recorded at 30 fps ","version":"Next","tagName":"h3"},{"title":"depth_images.vrs​","type":1,"pageTitle":"ADT Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#depth_imagesvrs","content":"depth_images.vrs1 contains 3 streams of images corresponding to the exact streams in video.vrs. Each depth image is the same size as their corresponding raw image, where the pixel contents are integers expressing the depth in the camera’s Z-axis, in units of mm. This should not to be confused with ASE depth images, which describe the depth along each pixel ray Depth data is calculated using ADT’s ground truth system ","version":"Next","tagName":"h3"},{"title":"segmentations.vrs​","type":1,"pageTitle":"ADT Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#segmentationsvrs","content":"segmentations.vrs contains 3 streams of images corresponding to the exact streams in video.vrs. Each segmentation image is the same size as their corresponding raw image, where the pixel contents are integers expressing the Instance Id that was observed by that pixelSegmentation data is calculated using ADT’s ground truth system ","version":"Next","tagName":"h3"},{"title":"Data Loader","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_loader","content":"","keywords":"","version":"Next"},{"title":"AriaDigitalTwinDataPathsProvider​","type":1,"pageTitle":"Data Loader","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_loader#ariadigitaltwindatapathsprovider","content":"The main goal of this loader is to give the user an easy way to load an ADT sequence and its metadata, to select a specific subsequence, and to select specific annotations to load (e.g., with or without skeleton). AriaDigitalTwinDataPathsProvider manages all ground truth file paths that can be used to load ground truth data in AriaDigitalTwinDataProvider. The following shows an example code snippet which loads an ADT sequence and select a subsequence to be passed to the AriaDigitalTwinDataProvider. PythonC++ from projectaria_tools.projects.adt import AriaDigitalTwinDataPathsProvider # define the sequence path you want to load sequence_path = &quot;PATH/TO/An_ADT_sequence&quot; # create path provider paths_provider = AriaDigitalTwinDataPathsProvider(sequence_path) # list all subsequences for this sequence all_device_serials = paths_provider.get_device_serial_numbers() # print the Aria device serial number used in each subsequence for idx, device_serial in enumerate(all_device_serials): print(&quot;device number - &quot;, idx, &quot;: &quot;, device_serial) # load the set of ground truth data files without skeleton occlusion of the first Aria device data_paths_without_skeleton_occlusion = paths_provider.get_datapaths_by_device_num(0, False) # load the set of ground truth data files with skeleton occlusion of the first Aria device data_paths_with_skeleton_occlusion = paths_provider.get_datapaths_by_device_num(0, True)  ","version":"Next","tagName":"h2"},{"title":"AriaDigitalTwinDataProvider​","type":1,"pageTitle":"Data Loader","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_loader#ariadigitaltwindataprovider","content":"This is the core data loader that takes an instance of the AriaDigitalTwinDataPaths class (generated by the AriaDigitalTwinDataPathsProvider) and provides you will query functions to access all ADT data. The following shows an example snippet to load ground truth data with the AriaDigitalTwinDataProvider: PythonC++ from projectaria_tools.projects.adt import AriaDigitalTwinDataPathsProvider, AriaDigitalTwinDataProvider # define the sequence path you want to load sequence_path = &quot;PATH/TO/An_ADT_sequence&quot; # create path provider paths_provider = AriaDigitalTwinDataPathsProvider(sequence_path) # load the set of ground truth data files with skeleton occlusion of the first Aria device data_paths_with_skeleton_occlusion = paths_provider.get_datapaths_by_device_num(0, True) # create data provider gt_provider = AriaDigitalTwinDataProvider(data_paths_with_skeleton_occlusion)  ","version":"Next","tagName":"h2"},{"title":"Skip Data loading​","type":1,"pageTitle":"Data Loader","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_loader#skip-data-loading","content":"All data loaders are designed to allow the user to skip the loading of specific data types. You can do this by setting the path to an empty string in your AriaDigitalTwinDataPathsinstance prior to constructing the AriaDigitalTwinDataProvider. ","version":"Next","tagName":"h3"},{"title":"Check Data Existence​","type":1,"pageTitle":"Data Loader","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_loader#check-data-existence","content":"Since we allow users to skip specific data type loading as explained above, we also provide functions in in AriaDigitalTwinDataProviderto check if data exists by calling their appropriate functions before calling the corresponding getter functions. E.g. hasObject3dBoundingboxes() ","version":"Next","tagName":"h2"},{"title":"Ground Truth Data Getter Functions​","type":1,"pageTitle":"Data Loader","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_loader#ground-truth-data-getter-functions","content":"For a full example of the python getters, please refer to the notebook in the Getting Started. For a full example of the C++ getters, please refer to the visualizer example. Getting Instance Information​ In ADT, we define an instance to be either a human or an object. The attributes of an instance is defined in class InstanceInfo in AriaDigitalTwinDataTypes. We use instanceType to differentiate a human and an object. Time Query Options​ You may have also noticed the timeQueryOptions parameter in the above getter functions. Same as dataprovider, all getter functions for timestamped data allow you to specify how to query the timestamps. The options are defined in TimeTypes Accessing Timestamped Data​ All timestamped data query APIs return a templated DataWithDt class. For example, BoundingBox2dDataWithDt defined in AriaDigitalTwinDataTypes as: using BoundingBox2dDataWithDt = DataWithDt&lt;TypeBoundingBox2dMap&gt;;  The goal of wrapping all data in a DataWithDt class is to ensure all returned timestamped data has two fields: isValid, and dtNs. Where isValid defined whether or not the returned data is valid, since all timestamp queries may be invalid times, and dtNs to ensure the user always knows the time difference between the returned data and the query time. Interpolation Function​ We provide interpolation functions for 6DoF Aria poses and Object 3d bounding boxes called &quot;getInterpolatedAria3dPoseAtTimestampNs&quot; and &quot;getInterpolatedObject3dBoundingBoxesAtTimestampNs&quot; in AriaDigitalTwinDataProvider ","version":"Next","tagName":"h3"},{"title":"Time Synchronization Between Subsequences​","type":1,"pageTitle":"Data Loader","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_loader#time-synchronization-between-subsequences","content":"The Advanced Tutorial shows how to synchronize subsequences in an ADT sequence. Further resources: Timestamps in Aria VRS Files - how Project Aria timestamp data is formatted in VRS for single and multiple devicesProject Aria Device Timestamping - how the hardware is configuredTemporal Alignment of Aria Sensor Data - how the data is temporally aligned and how to finely align IMU, barometer and magnetometer data ","version":"Next","tagName":"h3"},{"title":"Skeleton Data​","type":1,"pageTitle":"Data Loader","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_loader#skeleton-data","content":"Separate from the 2D skeleton data, we also have skeleton frames as measured by Optitrack. This data can be accessed directly from the AriaDigitalTwinDataProvider, or using the AriaDigitalTwinSkeletonProvider which can be extracted from AriaDigitalTwinDataProvider. Motive, the software that runs the Optitrack system, generates two types of skeleton data: Skeleton Markers: a set of 3D marker positions of all visible markers that are attached to the bodysuit. If markers are occluded, they are set to [0,0,0]. We provide a helper function to get the labels: getMarkerLabels() in AriaDigitalTwinSkeletonProvider. For more information see motive’s Biomech57 template Skeleton Joints: a set of estimated 3D joint positions. We provide a list of these joint positions for each timestamp, as well as the joint labels getJointConnections(), and connections getJointLabels() in in AriaDigitalTwinSkeletonProvider Note that both the markers and the joints are provided in the ADT Scene frame to be consistent with all other ground truth data. ","version":"Next","tagName":"h3"},{"title":"Aria Everyday Activities Dataset","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Aria Everyday Activities Dataset","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset#overview","content":"The Aria Everyday Activities (AEA) dataset provides sequences collected using Project Aria glasses in a variety of egocentric scenarios, including: cooking, exercising, playing games and spending time with friends. The goal of AEA is to provide researchers with data to engage in solving problems related to the challenges of always-on egocentric vision. AEA contains multiple activity sequences where 1-2 users wearing Project Aria glasses participate in scenarios to capture time synchronized data in a shared world location. For more information on the activities, see: Activities: details about the scenarios and where specific activities are in the recording sequenceRecording Scripts: more details about each scenario Aria Everyday Activities Dataset was first released as part of the Aria Pilot Dataset (Project Aria’s first open dataset). It has now been improved with a new data format, new tooling that enables it to be used with Project Aria Tools, and additional machine perception data. The data was recorded using Recording Profile 9. ","version":"Next","tagName":"h2"},{"title":"About the data​","type":1,"pageTitle":"Aria Everyday Activities Dataset","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset#about-the-data","content":"In addition to providing raw sensor data from Project Aria glasses, this dataset also contains annotated speech to text data, and results from our Machine Perception Services that provide additional context to the spatial-temporal reference frames. We provide: Per-frame eye tracking Generalized Eye Gaze data Accurate 3D trajectories of users across multiple everyday activities in the same location Trajectory and Semi-Dense Point Cloud data Timestamped with a shared clock to allow synchronization of data from multiple concurrent recordingsLocation information expressed in a shared global coordinate frame for all recordings collected in the same physical locationOnline calibration information for the cameras and IMUsSpeech-to-text annotation The dataset contains: 143 recordings for Everyday ActivitiesRecording in 5 locations, with 53 sequences of 2 users simultaneous recordingOver 1 million imagesOver 7.5 accumulated hours  Figure 1: Shared 3D Global Trajectories for Multi-User Activities in the Same Location Documentation The AEA section of this wiki covers: Getting Started With the AEA DatasetHow to Download the AEA DatasetAEA Data FormatAEA VisualizerAEA Everyday ActivitiesAria Wearer Data Scripts ","version":"Next","tagName":"h2"},{"title":"ADT Visualizers","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/visualizers","content":"","keywords":"","version":"Next"},{"title":"Python Visualizer​","type":1,"pageTitle":"ADT Visualizers","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/visualizers#python-visualizer","content":"viewer_projects_adt displays an interactive view of an ADT sequence with Rerun. It allows to see all data in 3D context. viewer_projects_adt --sequence_path Apartment_release_multiskeleton_party_seq104   tip The timeline enables you to see when a given object is being moved The 3D world view is clickable, you can quickly select an object and see its instance name You can quickly see where an object has been moved all along the sequence using the “Visible Time Range” feature in the RIGHT panel.  ","version":"Next","tagName":"h2"},{"title":"C++ Visualizer​","type":1,"pageTitle":"ADT Visualizers","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/visualizers#c-visualizer","content":"AriaDigitalTwinViewer is a C++ binary written to visualize ADT data with toggles for each ground truth data type and a slider bar for frame selection. The image below shows an example screenshot of the viewer.  ","version":"Next","tagName":"h2"},{"title":"Step 1 : Download Sample Sequence:​","type":1,"pageTitle":"ADT Visualizers","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/visualizers#step-1--download-sample-sequence","content":"Download the sample Aria Digital Twin (ADT) sequencefollow this guide. ","version":"Next","tagName":"h3"},{"title":"Step 2 : Build projectaria_tools C++ libraries​","type":1,"pageTitle":"ADT Visualizers","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/visualizers#step-2--build-projectaria_tools-c-libraries","content":"Follow the entire C++ installation to build projectaria_tools C++ libraries with visualization. ","version":"Next","tagName":"h3"},{"title":"Step 3 : Build AriaDigitalTwinViewer​","type":1,"pageTitle":"ADT Visualizers","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/visualizers#step-3--build-ariadigitaltwinviewer","content":"cd $HOME/Documents/projectaria_sandbox/build cmake ../projectaria_tools -DPROJECTARIA_TOOLS_BUILD_PROJECTS=ON -DPROJECTARIA_TOOLS_BUILD_PROJECTS_ADT=ON make -j2  ","version":"Next","tagName":"h3"},{"title":"Step 4 : Run AriaDigitalTwinViewer​","type":1,"pageTitle":"ADT Visualizers","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/visualizers#step-4--run-ariadigitaltwinviewer","content":"cd $HOME/Documents/projectaria_sandbox/build ./projects/AriaDigitalTwinDatasetTools/visualization/AriaDigitalTwinViewer \\ --sequence-path $HOME/Documents/projectaria_tools_adt_data/Apartment_release_golden_skeleton_seq100_10s_sample/ \\ --device-num 0 --skeleton-flag 0  ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"ADT Visualizers","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/visualizers#troubleshooting","content":"Go to Data Utilities Troubleshooting if you experience any issues. ","version":"Next","tagName":"h2"},{"title":"AEA Activities","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_activities","content":"","keywords":"","version":"Next"},{"title":"Recording Sessions​","type":1,"pageTitle":"AEA Activities","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_activities#recording-sessions","content":"We collected these scripted activities in five indoor location across the USA. Table 1: Every Day Activity Statistics and Scripts Used Per Recording Location Location\tScripts recorded in this location\tNumber of Wearers\tNumber of Recordings\tNumber of Frames\tNumber of RGB Frames\tTotal duration (hours)Location_1_indoor\tScripts 1-5\t1-2 Wearers\t29\t230,487\t115,235\t1.6 Location_2_indoor\tScripts 1-5\t1-2 Wearers\t43\t339,650\t169,824\t2.3 Location_3_indoor\tScripts 1-5\t1-2 Wearers\t38\t259,027\t129,514\t1.7 Location_4_indoor\tScripts 1-5\t1-2 Wearers\t19\t94,029\t47,015\t0.6 Location_5_indoor\tScripts 4-5\t1 Wearer\t14\t168,428\t84,214\t1.1 ","version":"Next","tagName":"h2"},{"title":"Activity Sequences​","type":1,"pageTitle":"AEA Activities","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_activities#activity-sequences","content":"Each script contains multiple daily activities recorded in a sequence. Use the list below to identify where scenarios are used in scripts. Numbers indicate how many wearers were in a sequence. Making coffee​ Script 2: Caffeination (2)Script 4: Perk up (1) Prepare snacks​ Script 1: Set out food and drink (1)Script 2: Grab the cream, sugar, and some snacks (2) Cooking​ Script 3: Cooking (2)Script 4: Breaking the Fast (1)Script 5: Check the provisions (1) Clean the place​ Script 1: Clean the place (1)Script 2: Clean up spilled coffee (2)Script 2: Time to go (2)Script 3: Clean up (2)Script 4: Cleaning up (1) Dining​ Script 3: Set the table (2)Script 4: Eating (1) Organization and laundry​ Script 1: Put up decorations (1)Script 5: Get home (1)Script 5: Wash clothes (1)Script 5: Straighten up (1)Script 5: Dry clothes (1)Script 5: Get and fold laundry (1) Reading, games and exercise​ Script 1: Get the blood pumping (1)Script 1: Watch a TV Show (1)Script 1: Video game break (1)Script 1: Read and wait (1)Script 4: Waking up (1)Script 4: Exercise (1)Script 4: Play console video game (1)Script 5: Catch up on email and social media (1) Touring the room​ Script 2: Tour of the house (2) Multi-person indoor activities​ Script 2: Guest arrives (2)Script 2: Play a board game (2)Script 2: Share videos (2)Script 3: Arriving home (2) Activities with indoor outdoor transitions​ Script 2: Guest arrives (2)Script 3: Arriving home (2)Script 5: Get home (1) ","version":"Next","tagName":"h2"},{"title":"AEA Data Format","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_data_format","content":"","keywords":"","version":"Next"},{"title":"SLAM output​","type":1,"pageTitle":"AEA Data Format","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_data_format#slam-output","content":"The SLAM outputs were created in a shared coordinate frame. The Multi-SLAM data format page contains more information about this output. Please note the file structure is slightly different (vrs_to_multi_slam.json is not necessary) compared to MPS CLI Multi-SLAM requests, as the shared coordinate frame is organized by location. ","version":"Next","tagName":"h2"},{"title":"Speech to Text annotation​","type":1,"pageTitle":"AEA Data Format","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_data_format#speech-to-text-annotation","content":"Speech to Text annotation provides text strings generated by Automatic Speech Recognition (ASR) with timestamps and confidence rating. The ASR annotation used an in-house proprietary system. Similar results can be acquired via open-source ASR solutions. Table 2: speech.csv Structure startTime_ns\tendTime_ns\twritten\tconfidence 54040\t55040\tI’m\t0.25608 72920\t73920\tlooking\t0.84339 ","version":"Next","tagName":"h2"},{"title":"Timestamps Mapping Data​","type":1,"pageTitle":"AEA Data Format","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_data_format#timestamps-mapping-data","content":"Project Aria glasses and multi-view devices operating in proximity to each other (&lt;100m) can leverage SMPTE timecode to receive a synchronized time clock with sub-millisecond accuracy. The mapping between device time and timecode clock for each sequence is stored in the VRS file as a Time Domain Mapping Class. Go to Timestamps in Aria VRS Files for more information about how Aria sensor data is timestamped. To translate the local timestamp of an arbitrary piece of data to the timecode time domain, you can interpolate between device timestamps in the time domain mapping data. An implementation of this mechanism is already provided in VrsDataProvider. To synchronize data from a secondary device, you can query that second VRS with this timecode time. This functionality is also already implemented in the VrsDataProvider class. ","version":"Next","tagName":"h2"},{"title":"How to Download the AEA Dataset","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_download_dataset","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"How to Download the AEA Dataset","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_download_dataset#overview","content":"This page covers how to download sample Aria Everyday Activities (AEA) sequences, as well as how to download specific sequences and types of data. Follow the instructions to download the sample sequence and from there you'll be able to use the CLI to download more data. By downloading the datasets you agree that you have read and accepted the terms of the Aria Everyday Activities Dataset License Agreement. ","version":"Next","tagName":"h2"},{"title":"Download the sample AEA sequence​","type":1,"pageTitle":"How to Download the AEA Dataset","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_download_dataset#download-the-sample-aea-sequence","content":"","version":"Next","tagName":"h2"},{"title":"Step 0: Install project_aria_tools package and create a virtual environment if not already done​","type":1,"pageTitle":"How to Download the AEA Dataset","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_download_dataset#step-0-install-project_aria_tools-package-and-create-a-virtual-environment-if-not-already-done","content":"Follow Step 0 to Step 3 in Getting Started. ","version":"Next","tagName":"h3"},{"title":"Step 1 : Visit the AEA website and sign up.​","type":1,"pageTitle":"How to Download the AEA Dataset","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_download_dataset#step-1--visit-the-aea-website-and-sign-up","content":"Scroll down to the bottom of the page. Enter your email and select Access the Datasets.  ","version":"Next","tagName":"h3"},{"title":"Step 2 : Download the download-links file​","type":1,"pageTitle":"How to Download the AEA Dataset","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_download_dataset#step-2--download-the-download-links-file","content":"Once you've selected Access the Datasets you'll be taken back to the top of the AEA page. Scroll down the page to select AEA Download Links and download the file to the folder $HOME/Downloads. The download-links file will expire in 14 days You can re-download the download-links whenever they expire ","version":"Next","tagName":"h3"},{"title":"Step 3 : Set up a folder for AEA data​","type":1,"pageTitle":"How to Download the AEA Dataset","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_download_dataset#step-3--set-up-a-folder-for-aea-data","content":"mkdir -p $HOME/Documents/projectaria_tools_aea_data mv $HOME/Downloads/aria_everyday_activities_dataset_download_urls.json $HOME/Documents/projectaria_tools_aea_data/  ","version":"Next","tagName":"h3"},{"title":"Step 4 : Download the sample sequence (~500MB) via CLI:​","type":1,"pageTitle":"How to Download the AEA Dataset","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_download_dataset#step-4--download-the-sample-sequence-500mb-via-cli","content":"From your Python virtual environment, run: aea_dataset_downloader -c $HOME/Documents/projectaria_tools_aea_data/aria_everyday_activities_dataset_download_urls.json \\ -o $HOME/Documents/projectaria_tools_aea_data/ \\ -d 0 1 2 3 -e  The sample sequence is representative of a typical single-user sequence, which gives you an idea of to expect from the dataset. ","version":"Next","tagName":"h3"},{"title":"Download the AEA dataset​","type":1,"pageTitle":"How to Download the AEA Dataset","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_download_dataset#download-the-aea-dataset","content":"","version":"Next","tagName":"h2"},{"title":"Data size​","type":1,"pageTitle":"How to Download the AEA Dataset","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_download_dataset#data-size","content":"The AEA dataset contains 143 sequences and the total size of the dataset is about 353GB. The dataset is split into main data and MPS outputs, eye gaze and Multi-SLAM (SLAM outputs created in shared coordinate frame) results. Go to Project Aria Machine Perception Services for more information about MPS data. The MPS data is also broken into chunks that can be included or excluded at download time. Data type\tWhat's included\tPer sequence size\tTotal size for all sequences main\tAria raw data, speech to text, metadata json\t2 - 4 GB\t~309 GB MPS eyegaze\tEyegaze, summary file\t&lt; 1 MB\t~31 MB MPS SLAM points\tSemi-dense points and observations\t200 - 500 MB\t~31 GB MPS SLAM trajectories\tOpen and closed loop trajectories\t100 - 200 MB\t12 GB MPS SLAM online calibration\tOnline calibrations &lt; 20 MB 1.2 GB ","version":"Next","tagName":"h3"},{"title":"Download via CLI​","type":1,"pageTitle":"How to Download the AEA Dataset","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_download_dataset#download-via-cli","content":"Follow the AEA Getting Started Guide to download the example data. This section will introduce how to download the dataset using the aea_dataset_downloader. Resumable download​ The aea_dataset_downloader checks the previous download status of the sequences in the --output_folder. If the downloading breaks in the middle, relaunch the CLI and it will continue the downloading. Detailed arguments​ Arguments\tType\tDescription --cdn_file\tstr\tThe download-urls file you downloaded from the AEA website page after signing up --output_folder\tstr\tA local path where the downloaded files and metadata will be stored --metadata_only\tflag\tOnly download the metadata file for the dataset --data_types\tlist of int\t0→main, 1→MPS eyegaze, 2→MPS trajectories, 3→MPS semidense pointclouds and observations, 3→MPS online calibrations --example_only\tflag\tOnly download example data --overwrite\tflag\tDisable resumable download. Force download and overwrite existing data --sequence_names\tlist of str\tlist of sequence names. If not specified, download all sequences ","version":"Next","tagName":"h3"},{"title":"Download Examples​","type":1,"pageTitle":"How to Download the AEA Dataset","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_download_dataset#download-examples","content":"tip All these commands must be run from your Python virtual environment that has the projectaria-tools package and dependencies installed. Download metadata for all datasets​ This will download the aria_everyday_activities_metadata.json, which contains metadata for each AEA sequence, including: location number, script number, sequence number, recording number, dataset version, dataset name, and list of concurrent recordings. You can use this data to select specific sequences to download. aea_dataset_downloader --cdn_file ${PATH_TO_YOUR_CDN_FILE} --output_folder ${OUTPUT_FOLDER_PATH} --metadata_only  Download main data for all sequences​ aea_dataset_downloader --cdn_file ${PATH_TO_YOUR_CDN_FILE} --output_folder ${OUTPUT_FOLDER_PATH} --data_types 0  Download all data for all sequences​ aea_dataset_downloader --cdn_file ${PATH_TO_YOUR_CDN_FILE} --output_folder ${OUTPUT_FOLDER_PATH} --data_types 0 1 2 3  Download main data for 2 specific sequences​ aea_dataset_downloader --cdn_file ${PATH_TO_YOUR_CDN_FILE} --output_folder ${OUTPUT_FOLDER_PATH} --data_types 0 --sequence_names loc1_script1_seq1_rec1 loc2_script1_seq1_rec1  Download main data for all sequences and overwrite​ aea_dataset_downloader --cdn_file ${PATH_TO_YOUR_CDN_FILE} --output_folder ${OUTPUT_FOLDER_PATH} --data_types 0 --overwrite  ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"How to Download the AEA Dataset","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_download_dataset#troubleshooting","content":"Go to troubleshooting if you experience issues using this guide. ","version":"Next","tagName":"h2"},{"title":"Getting Started With AEA","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_getting_started","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Getting Started With AEA","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_getting_started#overview","content":"This section provides a step-by-step guide to run the Aria Everyday Activities (AEA) quickstart tutorial in a Jupyter notebook. This notebook covers how to: Access raw sensor data (VRS files)Visualize Eye Gaze dataVisualize Speech dataLoad concurrent sequences from multiple Project Aria glasses in a shared space locationUse Timecode to get synchronized data between two devices ","version":"Next","tagName":"h2"},{"title":"Run Jupyter Notebook on Google Colab​","type":1,"pageTitle":"Getting Started With AEA","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_getting_started#run-jupyter-notebook-on-google-colab","content":"Use the following link to run the Python notebook in an installation free playground: Aria Everyday Activities (AEA) ","version":"Next","tagName":"h2"},{"title":"Running the Jupyter Notebook locally​","type":1,"pageTitle":"Getting Started With AEA","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_getting_started#running-the-jupyter-notebook-locally","content":"","version":"Next","tagName":"h2"},{"title":"Step 0 : Check system requirements and download codebase​","type":1,"pageTitle":"Getting Started With AEA","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_getting_started#step-0--check-system-requirements-and-download-codebase","content":"Ensure your system is supported and then download projectaria_tools codebase from the github ","version":"Next","tagName":"h3"},{"title":"Step 1 : Install Python​","type":1,"pageTitle":"Getting Started With AEA","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_getting_started#step-1--install-python","content":"If you have already installed projectaria-tools using Python Package Installation, you can skip to Step 4. The AEA Python code is part of the main projectaria-tools package. Jupyter notebook error If you have problems using Jupyter examples, please upgrade python3 to the latest version. ","version":"Next","tagName":"h3"},{"title":"Step 2 : Create a virtual environment​","type":1,"pageTitle":"Getting Started With AEA","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_getting_started#step-2--create-a-virtual-environment","content":"rm -rf $HOME/projectaria_tools_python_env python3 -m venv $HOME/projectaria_tools_python_env source $HOME/projectaria_tools_python_env/bin/activate  ","version":"Next","tagName":"h3"},{"title":"Step 3 : Install projectaria_tools from pypi​","type":1,"pageTitle":"Getting Started With AEA","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_getting_started#step-3--install-projectaria_tools-from-pypi","content":"pip3 install --upgrade pip pip3 install projectaria-tools'[all]'  ","version":"Next","tagName":"h3"},{"title":"Step 4 : Download Sample Sequence:​","type":1,"pageTitle":"Getting Started With AEA","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_getting_started#step-4--download-sample-sequence","content":"Download the sample AEA sequence by following steps 0 to 4 in Dataset Download. ","version":"Next","tagName":"h3"},{"title":"Step 5 : Run Tutorial​","type":1,"pageTitle":"Getting Started With AEA","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_getting_started#step-5--run-tutorial","content":"From your projectaria_tools python virtual environment, run: cd $HOME/Documents/projectaria_sandbox jupyter notebook projectaria_tools/projects/AriaEverydayActivities/examples/aea_quickstart_tutorial.ipynb  ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"Getting Started With AEA","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_getting_started#troubleshooting","content":"Go to Data Utilities Troubleshooting if you experience issues implementing this guide. ","version":"Next","tagName":"h2"},{"title":"Other useful links​","type":1,"pageTitle":"Getting Started With AEA","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_getting_started#other-useful-links","content":"Use projectaria_tools with CMakeTimestamps in Aria VRS Files ","version":"Next","tagName":"h2"},{"title":"Aria Wearer Data Scripts","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_scripts","content":"","keywords":"","version":"Next"},{"title":"Script 1: Lazy Morning Before the Party​","type":1,"pageTitle":"Aria Wearer Data Scripts","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_scripts#script-1-lazy-morning-before-the-party","content":"Activities: Communication, Media Consumption, Chores and Home Relaxation Objects: Clothes, Vacuum Cleaner, Game Console, Party Decorations, Cell Phone and Food Number of Participants: 1 ","version":"Next","tagName":"h2"},{"title":"Scenario description​","type":1,"pageTitle":"Aria Wearer Data Scripts","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_scripts#scenario-description","content":"Guests are coming over for a party later today. Finish up a relaxing morning before getting the house ready for guests. ","version":"Next","tagName":"h3"},{"title":"Sequence​","type":1,"pageTitle":"Aria Wearer Data Scripts","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_scripts#sequence","content":"Get the blood pumping. Healthy activity/exercise.Watch a TV show.Clean the place, straighten stuff up, vacuum and clean. May or may not be cleaning to music.Put up decorations and make the place look festive.Video game break, play a console video game.Set out food and drink.Read and wait for guests to arrive. ","version":"Next","tagName":"h3"},{"title":"Script 2: Catch Up and Have Some Fun​","type":1,"pageTitle":"Aria Wearer Data Scripts","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_scripts#script-2-catch-up-and-have-some-fun","content":"Activities: Social Small Group, Home Relaxation, Fun and Games Objects: Coffee, Coffee Maker, Board Game, Cream and Sugar Number of Participants: 2 (Host &amp; Guest) ","version":"Next","tagName":"h2"},{"title":"Scenario description​","type":1,"pageTitle":"Aria Wearer Data Scripts","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_scripts#scenario-description-1","content":"One friend is visiting another at their new apartment. They will catch up, share some food and coffee and play a board game. ","version":"Next","tagName":"h3"},{"title":"Sequence​","type":1,"pageTitle":"Aria Wearer Data Scripts","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_scripts#sequence-1","content":"Guest arrives at Host’s home - greetings and gift from Guest at the door.Tour of the house.Caffeination. Brew coffee Guest provided as a gift.Grab cream, sugar, and some snacks.Play a board game.Clean up spilled coffee.Share videos. Host and Guest show each other videos on their phones.Time to go. Guest helps Host tidy a little. Host walks guest to the door. Figure 1: Script 2, Catch Up and Have Some Fun ","version":"Next","tagName":"h3"},{"title":"Script 3: What Do You Want For Dinner?​","type":1,"pageTitle":"Aria Wearer Data Scripts","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_scripts#script-3--what-do-you-want-for-dinner","content":"Activities Social Small Group, Chores and Home Relaxation Objects Groceries, Bags, Pots and Food Number of Participants: 2 (Actor 1 &amp; Actor 2) ","version":"Next","tagName":"h2"},{"title":"Scenario description​","type":1,"pageTitle":"Aria Wearer Data Scripts","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_scripts#scenario-description-2","content":"Actor 1 is already at home. Actor 2 returns home with groceries needed for an easy meal. They make and eat dinner together and clean up afterwards. ","version":"Next","tagName":"h3"},{"title":"Sequence​","type":1,"pageTitle":"Aria Wearer Data Scripts","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_scripts#sequence-2","content":"Arriving home. Actor 1 entertains themselves (texting, reading, etc). Actor 2 comes home with groceries.Relax and talk about the day.Cooking. Discussing a plan for what to cook, gathering ingredients, prepare and cook meal.Set the table, serve dinner and eat.Clean up, put leftovers in containers, stack dishwasher. ","version":"Next","tagName":"h3"},{"title":"Script 4 : Easy Like Sunday Morning​","type":1,"pageTitle":"Aria Wearer Data Scripts","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_scripts#script-4--easy-like-sunday-morning","content":"Activities: Chores and Home Relaxation Objects: Bed, Coffee Maker, Yoga Mat and Pans Number of Participants: 1 ","version":"Next","tagName":"h2"},{"title":"Scenario description​","type":1,"pageTitle":"Aria Wearer Data Scripts","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_scripts#scenario-description-3","content":"Getting up and having a lazy Sunday morning (with a little bit of exercise). ","version":"Next","tagName":"h3"},{"title":"Sequence​","type":1,"pageTitle":"Aria Wearer Data Scripts","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_scripts#sequence-3","content":"Waking up. Get out of bed, brush teeth, stretch in the living room, go to the kitchen.Perk up a bit. Make and pour some coffee.Exercise. Put an exercise video on the TV and do a short workout.Breaking the fast. Take items out of the fridge and freezer and then cook breakfast.Eating. Watch a video on YouTube while eating breakfast.Cleaning up. Put dishes in the sink, brush teeth, do hair.Play console video game. ","version":"Next","tagName":"h3"},{"title":"Script 5: Get Home, Then Get Going​","type":1,"pageTitle":"Aria Wearer Data Scripts","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_scripts#script-5-get-home-then-get-going","content":"Activities: Chores and Home Relaxation Objects: Suitcase, Toiletries Bag, Laundry, Cell Phone, Pen and Paper Number of Participants: 1 ","version":"Next","tagName":"h2"},{"title":"Scenario Description​","type":1,"pageTitle":"Aria Wearer Data Scripts","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_scripts#scenario-description-4","content":"Returning home from a trip. Washing clothes, laying out clothes for a party and planning a shopping list. ","version":"Next","tagName":"h3"},{"title":"Sequence​","type":1,"pageTitle":"Aria Wearer Data Scripts","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_scripts#sequence-4","content":"Get home. Walk in the front door, lock it, dump bag’s contents on bed.Wash clothes. Go through clothes pile and put them in the wash.Straighten up, put away everything else from the trip.Dry clothes. Put clothes in the dryer.Catch up on email and social media.Get and fold laundry. Take clothes out of the dryer, fold them and put them away or hang them up in cupboard. Put aside clothes for the party.Check the provisions. Go through the kitchen and see what supplies you have. Create a shopping list: 10 minutes ","version":"Next","tagName":"h3"},{"title":"Aria Everyday Activities Visualization","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_visualizers","content":"","keywords":"","version":"Next"},{"title":"Python Visualizer​","type":1,"pageTitle":"Aria Everyday Activities Visualization","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_visualizers#python-visualizer","content":"viewer_projects_aea displays an interactive view of one or two synchronized sequences using Rerun. It also allows you to see all data in a 3D context. ","version":"Next","tagName":"h2"},{"title":"Visualization of a single sequence:​","type":1,"pageTitle":"Aria Everyday Activities Visualization","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_visualizers#visualization-of-a-single-sequence","content":"viewer_projects_aea --rotate-image --path loc1_script2_seq1_rec1  ","version":"Next","tagName":"h3"},{"title":"Visualization of two time synchronized recordings “locX_scriptY_seqZ_rec(1/2)”​","type":1,"pageTitle":"Aria Everyday Activities Visualization","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_visualizers#visualization-of-two-time-synchronized-recordings-locx_scripty_seqz_rec12","content":"The following command enables you to visualize recordings that were made in the same space and time. Select timecode_ns from the timeline to view the data via synchronized timestamps, and select device_time_ns to see device specific timestamps. Go to Timestamp Definitions to find out more about Project Aria data timestamps. viewer_projects_aea --rotate-image --path loc1_script2_seq6_rec1 loc1_script2_seq6_rec2   ","version":"Next","tagName":"h3"},{"title":"Visualization of aligned 3D data in a single location​","type":1,"pageTitle":"Aria Everyday Activities Visualization","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_visualizers#visualization-of-aligned-3d-data-in-a-single-location","content":"The MPS Viewer can be used to visualize all the recordings, across multiple sessions and users that were recorded in the same location. Go to Visualization of Multi-SLAM data for more details.  ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"Aria Everyday Activities Visualization","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_visualizers#troubleshooting","content":"Go to Data Utilities Troubleshooting if you experience any issues. ","version":"Next","tagName":"h2"},{"title":"Aria Synthetic Environments Dataset","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Aria Synthetic Environments Dataset","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset#overview","content":"Project Aria Tools provides Python and C++ APIs to access the Aria Synthetic Environments (ASE) dataset. ","version":"Next","tagName":"h2"},{"title":"About the data​","type":1,"pageTitle":"Aria Synthetic Environments Dataset","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset#about-the-data","content":"Aria Synthetic Environments (ASE) is a large scale dataset of 100K unique procedurally-generated scenes of interior layouts of apartments filled with 3D objects, and simulated with the sensor characteristics of Aria glasses. For each scene we have the rendering of a person walking around the synthetically generated rooms of the layout. These rooms vary from living rooms, bedrooms &amp; kitchens to bathrooms. In addition to the renders, each of these scenes come with semi-dense maps for the Aria walkthrough, which are aligned to the Ground Truth (GT) scene layout. This dataset was created to provide the wider research community with a dataset large enough to surface new challenges and research opportunities for first person object detection and tracking that had not been feasible. In the ASE: Scene Reconstruction Challenge, we invite researchers to train full scene structured language description models, drawing from the 100K annotated scenes, and then test their models on 1K test scenes provided in the challenge. ","version":"Next","tagName":"h2"},{"title":"Dataset Contents​","type":1,"pageTitle":"Aria Synthetic Environments Dataset","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset#dataset-contents","content":"100,000 unique multi-room interior scenesSimulated with realistic device trajectoriesAcross ~2-minute trajectoriesPopulated with ~8000 3D objectsWith semi-dense map representationsAnnotated using ASE Scene Language User oriented natural language mapping with architectural features, such as doors, windows and pillars, described with a CAD-like language that includes the feature type, location and dimensionsUnlocks new exciting ways to tackle research challenges related to reconstruction and detection tasks Simulated sensor data per sequence 1 x outward-facing RGB camera streamSimulated Aria camera &amp; lens characteristics Ground Truth Annotations 6DoF camera trajectory3D floor plan in Euston Structure Scene Language (SSL) format2D instance segmentation With per-scene mappings from the object instance image IDs to object classes 2D depth map ","version":"Next","tagName":"h2"},{"title":"Dataset Statistics​","type":1,"pageTitle":"Aria Synthetic Environments Dataset","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset#dataset-statistics","content":"Number of scenes: 100KNumber of images: 58M+Trajectories Total time: 67 daysTotal distance: London -&gt; San Francisco(7800 km) Rooms: Up to 5 complex Manhattan rooms All surfaces in the world are aligned with three dominant directions, typically corresponding to the X, Y, and Z axes Dataset size: ~23TB ","version":"Next","tagName":"h2"},{"title":"Documentation​","type":1,"pageTitle":"Aria Synthetic Environments Dataset","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset#documentation","content":"The ASE section of this wiki covers: Getting started A quickstart tutorial for installing the necessary tooling, download the dataset and visualize the data Dataset download A walkthrough using aria_synthetic_environments_downloader to download the ASE dataset. Data Format Aria Synthetic Environments (ASE) data formats and organization Data Tools and Visualization Data helper toolsVisualization notebook ASE Scene Reconstruction 2023-4 Challenge ","version":"Next","tagName":"h3"},{"title":"ASE Scene Reconstruction 2023-4 Challenge","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_challenges","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"ASE Scene Reconstruction 2023-4 Challenge","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_challenges#overview","content":"The 2023 Aria Synthetic Environments (ASE) challenge aims to accelerate research into scene reconstruction, serving as a catalyst for the broader research community. ","version":"Next","tagName":"h2"},{"title":"Task​","type":1,"pageTitle":"ASE Scene Reconstruction 2023-4 Challenge","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_challenges#task","content":"Given the ASE training data sequences with ground truth structured language, your task is to generate a full scene structured language description of the main elements of the scene, consisting of walls, doors and windows using the unseen 1K test scenes provided. ","version":"Next","tagName":"h2"},{"title":"Prize​","type":1,"pageTitle":"ASE Scene Reconstruction 2023-4 Challenge","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_challenges#prize","content":"First place: $10,000 and a certificate (per each challenge). Second and Third places will receive certificates and recognition on the Challenge leaderboard. Additionally, teams with the best performing or noteworthy submissions may be invited to present their work at the CVPR conference in June 2024. ","version":"Next","tagName":"h2"},{"title":"Important dates​","type":1,"pageTitle":"ASE Scene Reconstruction 2023-4 Challenge","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_challenges#important-dates","content":"Submission deadline for results: March 22, 2024 (11:59PM PST)Presentation of awards: CVPR in June 2024 (pending confirmation of our workshop) ","version":"Next","tagName":"h2"},{"title":"Key participation terms​","type":1,"pageTitle":"ASE Scene Reconstruction 2023-4 Challenge","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_challenges#key-participation-terms","content":"Entrants can register as an Individual or TeamPredictions are submitted online at Eval.aiWinners agree to share a public report (e.g. whitepaper or publication) describing the method used for generating the PredictionsParticipants are encouraged (but not required) to make source code used to generate the Predictions available under an Approved OSS License. Full terms can be found on ProjectAria.com/challenges/terms. ","version":"Next","tagName":"h2"},{"title":"How to participate​","type":1,"pageTitle":"ASE Scene Reconstruction 2023-4 Challenge","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_challenges#how-to-participate","content":"Follow the detailed instructions at Eval.ai. ","version":"Next","tagName":"h2"},{"title":"Synthetic Environments Data Tools and Visualization","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_data_tools","content":"","keywords":"","version":"Next"},{"title":"Data Helper Tools​","type":1,"pageTitle":"Synthetic Environments Data Tools and Visualization","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_data_tools#data-helper-tools","content":"These helper functions are broadly categorized into the following types: Data interpreter: interpreter.py Provides an interpreter for the ASE Scene Language to convert them into a 3D model in the form of bounding boxes Data readers: readers.py Provide readers for the: ASE Scene LanguageGround-truth trajectorySemi-dense Map points Data Plotters: plotters.py Provide simple plotting functions for the: 3D scene from ASE Scene LanguageGround-truth trajectorySemi Dense Map points ","version":"Next","tagName":"h2"},{"title":"Visualization​","type":1,"pageTitle":"Synthetic Environments Data Tools and Visualization","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_data_tools#visualization","content":"","version":"Next","tagName":"h2"},{"title":"Python sample​","type":1,"pageTitle":"Synthetic Environments Data Tools and Visualization","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_data_tools#python-sample","content":"Viewer_projects_ase displays an interactive view of an ASE sequence with Rerun. It allows you to see all data in 3D context. You can call as the following (frame_id being optional): viewer_projects_ase --dataset_path ase_data/7 --frame_id 745   tip The timeline enables you to switch before device_time and frame_id to know to which image frame_id a camera pose corresponds toThe 3D world view is clickable, you can quickly select an object and see its instance name ","version":"Next","tagName":"h2"},{"title":"Python Notebook​","type":1,"pageTitle":"Synthetic Environments Data Tools and Visualization","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_data_tools#python-notebook","content":"We also provide Jupyter notebooks to visualize the data for each sequence. To get started download ASE data following steps from Dataset Download cd /path_to/projectaria_tools jupyter notebook projects/AriaSyntheticEnvironment/tutorial/ase_tutorial_notebook.ipynb  ","version":"Next","tagName":"h2"},{"title":"Part 1: 3D visualization of the scene​","type":1,"pageTitle":"Synthetic Environments Data Tools and Visualization","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_data_tools#part-1-3d-visualization-of-the-scene","content":"This section will introduce the dataset’s 3D components as well as code snippets to help users get familiar with them. You will be taken through examples of how to load the 3D dataset annotations namely: the ground-truth trajectory, the ASE Scene Language, and the Semi-dense Map point cloud. In addition, we provide examples of how they can each be plotted. At the end of the section you should see 3D plots containing: The Semi-dense Map point cloud,The layout annotations, visualized as 3D box wireframes,The trajectory plotted as a dotted line in 3D. Example scene visualization: ","version":"Next","tagName":"h3"},{"title":"Part 2: Loading and Plotting Images and Image Annotations​","type":1,"pageTitle":"Synthetic Environments Data Tools and Visualization","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_data_tools#part-2-loading-and-plotting-images-and-image-annotations","content":"Since the file structure and format are straightforward, the code consists of very simple PIL and matplotlib code to show the 3 images (RGB, depth and instance maps) side-by-side: ","version":"Next","tagName":"h3"},{"title":"Part 3: Projecting Points into Images​","type":1,"pageTitle":"Synthetic Environments Data Tools and Visualization","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_data_tools#part-3-projecting-points-into-images","content":"Running the final part of the notebook will load the camera calibration, as well as the pointcloud, trajectory and select a random frame. Then given the device pose from the trajectory, we project the points into the frame. Points that project outside of the valid radius, should not be plotted  ","version":"Next","tagName":"h3"},{"title":"Dataset Download","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_download_dataset","content":"","keywords":"","version":"Next"},{"title":"Download via CLI​","type":1,"pageTitle":"Dataset Download","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_download_dataset#download-via-cli","content":"Follow the ASE quickstart guide to get the system ready to download data/example data. This section will introduce how to download the dataset using the aria_synthetic_environments_downloader python script. ","version":"Next","tagName":"h2"},{"title":"Detailed arguments​","type":1,"pageTitle":"Dataset Download","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_download_dataset#detailed-arguments","content":"Arguments\tType\tDescription--cdn_file\tstr\tThe download-urls file you downloaded from the ASE website page after signing up --output-dir\tstr\tA local path where the downloaded files will be stored --set\tstr\tDownload either train / test data. All the 100K data is for training data and comes with GT ASE language. At a later point we will add test data without GT ASE language, which will be used for evaluation --scene-ids\tstr\tRange of scene ids to download --unzip\tbool\tAllows the user to unzip in the output directory or keep it as a zip ","version":"Next","tagName":"h3"},{"title":"Examples​","type":1,"pageTitle":"Dataset Download","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_download_dataset#examples","content":"","version":"Next","tagName":"h2"},{"title":"Download ASE datasets​","type":1,"pageTitle":"Dataset Download","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_download_dataset#download-ase-datasets","content":"cd $HOME/Documents/projectaria_sandbox/projectaria_tools  Download first 10 scenes​ python3 projects/AriaSyntheticEnvironment/aria_synthetic_environments_downloader.py --set train --scene-ids 0-9 --cdn-file aria_synthetic_environments_dataset_download_urls.json --output-dir $HOME/projectaria_tools_ase_data --unzip True  Download a large number of scenes: This downloads all 100 scenes (10 chunks)​ python3 projects/AriaSyntheticEnvironment/aria_synthetic_environments_downloader.py --set train --scene-ids 0-99 --cdn-file aria_synthetic_environments_dataset_download_urls.json --output-dir $HOME/projectaria_tools_ase_data --unzip True  Download specific scenes: 560-569​ python3 projects/AriaSyntheticEnvironment/aria_synthetic_environments_downloader.py --set train --scene-ids 560-569 --cdn-file aria_synthetic_environments_dataset_download_urls.json --output-dir $HOME/projectaria_tools_ase_data --unzip True  ","version":"Next","tagName":"h3"},{"title":"ASE Data Format","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_data_format","content":"","keywords":"","version":"Next"},{"title":"Overall Data Organization​","type":1,"pageTitle":"ASE Data Format","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_data_format#overall-data-organization","content":"Each scene has its own subdirectory with a unique ID (0-100K)Each scene directory contains separate files and directories for each type of data &lt;sceneID&gt; ├── rgb │ └── vignette0000000.jpg │ └── vignette0000001.jpg │ ... │ └── vignette0xxn.jpg ├── depth │ └── depth0000000.jpg │ └── depth0000001.jpg │ ... │ └── depth0xxn.jpg ├── instances │ └── instance0000000.jpg │ └── instance0000001.jpg │ ... │ └── instance0xxn.jpg ├── ase_scene_language.txt ├── trajectory.txt ├── semidense_points.csv.gz ├── semidense_observations.csv.gz └── object_instances_to_classes.json  rgb - 2D RGB fisheye images Synthetically generated Aria RGB images at 10 FPSEach image is saved in JPEG format depth - 2D depth maps (16 bit) Each depth image is the same size as the corresponding synthetic RGB image, where the pixel contents are integers expressing the depth along the pixel’s ray direction, in units of mm. This should not be confused with ADT depth images, which describe the depth in the camera’s Z-axis Each image is saved in PNG format instances - 2D segmentation maps (16 bit) Each segmentation image is the same size as the corresponding synthetic RGB image, where the pixel contents are integers expressing the object Id that was observed by the pixelEach image is saved as PNG format ase_scene_language.txt - 3D floor plan definition Describes the scene in the form of a language.Each row is a command which includes its own set of parameters. A set of such commands describe the geomtery of the scene specified.Go to ASE scene language format below for more details trajectory.txt - Ground-truth trajectory Go to MPS Output - Trajectory for how the data is structured While the file structure is the same, please note, this is the ground truth trajectory, not an output generated by MPS semidense_points.csv.gz - Semi-dense map points Go to MPS Output - Semi-Dense Point Cloud for how the data is structuredProduced by MPS run on synthetic SLAM (mono scene) camera data semidense_observations.csv.gz - Semi-dense map observations Go to MPS Output - Semi-Dense Point Cloud for how the data is structuredProduced by MPS run on synthetic SLAM (mono scene) camera data object_instances_to_classes.json Per-scene mappings from the object instance image IDs to object classesGiven an instance image pixel value/object ID, one will then be able to look up the class from this mapping How to convert them to point clouds based on depth images and RGB images ","version":"Next","tagName":"h2"},{"title":"Aria RGB Sensor - Image, Depth and Instance Segmentation​","type":1,"pageTitle":"ASE Data Format","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_data_format#aria-rgb-sensor---image-depth-and-instance-segmentation","content":"For each frame from the RGB sensor we provide: A vignetted sensor imageSimulated 16 bit metric depth (mm) in PNG image formatA segmentation image (16 bit PNG) The images in each folder are in sync. This means there will be same number of images in each folder. We also provide example data visualizers to load these images and/or associate them.  ","version":"Next","tagName":"h2"},{"title":"ASE Scene Language Format​","type":1,"pageTitle":"ASE Data Format","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_data_format#ase-scene-language-format","content":"The ASE Scene Language format is set of hand-designed procedural commands in pure text form. To handle commonly encountered static indoor layout elements, we use three commands: make_wall - the full set of parameters specifies a gravity-aligned oriented boxmake_door - specify box-based cutouts from wallsmake_window - specify box-based cutouts from wall Each command includes its own set of parameters, as described below. Given the command’s full set of parameters, a geometry is completely specified. A single scene is described via a sequence of multiple commands stored in ase_scene_language.txt. The sequence length is arbitrary and follows no specific ordering. The interpretation of the command and its arguments is carried out by a customized interpreter responsible for parsing the sequence and generating a 3D mesh of the scene.  ","version":"Next","tagName":"h2"},{"title":"Trajectory and Semi-Dense Map Points​","type":1,"pageTitle":"ASE Data Format","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_data_format#trajectory-and-semi-dense-map-points","content":"Ground-truth trajectory data provides poses for each frame generated from a simulation at 10 FPS. We are follow the same trajectory format as the closed loop trajectory used by Machine Perception Services (MPS). For semi-dense map point clouds and their observations, we follow the same point cloud points and observations format as MPS. The semi-dense map point cloud is generated using same algorithm as MPS, with the addition of ground-truth trajectory and simulated SLAM camera images. ","version":"Next","tagName":"h2"},{"title":"Project Aria Support","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/support","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Project Aria Support","url":"/projectaria_tools/docs/support#overview","content":"This page provides links to various troubleshooting pages, as well as how to request support or report an issue. ","version":"Next","tagName":"h2"},{"title":"Troubleshooting pages​","type":1,"pageTitle":"Project Aria Support","url":"/projectaria_tools/docs/support#troubleshooting-pages","content":"Data Utilities TroubleshootingAria Glasses Quickstart GuideAria Research Kit Troubleshooting Client SDK and CLI Troubleshooting ","version":"Next","tagName":"h2"},{"title":"How do I get support/report issues?​","type":1,"pageTitle":"Project Aria Support","url":"/projectaria_tools/docs/support#how-do-i-get-supportreport-issues","content":"We encourage you to post issues to GitHub, so that they can be tracked. If you are a Research Partner with access to the Aria Research Kit there are several other support options available, although GitHub is ideal for bug reports and feature requests. Post to Project Aria Discord - best for discussion, feedback or user supportPost to Academic Partners Feedback and Support workplace group - discussion, feedback or user supportEmail AriaOps@meta.com - for feedback or user support ","version":"Next","tagName":"h2"},{"title":"Getting Started With the Synthetic Environments Dataset","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_getting_started","content":"","keywords":"","version":"Next"},{"title":"Quickstart Tutorial - Python​","type":1,"pageTitle":"Getting Started With the Synthetic Environments Dataset","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_getting_started#quickstart-tutorial---python","content":"","version":"Next","tagName":"h2"},{"title":"Step 0 : Check system requirements and download codebase​","type":1,"pageTitle":"Getting Started With the Synthetic Environments Dataset","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_getting_started#step-0--check-system-requirements-and-download-codebase","content":"Ensure your system is supported and then download projectaria_tools codebase from the github ","version":"Next","tagName":"h3"},{"title":"Step 1 : Install Python​","type":1,"pageTitle":"Getting Started With the Synthetic Environments Dataset","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_getting_started#step-1--install-python","content":"Ensure python3 is installed on the system (check with python3 --version) ","version":"Next","tagName":"h3"},{"title":"Step 2 : Create a virtual environment​","type":1,"pageTitle":"Getting Started With the Synthetic Environments Dataset","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_getting_started#step-2--create-a-virtual-environment","content":"rm -rf $HOME/projectaria_tools_python_env python3 -m venv $HOME/projectaria_tools_python_env source $HOME/projectaria_tools_python_env/bin/activate  ","version":"Next","tagName":"h3"},{"title":"Step 3 : Install projectaria_tools from pypi​","type":1,"pageTitle":"Getting Started With the Synthetic Environments Dataset","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_getting_started#step-3--install-projectaria_tools-from-pypi","content":"pip3 install --upgrade pip pip3 install projectaria-tools'[all]'  The ASE python tooling for projection of 3D points to RGB images is included in the projectaria_tools package, so no further steps are needed. The following packages used in this tutorial are standard python packages that are also included in project_aria_tools build. plotlynumpyscipypandasmatplotlibrequeststqdmjupyter ","version":"Next","tagName":"h3"},{"title":"Step 4 : Download sample data​","type":1,"pageTitle":"Getting Started With the Synthetic Environments Dataset","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_getting_started#step-4--download-sample-data","content":"Navigate to the ASE page on the Project Aria Website and follow the instructions to download the download-urls file. This same download-urls file can be used for any dataset download until the link expiresSetup ASE local folder and move download-urls file: mkdir -p $HOME/Documents/projectaria_sandbox/projectaria_tools_ase_data mv $HOME/Downloads/aria_synthetic_environments_dataset_download_urls.json $HOME/Documents/projectaria_sandbox/projectaria_tools_ase_data/  Download sample dataset using the download tool: cd $HOME/Documents/projectaria_sandbox/projectaria_tools python3 projects/AriaSyntheticEnvironment/aria_synthetic_environments_downloader.py --set train --scene-ids 0-10 --cdn-file $HOME/Documents/projectaria_sandbox/projectaria_tools_ase_data/aria_synthetic_environments_dataset_download_urls.json --output-dir $HOME/Documents/projectaria_sandbox/projectaria_tools_ase_data --unzip True  ","version":"Next","tagName":"h3"},{"title":"Step 5 : Run the visualization notebooks​","type":1,"pageTitle":"Getting Started With the Synthetic Environments Dataset","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_getting_started#step-5--run-the-visualization-notebooks","content":"jupyter notebook projects/AriaSyntheticEnvironment/tutorial/ase_tutorial_notebook.ipynb  ","version":"Next","tagName":"h3"},{"title":"Camera Photometric and Noise Models for Project Aria Glasses","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/tech_insights/camera_photometric_and_noise_model","content":"","keywords":"","version":"Next"},{"title":"Photometric Models​","type":1,"pageTitle":"Camera Photometric and Noise Models for Project Aria Glasses","url":"/projectaria_tools/docs/tech_insights/camera_photometric_and_noise_model#photometric-models","content":"In their working distance range, Aria camera lenses are well-focused, i.e. their point spread function is at sub-pixel level. Thus, we can establish a simplified photometric model where each camera pixel collects the photon emitted from a tiny surface area around a corresponding world point. The irradiance of each pixel is attenuated by vignetting. The vignetting of Aria cameras are dominated by two factors (1) cos^4 fall-off (2) mechanical cropping of the lens barrel. Points that falls out of the camera's FOV are not visible, and cannot be applied by the above intrinsic model. Then each pixel takes the time integral of the irradiance, as the sensors collect the arriving photon over the exposure time. The pixel intensity of a non-linear function of the amount of received photons as the ADC transform is non-linear and saturated. ","version":"Next","tagName":"h2"},{"title":"Noise Models​","type":1,"pageTitle":"Camera Photometric and Noise Models for Project Aria Glasses","url":"/projectaria_tools/docs/tech_insights/camera_photometric_and_noise_model#noise-models","content":"The two sources of noise dominating Aria camera sensors are: Shot noise, which accounts for the noise generated due to arrival of photons. Shot noise follows the Poisson distribution.Read noise, which accounts for the noise generated due to ADC conversion, etc. Read noise can be modeled by a zero-mean Gaussian random variable. ","version":"Next","tagName":"h2"},{"title":"Tech Insights","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/tech_insights","content":"Tech Insights Technical deeper dives on domain-specific topics. You don't need to read this section to use Project Aria data or glasses, but you may find it interesting to understand how Aria glasses work.","keywords":"","version":"Next"},{"title":"How Data From Project Aria Devices is Timestamped","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/tech_insights/device_timestamping","content":"","keywords":"","version":"Next"},{"title":"Aria device hardware​","type":1,"pageTitle":"How Data From Project Aria Devices is Timestamped","url":"/projectaria_tools/docs/tech_insights/device_timestamping#aria-device-hardware","content":"The figure below illustrates the various hardware components in Aria devices and how they are connected electrically. The device consists of a microcontroller unit (MCU) that interfaces with most of the sensors for configuring and controlling them. The MCU is responsible for timestamping the data from these sensors, which enables capturing the multi-modal data with common timestamps across the motion sensors, microphones and camera sensors. The device also has an Application Processor (AP) that runs Android High Level OS.  Figure 1: Project Aria device hardware diagram The device timestamp is ideally assigned, by the embedded micro-controller (MCU), to the measurement as close as possible to the time the measurement is captured. However, the meaning of the event effectively timestamped and the way the timestamp is obtained differs significantly depending on the sensor. ","version":"Next","tagName":"h2"},{"title":"Mono Scene/SLAM and Eye Tracking (ET) cameras​","type":1,"pageTitle":"How Data From Project Aria Devices is Timestamped","url":"/projectaria_tools/docs/tech_insights/device_timestamping#mono-sceneslam-and-eye-tracking-et-cameras","content":"The Mono Scene (also called SLAM) and Eye Tracking (ET) cameras have electronic global shutter sensors. They are triggered at regular rate. Their image timestamps mark the center of the exposure window and are derived from the value of a MCU counter. The timestamping error is expected to be upper-bounded by 19us. ","version":"Next","tagName":"h3"},{"title":"RGB camera​","type":1,"pageTitle":"How Data From Project Aria Devices is Timestamped","url":"/projectaria_tools/docs/tech_insights/device_timestamping#rgb-camera","content":"The RGB camera has an electronic rolling shutter. It is triggered at regular rate, often a divider of the Mono Scene camera rate. The timestamp marks the center of the exposure of the middle row and is obtained similarly to the Mono Scene camera timestamp. ","version":"Next","tagName":"h3"},{"title":"IMUs, barometer and magnetometer​","type":1,"pageTitle":"How Data From Project Aria Devices is Timestamped","url":"/projectaria_tools/docs/tech_insights/device_timestamping#imus-barometer-and-magnetometer","content":"The two IMUs, the barometer and the magnetometer sensors operate respectively at 800Hz, 1000Hz, 50Hz and 10Hz in free-running mode. We timestamp their data-ready signal on the MCU. Because of on-chip signal processing operations, those timestamps correspond to a time point after the instant for which the measurement is valid. Go to Temporal Alignment of Sensor Data for how to finely align the data with the images. ","version":"Next","tagName":"h3"},{"title":"Global Navigation Satellite System (GNSS) data​","type":1,"pageTitle":"How Data From Project Aria Devices is Timestamped","url":"/projectaria_tools/docs/tech_insights/device_timestamping#global-navigation-satellite-system-gnss-data","content":"GNSS data is timestamped on the AP at their time of arrival from the receiver of and converted to a device timestamp. Conversion of the timestamp is based on a bidirectional communication between SoC and MCU and is expected to introduce less than 100us of error. ","version":"Next","tagName":"h3"},{"title":"Spatial Microphones​","type":1,"pageTitle":"How Data From Project Aria Devices is Timestamped","url":"/projectaria_tools/docs/tech_insights/device_timestamping#spatial-microphones","content":"For the audio stream, each samples are individually timestamped with an accuracy expected to be better than one audio sample. This synchronization relies on the MCU periodically injecting an encoded version of the current device timestamp into an unused microphone channel; the AP decodes it on reception. ","version":"Next","tagName":"h3"},{"title":"Bluetooth and Wi-Fi​","type":1,"pageTitle":"How Data From Project Aria Devices is Timestamped","url":"/projectaria_tools/docs/tech_insights/device_timestamping#bluetooth-and-wi-fi","content":"Bluetooth and Wi-Fi scan data is received and timestamped on the AP using a time estimate of the MCU time. Conversion of timestamp is based on the protocol between SoC and MCU and is expected to introduce less than 100us of error. ","version":"Next","tagName":"h3"},{"title":"IMU Noise Model for Project Aria Data","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/tech_insights/imu_noise_model","content":"IMU Noise Model for Project Aria Data In our visual-inertial fusion algorithm, we model, as traditionally done, the stochastic part of the IMU error as including three components: Turn-on biasBias random walkWhite noise The noise along any of the axis of the inertial sensor is described as the sum of those contributions: xksampled=xktrue+nk0turn-on bias+nkbias random walk+nkwhite noisex^{\\text{sampled}}_k = x^{\\text{true}}_k + n^{\\text{turn-on bias}}_{k0} + n^{\\text{bias random walk}}_k + n^{\\text{white noise}}_kxksampled​=xktrue​+nk0turn-on bias​+nkbias random walk​+nkwhite noise​ Where xktruex^{\\text{true}}_kxktrue​ is the real value of the quantity measured projected on the sensor sensitive axis. xksampledx^{\\text{sampled}}_kxksampled​ is the sampled value seen as a random variable. nk0turn-on biasn^{\\text{turn-on bias}}_{k0}nk0turn-on bias​ is a random variable draw from a Gaussian when the device is turned-on (denoted here as step k0k_0k0​). nkwhite noisen^{\\text{white noise}}_{k}nkwhite noise​ is the time-independent noise components and draw from a 0-centered Gaussian at each step kkk. The latter noise is sometime \\emph{Angle Random walk in rad/s/Hzrad/s/\\sqrt{Hz}rad/s/Hz​} for the gyrometer and the \\emph{Velocity Random Walk in m/s/Hzm/s/\\sqrt{Hz}m/s/Hz​} for the accelerometer. The covariance of the Gaussian is usually characterized by the continuous noise σc\\sigma_cσc​ strength (in the same unit), which needs to be multiplied by the sampling period Δt\\Delta tΔt to get the distribution of the sample noise: σc2Δt\\sigma^2_c \\Delta tσc2​Δt. Finally, nkbias random walkn^{\\text{bias random walk}}_knkbias random walk​ is drawn from a random walk process. The parameter describing those noises is derived from an Allan Variance plot computed from data collected over a period of 24 hours in a temperature stable environment. Table 1: White noise and bias instability parameters of Aria IMU sensors accel-left\taccel-right\tgyro-left\tgyro-rightwhite noise\t0.9×10−60.9\\times 10^-60.9×10−6\t0.8×10−60.8\\times 10^-60.8×10−6\t5×10−35\\times 10^-35×10−3\t10−210^-210−2 bias instability\t280×10−6280\\times 10^-6280×10−6\t350×10−6350\\times 10^-6350×10−6\t1.3×10−31.3\\times 10^-31.3×10−3\t0.6−30.6^-30.6−3 The figure below shows the Allan Variance plot supporting this measurement. Note that the data duration used for Allan Variance was not enough to capture the bias random walk confidently. In practice, we tune the parameter of the bias random walk used for sensor-fusion starting from the bias instability measured on the Allan Variance of the sensor (the floor of the curve) and inflating it by a tuning factor. This is because real MEMS sensors are not that well modelled by this stochastic model. Figure 1: Allan Variance plot computed from data collected over 24 hours in a temperature stable environment","keywords":"","version":"Next"},{"title":"Project Aria Technical Specifications","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/tech_spec","content":"Project Aria Technical Specifications The Technical Specifications section provides information about Project Aria glasses hardware, the different configurations Aria glasses can use when recording, and how Aria glasses are calibrated.","keywords":"","version":"Next"},{"title":"Sensor Measurement Models in Project Aria Devices","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/tech_insights/sensor_measurement_model","content":"","keywords":"","version":"Next"},{"title":"IMUs​","type":1,"pageTitle":"Sensor Measurement Models in Project Aria Devices","url":"/projectaria_tools/docs/tech_insights/sensor_measurement_model#imus","content":"For IMUs, we employ an affine model where the value from the readout of accelerometer sas_asa​ or gyroscope sgs_gsg​, is compensated to obtain a &quot;real&quot; acceleration aaa and angular velocity ω\\omegaω by a=Ma−1(sa−ba)ω=Mg−1(sg−bg)a = M_a^{-1}(s_a - b_a) \\qquad \\omega = M_g^{-1}(s_g - b_g)a=Ma−1​(sa​−ba​)ω=Mg−1​(sg​−bg​) MaM_aMa​ and MgM_gMg​ are assumed to be upper triangular so that there is no global rotation from the imu body frame to the accelerometer frame. Inversely, we can simulate the sensor read-out from acceleration or angular velocity by sa=Maa+basg=Mgω+bgs_a = M_a a + b_a \\qquad s_g = M_g \\omega + b_gsa​=Ma​a+ba​sg​=Mg​ω+bg​ When the read-out signal exceeds a threshold, the signal saturates. Saturation limits are sensor dependent and referenced in the following table for accelerometer and gyrometers. \taccel-left\taccel-right\tgyro-left\tgyro-rightsaturation\t4g\t8g\t5000\t1000 ","version":"Next","tagName":"h2"},{"title":"Magnetometer, barometer and audio​","type":1,"pageTitle":"Sensor Measurement Models in Project Aria Devices","url":"/projectaria_tools/docs/tech_insights/sensor_measurement_model#magnetometer-barometer-and-audio","content":"Similar to the IMU rectification model, the sensor readouts for magnetometer, barometer, and audio data are modeled as linear to the real rrr (magnetic field, air pressure and sound intensity). Audio specifically is bias only. ","version":"Next","tagName":"h2"},{"title":"Project Aria Device Calibration","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/tech_spec/device_calibration","content":"","keywords":"","version":"Next"},{"title":"Further resources​","type":1,"pageTitle":"Project Aria Device Calibration","url":"/projectaria_tools/docs/tech_spec/device_calibration#further-resources","content":"Go to Calibration in Device Utilities to find out how to access device and sensor calibration. Go to Camera intrinsic models and Sensor measurement model for how we model sensors mathematically in calibration. ","version":"Next","tagName":"h3"},{"title":"Project Aria Hardware Specifications","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/tech_spec/hardware_spec","content":"","keywords":"","version":"Next"},{"title":"Sensor specifications​","type":1,"pageTitle":"Project Aria Hardware Specifications","url":"/projectaria_tools/docs/tech_spec/hardware_spec#sensor-specifications","content":"","version":"Next","tagName":"h2"},{"title":"Visual sensors​","type":1,"pageTitle":"Project Aria Hardware Specifications","url":"/projectaria_tools/docs/tech_spec/hardware_spec#visual-sensors","content":"The following table summarizes the specs for the five glasses on Aria glasses (two Mono Scene/SLAM, one RGB, and two ET cameras). Table 1: Aria Glasses Camera Specs Camera\tHFOV (deg)\tVFOV (deg)\tIFOV (deg/pix)\tMaximum resolution (pix)\tDownsampled resolution (pix)\tMax frame rate (FPS)\tNominal frame rate (FPS)\tShutterMono Scene (x2)\t150\t120\t0.26\t640x480\t-\t30\t10\tglobal RGB (x1)\t110\t110\t0.038\t2880x2880\t1408x1408\t30\t10\trolling ET (x2)\t64\t48\t0.2\t640x480\t320x240\t90\t10\tglobal note Cameras on Project Aria devices are installed sideways. Project Aria Tools visualizers rotate the images to show a more natural view. ","version":"Next","tagName":"h3"},{"title":"Non-visual sensors​","type":1,"pageTitle":"Project Aria Hardware Specifications","url":"/projectaria_tools/docs/tech_spec/hardware_spec#non-visual-sensors","content":"The non-visual sensors in Aria glasses are: Two IMUs operating at 1000Hz and 800Hz respectivelyOne Magnetometer operating at 10HzOne barometer operating at 50HzSeven-channel spatial microphone array with a sampling rate of 48kHz The microphone also has a stereo mode where only two channels record One GPS receiver, Wi-Fi beacon, and Bluetooth beacon. All cameras, as well as the IMU, magnetometer, barometer and microphone are calibrated and all sensor measurements are timestamped on a common clock at nanosecond resolution. The SLAM and RGB cameras have fisheye lenses to maximize the visible field of view. ","version":"Next","tagName":"h3"},{"title":"Coordinate systems​","type":1,"pageTitle":"Project Aria Hardware Specifications","url":"/projectaria_tools/docs/tech_spec/hardware_spec#coordinate-systems","content":"Applications like stereo vision and navigation usually handle 2D and 3D points in different spaces, and transformations need to be conducted between them. With Project Aria data, we attach a local R3 coordinate frame to each sensor.  Figure 2: Sensors and Sensor Directions on Project Aria Devices To find out more about the coordinate systems and access calibration data, got to: 2D Image Coordinate System Conventions3D Coordinate Frame ConventionsCalibration Code Snippets ","version":"Next","tagName":"h3"},{"title":"Other hardware specifications​","type":1,"pageTitle":"Project Aria Hardware Specifications","url":"/projectaria_tools/docs/tech_spec/hardware_spec#other-hardware-specifications","content":"","version":"Next","tagName":"h2"},{"title":"Compute​","type":1,"pageTitle":"Project Aria Hardware Specifications","url":"/projectaria_tools/docs/tech_spec/hardware_spec#compute","content":"Qualcomm SD835, 4GB RAM, 128GB storageFlash memory (UFS)Android 7.1SW configurable user button and switch ","version":"Next","tagName":"h3"},{"title":"Weight & Size​","type":1,"pageTitle":"Project Aria Hardware Specifications","url":"/projectaria_tools/docs/tech_spec/hardware_spec#weight--size","content":"75g in two sizes 147mm and 152mm frame width, with adjustable nose pads and temple arms (87% fit coverage). ","version":"Next","tagName":"h3"},{"title":"Visual Correction​","type":1,"pageTitle":"Project Aria Hardware Specifications","url":"/projectaria_tools/docs/tech_spec/hardware_spec#visual-correction","content":"Removable lenses with plano (Non-Rx) or single vision Rx correction [-4.5D to +3.5D]. ","version":"Next","tagName":"h3"},{"title":"Battery Life​","type":1,"pageTitle":"Project Aria Hardware Specifications","url":"/projectaria_tools/docs/tech_spec/hardware_spec#battery-life","content":"Capacity is 2.5Wh. Operating time depends on the recording profile. Battery life is 1.5 hours of continuous recording + 30 hours standby when using profile 0: 10 FPS ET x 210 FPS Mono Scene (SLAM) x 21 FPS RGB 8MPIMU, Wi-Fi + GPS and 7-channel audio on at nominal FPS Aria glasses connects to USB via a magnetic connector on the right temple arm. ","version":"Next","tagName":"h3"},{"title":"Temporal Alignment of Aria Sensor Data","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/tech_insights/temporal_alignment_of_sensor_data","content":"","keywords":"","version":"Next"},{"title":"Temporal Alignment​","type":1,"pageTitle":"Temporal Alignment of Aria Sensor Data","url":"/projectaria_tools/docs/tech_insights/temporal_alignment_of_sensor_data#temporal-alignment","content":"Although each sensor data pieces are timestamped in the same time domain, there might be an offset between the instant represented by their timestamps and the actual instant for which the enclosed measurement is valid. For proper sensor fusion where precise temporal alignment of the data is required, we must take this offset into account. We need an estimate of this offset for each sensor. ","version":"Next","tagName":"h2"},{"title":"Per sensor time offset : dtDeviceSensor\\text{dt}_{\\text{Device}}^{\\text{Sensor}}dtDeviceSensor​​","type":1,"pageTitle":"Temporal Alignment of Aria Sensor Data","url":"/projectaria_tools/docs/tech_insights/temporal_alignment_of_sensor_data#per-sensor-time-offset--textdt_textdevicetextsensor","content":"Since the SLAM (mono scene) camera timestamps are direct measurements of a well defined instant (the center of exposure of our image sensor), we choose the SLAM camera timestamps as the reference timestamp and define dtDeviceSlam=0\\text{dt}_{\\text{Device}}^{\\text{Slam}} = 0dtDeviceSlam​=0. The same goes for the RGB camera : dtDeviceRGB=0\\text{dt}_{\\text{Device}}^{\\text{RGB}} = 0dtDeviceRGB​=0. Note that the next subsection describes in more details the temporal aspect of the image formation model. For IMUs, the time offset is estimated in the factory by our camera-imu calibration process and is so that the following relation approximately holds (after IMU intrinsics compensation): a^tDevice=a~(tDevice+dtDeviceAccel+0.5Δt)\\hat{a}_{t_{\\text{Device}}} = \\tilde{a}\\left(t_{\\text{Device}} + \\text{dt}_{\\text{Device}}^{\\text{Accel}} + 0.5\\Delta_t \\right)a^tDevice​​=a~(tDevice​+dtDeviceAccel​+0.5Δt​) where tDevicet_{\\text{Device}}tDevice​ is a timestamp in the device time domain, a^tDevice\\hat{a}_{t_{\\text{Device}}}a^tDevice​​ is an estimate of the true acceleration at the instant represented by the timestamp tDevicet_{\\text{Device}}tDevice​, dtDeviceAccel\\text{dt}_{\\text{Device}}^{\\text{Accel}}dtDeviceAccel​ is the estimate of the time offset, Δt\\Delta_tΔt​ is the sampling period of the sensor. Finally, the operator t→a~(t)t\\xrightarrow{}\\tilde{a}(t)t​a~(t) is a temporal interpolation of the compensated IMU sample time series (tk,a~k)(t_k, \\tilde{a}_k)(tk​,a~k​). Note the appearance of 0.5Δt0.5\\Delta_t0.5Δt​ in previous equation, which stems from internal implementation choice. The same relation exists for the gyrometer. For magnetometer, we estimate dtDeviceMag\\text{dt}_{\\text{Device}}^{\\text{Mag}}dtDeviceMag​ to be around +5ms+5ms+5ms with the following relation: B^tDevice=B~(tDevice+dtDeviceMag)\\hat{B}_{t_{\\text{Device}}} = \\tilde{B}\\left(t_{\\text{Device}} + \\text{dt}_{\\text{Device}}^{\\text{Mag}} \\right)B^tDevice​​=B~(tDevice​+dtDeviceMag​) Where notation are similar as above with B^\\hat{B}B^ representing the magnetic field vector. For barometer, audio signal and GPS data, such an offset is undetermined. ","version":"Next","tagName":"h3"},{"title":"Images formation temporal model: rolling shutter and PLS artifact​","type":1,"pageTitle":"Temporal Alignment of Aria Sensor Data","url":"/projectaria_tools/docs/tech_insights/temporal_alignment_of_sensor_data#images-formation-temporal-model-rolling-shutter-and-pls-artifact","content":"In practice, the images obtained from our environment facing camera are not well described as captured at a unique timestamp for the most demanding applications. First, the regular exposure duration window can range from 19us up to 14ms, second the RGB sensor has a rolling shutter, where each row is being captured at different time per design, finally the SLAM cameras, even if specified as global shutter sensors are impacted by a row-dependent parasitic light sensitivity: a proportion of the photons forming the image are captured outside of the regular exposure window. This proportion depends on the row. For the RGB sensor, we characterize the rolling shutter behavior through the read-out time Δdtreadout\\Delta \\text{dt}_{\\text{readout}}Δdtreadout​, we define is as the time between the readout of the first row and the readout of the last row. This time depends on the sensor binning configuration, that can differ on a recording basis. The readout time is specified to be 16.26ms16.26ms16.26ms for the full resolution (2880×28802880\\times28802880×2880) and 5ms5ms5ms for the binned/cropped configuration (1408×14081408\\times14081408×1408). We can account for rolling shutter by assigning a center of exposure timestamp to each pixel with the following formulae: tDevice(p)=tDeviceImage+(row(p)H−0.5)⋅Δdtreadoutt_{\\text{Device}}(\\boldsymbol{p}) = t_{\\text{Device}}^{\\text{Image}} + \\left(\\frac{\\text{row}(\\boldsymbol{p})}{H} - 0.5\\right) \\cdot \\Delta \\text{dt}_{\\text{readout}}tDevice​(p)=tDeviceImage​+(Hrow(p)​−0.5)⋅Δdtreadout​ Where tDevice(p)t_{\\text{Device}}(\\boldsymbol{p})tDevice​(p) is the timestamp of the observation at pixel p\\boldsymbol{p}p, tDeviceImaget_{\\text{Device}}^{\\text{Image}}tDeviceImage​ is the device timestamp of the image assigned by the MCU, p→row(p)\\boldsymbol{p}\\xrightarrow{}\\text{row}(\\boldsymbol{p})p​row(p) is the projection of the pixel coordinate on the image dimension aligned with sensor rows, HHH is the size of the image along this dimension, Δdtreadout\\Delta \\text{dt}_{\\text{readout}}Δdtreadout​ is the readout time value mentioned above. For even more demanding applications, one might need to compensate timestamp of pixels observation for Slam cameras too. On Aria camera sensors, readout of the image is done row by row. Each row can still accumulate charges after the regular exposure time and until it is fully read out, an effect sometimes called parasitic light sensitivity (PLS). The readout time of the last row Δdtpls\\Delta \\text{dt}_{\\text{pls}}Δdtpls​, i.e. the time a pixel on the last row still accumulates electrons before being discharged is specified by the manufacturer as being 9.12ms. The ratio SplsS_{\\text{pls}}Spls​ of the sensitivity during readout over the sensitivity during regular exposure was estimated to be ~0.01, instead of the ideal 0 value. From this, it results that when dealing with pixel observation time, it might be necessary to take effect into account by assigning to each pixel their effective center of exposure, we suggest the the following formulae: tDevice(p)=tDeviceImage+12⋅r(p)⋅Spls⋅1+re(p)1+re(p)⋅Splst_{\\text{Device}}(\\boldsymbol{p}) = t_{\\text{Device}}^{\\text{Image}} + \\frac{1}{2} \\cdot r\\left(\\boldsymbol{p}\\right) \\cdot S_{\\text{pls}} \\cdot \\frac{1 + r_e\\left({\\boldsymbol{p}}\\right)}{1 + r_e\\left({\\boldsymbol{p}}\\right) \\cdot S_{\\text{pls}}}tDevice​(p)=tDeviceImage​+21​⋅r(p)⋅Spls​⋅1+re​(p)⋅Spls​1+re​(p)​ Where tDevice(p)t_{\\text{Device}}(\\boldsymbol{p})tDevice​(p) represents the time of the pixel observation (effective center of exposure), r(p)r\\left({\\boldsymbol{p}}\\right)r(p) represents the readout time of the current pixel p\\boldsymbol{p}p and is computed as Δdtplsrow(p)H\\Delta \\text{dt}_{\\text{pls}} \\frac{\\text{row}(\\boldsymbol{p})}{H}Δdtpls​Hrow(p)​, SplsS_{\\text{pls}}Spls​ represents the sensitivity ratio of the readout phase over the exposure phase, re(p)r_e\\left({\\boldsymbol{p}}\\right)re​(p) represents the ratio of the current row readout time over the regular exposure duration. ","version":"Next","tagName":"h3"},{"title":"Project Aria Recording Profiles","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/tech_spec/recording_profiles","content":"Project Aria Recording Profiles Project Aria glasses have multiple recording profiles that enable users to choose what sensors to record with and what settings to use. Aria glasses currently support 17 different recording profiles, which vary by: Sub-selection of sensor streamsRGB and ET (Eye Tracking) camera resolutionMono Scene (often, but not exclusively used for SLAM), RGB and ET camera frame rate and auto-exposure settingsImage stream formatNumber of audio channels: all (7) v.s. stereo(2) The default profile is Profile0. Its settings are: All sensors are turned onMono Scene cameras at 10 frame per second (FPS), RGB camera at 1 FPS and ET at 10 FPSFull resolution (2880x2880) RGB and downsampled (320x240) ETAll images in JPEG7-channel audio See the following table for a detailed spec for each profile we currently support. We add new profiles when necessary. If you are a research partner with access to the ARK, you can also find details of each recording profile using the Mobile Companion app. If you would like a quick guide on how some recording profiles are used, as well as which ones are compatible with Machine Perception Services, go to the Recording Profile Guide. Microphones\tET Cameras\tRGB Cameras Mono Scene Cameras GPS IMU 1 IMU 2 Magnetometer Barometer Wi-Fi Profile\tChannels Sample Rate (kHz)\tResolution FPS\tAuto Exposure\tImage Format\tResolution FPS\tAuto Exposure\tImage Format\tResolution FPS\tAuto Exposure\tImage Format\tData Rate (Hz)\tData Rate (Hz)\tData Rate (Hz)\tData Rate (Hz)\tData Rate (Hz)\tScan Duration(s)0\t7 48 320x240 10 OFF JPEG 2880x2880 1 ON JPEG 640x480 10 ON JPEG 1 1000 800 10 50 10 2\t- - 1408x1408 20 ON JPEG 640x480 20 ON JPEG - - - - 1 1000 800 10 50 10 4\t7 48 - - - - 1408x1408 10 ON JPEG - - - - 1 1000 800 10 50 - 5\t- - 640x480 20 OFF JPEG 1408x1408 20 ON JPEG - - - - - 1000 800 - - - 7\t7 48 - - - - 1408x1408 10 ON RAW - - - - 1 1000 800 10 50 - 8\t7 48 320x240 30 OFF JPEG 1408x1408 5 ON JPEG 640x480 15 ON JPEG - 1000 800 10 50 - 9\t7 48 320x240 10 OFF JPEG 1408x1408 20 ON JPEG 640x480 10 ON JPEG - 1000 800 10 50 - 10\t7 48 320x240 10 OFF JPEG 1408x1408 10 ON JPEG 640x480 10 ON JPEG 1 1000 800 10 50 10 12\t- - 320x240 10 OFF JPEG 1408x1408 10 ON JPEG 640x480 10 ON JPEG - 1000 800 10 50 - 14\t- - 320x240 10 OFF JPEG 1408x1408 10 ON JPEG 640x480 30 ON JPEG 1 1000 800 10 50 - 15\t7 48 320x240 10 OFF JPEG 1408x1408 30 ON JPEG 640x480 30 ON JPEG - 1000 800 10 50 - 16\t2 48 640x480 90 OFF JPEG 1408x1408 10 ON JPEG - - - - - 1000 800 10 50 - 17\t- - 320x240 10 OFF VIDEO 1408x1408 10 ON VIDEO 640x480 10 ON VIDEO - 1000 800 10 50 - 18\t7 48 320x240 10 OFF JPEG 1408x1408 10 ON JPEG 640x480 10 ON JPEG - 1000 800 10 50 - 19\t- - - - - - 1408x1408 10 ON JPEG 640x480 10 ON JPEG 1 1000 800 10 50 10 20\t2 48 - - - - - - - - - - - - - 1000 800 - - - 21\t7 48 320x240 30 OFF JPEG 1408x1408 15 ON JPEG 640x480 15 ON JPEG - 1000 800 10 50 22\t7 48 320x240 10 OFF JPEG 1408x1408 30 ON JPEG 640x480 10 ON JPEG - 1000 800 10 50 23\t7 48 320x240 10 OFF JPEG 1408x1408 30 ON JPEG 640x480 10 ON JPEG - 1000 800 10 50 24\t- - - - - JPEG 2880x2880 10 ON JPEG 640x480 10 ON JPEG - 1000 800 10 50 25\t- - - - - JPEG 1408x1408 10 ON JPEG 640x480 20 ON JPEG 1 1000 800 - - 10 ","keywords":"","version":"Next"},{"title":"Camera Intrinsic Models for Project Aria devices","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/tech_insights/camera_intrinsic_models","content":"","keywords":"","version":"Next"},{"title":"The linear camera model​","type":1,"pageTitle":"Camera Intrinsic Models for Project Aria devices","url":"/projectaria_tools/docs/tech_insights/camera_intrinsic_models#the-linear-camera-model","content":"The linear camera model (a.k.a pinhole model) is parametrized by 4 coefficients : f_x, f_y, c_x, c_y. (fx,fy)(f_x, f_y)(fx​,fy​) are the focal lengths, and cx,cyc_x, c_ycx​,cy​ are the coordinate of the projection of the optical axis. It maps from world point (x,y,z)(x,y,z)(x,y,z) to 2D camera pixel p=(u,v)\\mathbf{p}=(u, v)p=(u,v) with the following formulae. u=fxx/z+cxv=fyy/z+cyu = f_x x/z + c_x \\\\ v = f_y y/z + c_yu=fx​x/z+cx​v=fy​y/z+cy​ Or, in polar coordinates: u=fxtan(θ)cos⁡(φ)+cx,v=fytan(θ)sin⁡(φ)+cy.u = f_x tan(\\theta) \\cos(\\varphi) + c_x, \\\\ v = f_y tan(\\theta) \\sin(\\varphi) + c_y.u=fx​tan(θ)cos(φ)+cx​,v=fy​tan(θ)sin(φ)+cy​. Inversely, we can unproject from 2D camera pixel p=(u,v)\\mathbf{p}=(u, v)p=(u,v) to the homogeneous coordinate of the world point by x/z=(u−cx)/fx,y/z=(v−cy)/fy.x/z=(u-c_x)/f_x, \\\\ y/z=(v-c_y)/f_y.x/z=(u−cx​)/fx​,y/z=(v−cy​)/fy​. The linear camera model preserves linearity in 3D space, thus straight lines in the real world are supposed to look straight under the linear camera model. ","version":"Next","tagName":"h2"},{"title":"The spherical camera model​","type":1,"pageTitle":"Camera Intrinsic Models for Project Aria devices","url":"/projectaria_tools/docs/tech_insights/camera_intrinsic_models#the-spherical-camera-model","content":"The spherical camera model is, similarly from the linear camera model parametrized by 4 coefficients : f_x, f_y, c_x, c_y. The pixel coordinates are linear to solid angles rather than the homography coordinate system. The projection function can be written in polar coordinates u=fxθcos⁡(φ)+cx,v=fyθsin⁡(φ)+cy.u = f_x \\theta \\cos(\\varphi) + c_x, \\\\ v = f_y \\theta \\sin(\\varphi) + c_y.u=fx​θcos(φ)+cx​,v=fy​θsin(φ)+cy​. Note the difference from the linear camera model — under spherical projection, 3D straight lines look curved in images. Inversely, we can unproject from 2D camera pixel p=(u,v)\\mathbf{p}=(u, v)p=(u,v) to the homogeneous coordinate of the world point by θ=(u−cx)2/fx2+(v−cy)2/fy2,φ=arctan⁡((u−cx)/fx,(v−cy)/fy).\\theta = \\sqrt{(u - c_x)^2/f_x^2 + (v - c_y)^2/f_y^2}, \\\\ \\varphi = \\arctan((u - c_x)/f_x, (v - c_y)/f_y).θ=(u−cx​)2/fx2​+(v−cy​)2/fy2​​,φ=arctan((u−cx​)/fx​,(v−cy​)/fy​). ","version":"Next","tagName":"h2"},{"title":"The KannalaBrandtK3 (KB3) model​","type":1,"pageTitle":"Camera Intrinsic Models for Project Aria devices","url":"/projectaria_tools/docs/tech_insights/camera_intrinsic_models#the-kannalabrandtk3-kb3-model","content":"The KannalaBrandtK3 model adds radial distortion to the linear model u=fxr(θ)cos⁡(φ)+cx,v=fyr(θ)sin⁡(φ)+cy.u = f_x r(\\theta) \\cos(\\varphi) + c_x, \\quad v = f_y r(\\theta) \\sin(\\varphi) + c_y.u=fx​r(θ)cos(φ)+cx​,v=fy​r(θ)sin(φ)+cy​. where r(θ)=θ+k0θ3+k1θ5+k2θ7+k3θ9+...r(\\theta) = \\theta + k_0 \\theta^3 + k_1 \\theta^5 + k_2 \\theta^7 + k_3 \\theta^9 + ...r(θ)=θ+k0​θ3+k1​θ5+k2​θ7+k3​θ9+... In KannalaBrandtK3 model we use a 9-th order polynomial with four radial distortion parameters k0,...k3k_0, ... k_3k0​,...k3​. To unproject from camera pixel (u,v)(u, v)(u,v) to the world point (θ,φ)(\\theta, \\varphi)(θ,φ), we first compute φ=arctan⁡((u−cx)/fx,(v−cy)/fy)r(θ)=(u−cx)2/fx2+(v−cy)2/fy2\\varphi = \\arctan((u - c_x)/f_x, (v - c_y)/f_y) \\\\ r(\\theta) = \\sqrt{(u - c_x)^2/f_x^2 + (v - c_y)^2/f_y^2}φ=arctan((u−cx​)/fx​,(v−cy​)/fy​)r(θ)=(u−cx​)2/fx2​+(v−cy​)2/fy2​​ Then we use Newton method to inverse the function r(θ)r(\\theta)r(θ) to compute θ\\thetaθ. See the code here. ","version":"Next","tagName":"h2"},{"title":"The Fisheye62 model​","type":1,"pageTitle":"Camera Intrinsic Models for Project Aria devices","url":"/projectaria_tools/docs/tech_insights/camera_intrinsic_models#the-fisheye62-model","content":"The Fisheye62 model adds tangential distortion on top of the KB3 model parametrized by two new coefficients: p_0 p_1. u=fx.(ur+tx(ur,vr))+cx,v=fy.(vr+ty(ur,vr))+cy.u = f_x . (u_r + t_x(u_r, v_r)) + c_x, \\\\ v = f_y . (v_r + t_y(u_r, v_r)) + c_y.u=fx​.(ur​+tx​(ur​,vr​))+cx​,v=fy​.(vr​+ty​(ur​,vr​))+cy​. where ur=r(θ)cos⁡(φ),vr=r(θ)sin⁡(φ).u_r = r(\\theta) \\cos(\\varphi), \\\\ v_r = r(\\theta) \\sin(\\varphi).ur​=r(θ)cos(φ),vr​=r(θ)sin(φ). and tx(ur,vr)=p0(2ur2+r(θ)2)+2p1urvr,ty(ur,vr)=p1(2vr2+r(θ)2)+2p0urvr.t_x(u_r, v_r) = p_0(2 u_r^2 + r(\\theta)^2) + 2p_1u_rv_r, \\\\ t_y(u_r, v_r) = p_1(2 v_r^2 + r(\\theta)^2) + 2p_0u_rv_r.tx​(ur​,vr​)=p0​(2ur2​+r(θ)2)+2p1​ur​vr​,ty​(ur​,vr​)=p1​(2vr2​+r(θ)2)+2p0​ur​vr​. To unproject from camera pixel (u,v)(u, v)(u,v) to the world point (θ,φ)(\\theta, \\varphi)(θ,φ), we first use Newton method to compute uru_rur​ and vrv_rvr​ from (u−cx)/fx(u - c_x)/f_x(u−cx​)/fx​ and (v−cy)/fy(v - cy)/f_y(v−cy)/fy​, and then compute (θ,φ)(\\theta, \\varphi)(θ,φ) using the above KB3 unproject method. ","version":"Next","tagName":"h2"},{"title":"The FisheyeRadTanThinPrism (Fisheye624) model​","type":1,"pageTitle":"Camera Intrinsic Models for Project Aria devices","url":"/projectaria_tools/docs/tech_insights/camera_intrinsic_models#the-fisheyeradtanthinprism-fisheye624-model","content":"The FisheyeRadTanThinPrism (also called Fisheye624 in file and codebase) models thin-prism distortion (noted tptptp) on top of the Fisheye62 model above. Its parametrization contains 4 additional coefficients: s_0 s_1 s_2 s_3. The projection function writes: u=fx⋅(ur+tx(ur,vr)+tpx(ur,vr))+cx,v=fy⋅(vr+ty(ur,vr)+tpy(ur,vr))+cy.u = f_x \\cdot (u_r + t_x(u_r, v_r) + tp_x(u_r, v_r)) + c_x, \\\\ v = f_y \\cdot (v_r + t_y(u_r, v_r) + tp_y(u_r, v_r)) + c_y.u=fx​⋅(ur​+tx​(ur​,vr​)+tpx​(ur​,vr​))+cx​,v=fy​⋅(vr​+ty​(ur​,vr​)+tpy​(ur​,vr​))+cy​. u_r, v_r, t_x, t_y are defined as in the Fisheye62 model, while tpxtp_xtpx​ and tpytp_ytpy​ are defined as: tpx(ur,vr)=s0r(θ)2+s1r(θ)4tpx(ur,vr)=s2r(θ)2+s3r(θ)4tp_x(u_r, v_r) = s_0 r(\\theta)^2 + s_1 r(\\theta)^4 tp_x(u_r, v_r) = s_2 r(\\theta)^2 + s_3 r(\\theta)^4tpx​(ur​,vr​)=s0​r(θ)2+s1​r(θ)4tpx​(ur​,vr​)=s2​r(θ)2+s3​r(θ)4 To unproject from camera pixel (u,v)(u, v)(u,v) to the world point (θ,φ)(\\theta, \\varphi)(θ,φ), we first use Newton method to compute uru_rur​ and vrv_rvr​ from (u−cx)/fx(u - c_x)/f_x(u−cx​)/fx​ and (v−cy)/fy(v - cy)/f_y(v−cy)/fy​, and then compute (θ,φ)(\\theta, \\varphi)(θ,φ) using the above KB3 unproject method. Note that in practice, in our codebase and calibration file we assume fxf_xfx​ and fyf_yfy​ are equal. ","version":"Next","tagName":"h2"}]